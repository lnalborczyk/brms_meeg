---
title: "Precise temporal localisation of M/EEG effects with Bayesian generalised additive multilevel models"
shorttitle: "Bayesian modelling of M/EEG data"
author:
  - name: Ladislas Nalborczyk
    corresponding: true
    orcid: 0000-0002-7419-9855
    email: ladislas.nalborczyk@cnrs.fr
    url: https://lnalborczyk.github.io
    affiliations:
      - id: id1
        name: "Aix Marseille Univ, CNRS, LPL"
        address: "5 avenue Pasteur"
        city: "13100 Aix-en-Provence"
        country: France
  - name: Paul Bürkner
    orcid: 0000-0001-5765-8995
    url: https://paulbuerkner.com
    affiliations:
      - name: "TU Dortmund University, Department of Statistics"
author-note:
  disclosures:
    conflict-of-interest: "The authors have no conflicts of interest to disclose."
abstract: "Time-resolved electrophysiological measurements such as those obtained through magneto- and electroencephalography (M/EEG) offer a unique window onto the neural activity underlying cognitive processes. Researchers are often interested in determining whether and when these signals differ across experimental conditions or participant groups. The conventional approach involves mass-univariate statistical testing across time and space followed by corrections for multiple comparisons or some form of cluster-based inference. While effective for controlling error rates at the cluster-level, cluster-based inference comes with a significant limitation: by shifting the focus of inference from individual time points to clusters, it prevents drawing conclusions about the precise onset or offset of observed effects. Here, we present a *model-based* alternative for analysing M/EEG timeseries, such as event-related potentials or time-resolved decoding accuracy. Our approach leverages Bayesian generalised additive multilevel models, providing posterior odds that an effect exceeds zero (or chance) at each time point, while naturally accounting for temporal dependencies and between-subject variability. Using both simulated and empirical M/EEG datasets, we show that this approach substantially outperforms conventional methods in estimating the onset and offset of neural effects, yielding more precise and reliable estimates. We provide an open-source R package implementing the method and describe how it can be integrated into M/EEG analysis pipelines using MNE-Python."
keywords: [EEG, MEG, cluster-based inference, multiple comparisons, generalised additive models, mixed-effects models, multilevel models, Bayesian statistics, brms]
floatsintext: true
numbered-lines: true
word-count: true
bibliography:
  - meeg_modelling.bib
  - grateful-refs.bib
suppress-title-page: false
link-citations: true
# see https://www.overleaf.com/learn/latex/Font_typefaces
# see available font via systemfonts::system_fonts()
# monofont: Monaco
# monofont: Courier
mainfont: CMU Serif
fontsize: 10pt
draftfirst: false
draftall: false
a4paper: true
donotrepeattitle: true
number-sections: true
number-depth: 3
lang: en
toc: true
language:
  citation-last-author-separator: "&"
  email: "email"
  title-block-author-note: "Author note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
format:
  apaquarto-pdf:
    latex-engine: xelatex
    documentmode: doc # man, jou, doc, stu
    # number_sections: true
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{mathtools}
          \usepackage{booktabs, caption, longtable, colortbl, array}
          \usepackage{setspace}
          \onehalfspacing
---

```{r setup, include = FALSE}
# loading packages
library(changepoint)
library(tidyverse)
library(tidybayes)
library(patchwork)
library(MetBrewer)
library(tidytext)
library(grateful)
library(ggrepel)
library(ggpubr)
library(scales)
library(pakret)
library(scico)
library(knitr)
library(brms)
library(mgcv)
library(gt)

# setting knitr options
knitr::opts_chunk$set(
    cache = TRUE,
    eval = TRUE,
    echo = FALSE,
    warning = FALSE,
    message = FALSE,
    fig.align = "center",
    out.width = "100%",
    fig.asp = 0.75,
    fig.pos = "!htb",
    dev = "png",
    dpi = 300
    )

# setting a default ggplot2 theme
theme_set(theme_bw(base_size = 10, base_family = "Open Sans") )

# template for citing R packages
pkrt_set(pkg = "the `R` package `:pkg` v :ver [:ref]")
```

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

# Introduction

## Problem statement

Understanding the temporal dynamics of cognitive processes requires methods that can capture fast-changing neural activity with high temporal resolution. Magnetoencephalography and electroencephalography (M/EEG) are two such methods, widely used in cognitive neuroscience for their ability to track brain activity at the millisecond scale. These techniques provide rich time series data that reflect how neural responses unfold in response to stimuli or tasks. A central goal in many M/EEG studies is to determine whether, when, and where neural responses differ across experimental conditions or groups.

The conventional approach involves mass-univariate statistical testing through time and/or space followed by some form of correction for multiple comparisons with the goal of maintaining the familywise error rate (FWER) or the false discovery rate (FDR) at the nominal level (e.g., 5%). Cluster-based inference is the most common way of achieving this sort of error control in the M/EEG literature, being the recommended approach in several software programs [e.g., `EEGlab`, @delorme2004; `MNE-Python`, @gramfort2013]. While effective for controlling error rates, cluster-based inference comes with a significant limitation: by shifting the focus of inference from individual datapoints (e.g., timesteps, sensors, voxels) to clusters, it prevents the ability to draw precise conclusions about the spatiotemporal localisation of such effects [@maris2007; @sassenhagen2019]. As pointed out by @maris2007: "there is a conflict between this interest in localized effects and our choice for a global null hypothesis: by controlling the FA [false alarm] rate under this global null hypothesis, one cannot quantify the uncertainty in the spatiotemporal localization of the effect". Even worse, @rosenblatt2018 note that cluster-based inference suffers from low spatial resolution: "Since discovering a cluster means that 'there exists at least one voxel with an evoked response in the cluster', and not that 'all the voxels in the cluster have an evoked response', it follows that the larger the detected cluster, the less information we have on the location of the activation." As a consequence, cluster-based inference is expected to perform poorly for identifying the onset of M/EEG effects; a property that was later demonstrated in simulation studies [e.g., @rousselet_using_2025; @sassenhagen2019].

To overcome the limitations of cluster-based inference, we introduce a novel *model-based* approach for precisely localising M/EEG effects in time, space, and other dimensions. The proposed approach, based on Bayesian generalised additive multilevel models, allows quantifying the posterior odds of effects being above chance at the level of timesteps, sensors, voxels, etc, while naturally taking into account spatiotemporal dependencies present in M/EEG data. We compare the performance of the proposed approach to well-established alternative methods using both simulated and actual M/EEG data and show that it significantly outperforms alternative methods in estimating the onset and offset of M/EEG effects.

## Statistical errors and cluster-based inference

The issues with multiple comparisons represent a common and well-recognised danger in neuroimaging and M/EEG research, where the collected data allows for a multitude of potential hypothesis tests and is characterised by complex structures of spatiotemporal dependencies. The probability of obtaining at least one false positive in an ensemble (family) of $m$ tests (i.e., the FWER) is computed as $1-\left(1-\alpha\right)^{m}$ (for $m=10$ independent tests and $\alpha=0.05$, it is approximately equal to $0.4$). Different methods exist to control the FWER, that is, to bring it back to $\alpha$. Most methods apply a simple correction to series of $p$-values issued from univariate statistical tests (e.g., t-tests). For instance, the Bonferroni correction [@dunn1961] consists in setting the significance threshold to $\alpha/m$, or equivalently, multiplying the $p$-values by $m$ and using the standard $\alpha$ significance threshold. This method is generally overconservative (i.e., under-powered) as it assumes statistical independence of the tests, an assumption that is clearly violated in the context of M/EEG timeseries characterised by massive spatiotemporal dependencies. Some alternative methods aims at controlling the FDR, defined as the proportion of false positive *among positive tests* [e.g., @benjamini1995; @benjamini2001]. However, a major limitation of both types of corrections is that they do not take into account the spatial and temporal information contained in M/EEG data.

A popular technique to account for spatiotemporal dependencies while controlling the FWER is cluster-based inference [@bullmore1999; @maris2007]. A typical cluster-based inference consists of two successive steps [for more details on cluster-based inference, see for instance @frossard2022; @maris2011; @maris2007; @sassenhagen2019]. First, clusters are defined as sets of contiguous timesteps, sensors, voxels, etc, whose activity, summarised by some test statistic (e.g., a $t$-value), exceeds a predefined threshold (e.g., the 95th percentile of the parametric null distribution). Clusters are then characterised by their height (i.e., maximal value), extent (number of constituent elements), or some combination of both, for instance by summing the statistics within a cluster, an approach referred to as "cluster mass" [@maris2007; @pernet2015]. Then, the null hypothesis is tested by computing a $p$-value for each identified cluster by comparing its mass with the null distribution of cluster masses (obtained via permutation). As alluded previously, a significant cluster is a cluster which contains *at least one* significant time-point. As such, it would be incorrect to conclude, for instance, that the timestep of a significant cluster is the first moment at which some conditions differ [@frossard2022; @sassenhagen2019]. Because the inference is performed at the second step (i.e., once clusters have been formed), no conclusion can be made about individual datapoints (e.g., timesteps, sensors, etc).

As different cluster-forming thresholds lead to clusters with different spatial or temporal extent, this initial threshold modulates the sensitivity of the subsequent permutation test. The threshold-free cluster enhancement (TFCE) method was introduced by @smith2009 to overcome this choice of an arbitrary threshold. In brief, the TFCE method works as follows. Instead of picking an arbitrary cluster-forming threshold (e.g., $t=2$), the methods consist in trying all (or many) possible thresholds in a given range and checking whether a given datapoint (e.g., timestep, sensor, voxel) belongs to a significant cluster under any of the set of thresholds. Then, instead of using cluster mass, one uses a weighted average between the cluster extend ($e$, how broad is the cluster, that is, how many connected samples it contains) and the cluster height ($h$, how high is the cluster, that is, how large is the test statistic). The TFCE score at each timestep $t$ is given by:

$$
\text{TFCE(t)} = \int_{h=h_{0}}^{h=h_{t}} e(h)^{E} h^{H} \mathrm{d}h
$$

where $h_{0}$ is typically $0$ and parameters $E$ and $H$ are set a priori (typically to 0.5 and 2, respectively) and control the influence of the extend and height on the TFCE. In practice, this integral is approximated by a sum over small $h$ increments. Then, a *p*-value for each timestep $t$ is computed by comparing its TFCE with the null distribution of TFCE values (obtained via permutation). For each permuted signal, we keep the maximal value over the whole signal for the null distribution of the TFCE. The TFCE combined with permutation (assuming a large enough number of permutations) has been shown to provide accurate FWER control [e.g., @pernet2015]. However, further simulation work showed that cluster-based methods (including TFCE) perform poorly in localising the onset of M/EEG effects [e.g., @rousselet_using_2025; @sassenhagen2019].

To sum up, the main limitation of cluster-based inference is that it allows for inference at the cluster level only, not allowing inference at the level of timesteps, sensors, etc. As a consequence, it does not allow inferring the precise spatial and temporal localisation of effects. In the following, we briefly review previous modelling work of M/EEG data. Then, we provide a short introduction to generalised additive models (GAMs) to illustrate how these models can be used to precisely estimate the onset and offset of M/EEG effects.

## Previous work on modelling M/EEG data

<!--

Weighting single trials [@pernet2022]... From @dimigen2021: Recently, spline regression has been applied to ERPs [e.g., @hendrix2017; @kryuchkova2012]... 

M/EEG recordings at the scalp measure activity stemming from various parts of the brain and contain many artefacts not related to the cognitive and neural processes of interest. Therefore, analysing M/EEG data requires methods for separating the parts of the signals that are related to cognitive and neural processes and the parts of the signals that contain task-unrelated "noise". The most widely-used method for achieving this separation consists in estimating the event-related potential (ERP), defined as the brain response elicited by some specific time-locked sensory, cognitive, or motor event. Most often, the ERP is computed as the average response over trials computed from epochs time-locked on the event of interest (e.g., stimulus onset). However, this simple averaging approach comes with several limitations, such as being limited to simple categorical designs.

-->

Scalp-recorded M/EEG signals capture neural activity originating from various brain regions and are often contaminated by artifacts unrelated to the cognitive processes under investigation. Consequently, analysing M/EEG data necessitates methods that can disentangle task-relevant neural signals from extraneous "noise." A widely adopted technique for this purpose is the estimation of event-related potentials (ERPs), which are stereotyped electrophysiological responses time-locked to specific sensory, cognitive, or motor events. Typically, ERPs are derived by averaging EEG or MEG epochs across multiple trials aligned to the event of interest (e.g., stimulus onset), thereby enhancing the signal-to-noise ratio by attenuating non-time-locked activity. However, this averaging approach has notable limitations: it assumes consistent latency and amplitude across trials and is primarily suited for simple categorical designs. Such assumptions may not hold in more complex experimental paradigms, potentially leading to suboptimal ERP estimations [e.g., @smith2014a].

To overcome the limitations of simple averaging, several model-based approaches for estimating ERPs have been proposed. These methods are motivated by the observation that traditional ERP averaging is mathematically equivalent to fitting an intercept-only linear regression model in a simple categorical design without overlapping events [@smith2014a]. In contrast to simple averaging, regression-based approaches to ERP estimation offer substantially greater flexibility. Notably, they allow for the modelling of both linear and nonlinear effects of continuous predictors, such as word frequency or age [e.g., @smith2014a; @smith2014b; @tremblay2014], and enable the disentangling of overlapping cognitive processes [e.g., @ehinger_unfold_2019; @skukies_brain_2024; @skukies_modelling_2021]. One widely used implementation of this approach is provided by the LIMO EEG toolbox [@pernet2011], which follows a multi-stage analysis pipeline. First, a separate regression model is fit for each datapoint (e.g., each time point and electrode) at the individual participant level to estimate ERP responses. This is followed by group-level statistical analyses of the resulting regression coefficients, often accompanied by corrections for multiple comparisons or cluster-based inference [for recent applied examples, see @dunagan2025; @wüllhorst2025].

Although this framework allows for the inclusion of a wide range of predictors--both continuous and categorical, linear and nonlinear--it still has important limitations. First, fitting separate models for each datapoint ignores the spatiotemporal dependencies inherent in M/EEG data, potentially reducing statistical power and interpretability. Second, the subsequent group-level analyses typically do not account for hierarchical dependencies which could otherwise be addressed through multilevel modelling. Finally, because the output of this procedure is summarised by cluster-based inference, its conclusions remain subject to the limitations discussed in the previous section.

Beyond modelling nonlinear effects of continuous predictors on ERP amplitudes, GAMs have been employed to capture the temporal dynamics of ERPs themselves, effectively modelling the shape of the waveform over time [@abugaber2023; @baayen2018; @meulman2015; @meulman2023]. This approach allows for the estimation of smooth, data-driven functions that characterise how neural responses evolve over time, offering a flexible alternative to traditional linear models. In the following section, we provide a brief introduction to GAMs, highlighting their applicability to M/EEG time series analysis and the advantages they offer over conventional methods.

## Generalised additive models

In generalised additive models, the functional relationship between the predictors and the response variable is decomposed into a sum of low-dimensional non-parametric functions. A typical GAM has the following form:

$$
\begin{aligned} 
y_{i} &\sim \mathrm{EF}\left(\mu_{i}, \phi\right)\\
g\left(\mu_i\right) &= \underbrace{\mathbf{A}_{i} \gamma}_{\text{parametric part}} + \underbrace{\sum_{j=1}^{J} f_{j}\left(x_{ij}\right)}_{\text{non-parametric part}}
\end{aligned}
$$

where $y_{i} \sim \mathrm{EF}\left(\mu_{i}, \phi\right)$ denotes that the observations $y_{i}$ are distributed as some member of the exponential family of distributions (e.g., Gaussian, Gamma, Beta, Poisson) with mean $\mu_{i}$ and scale parameter $\phi$; $g(\cdot)$ is the link function, $\mathbf{A}_{i}$ is the $i$th row of a known parametric model matrix, $\gamma$ is a vector of parameters for the parametric terms (to be estimated), $f_{j}$ is a smooth function of covariate $x_{j}$ (to be estimated as well). The smooth functions $f_{j}$ are represented in the model as a weighted sum of $K$ simpler, basis functions:

$$
f_{j}\left(x_{i j}\right) = \sum_{k=1}^K \beta_{jk} b_{jk}\left(x_{ij}\right)
$$

where $\beta_{jk}$ is the weight (coefficient) associated with the $k$th basis function $b_{jk}()$ evaluated at the covariate value $x_{ij}$ for the $j$th smooth function $f_{j}$. To clarify the terminology at this point: *splines* are functions composed of simpler functions. These simpler functions are called *basis functions* (e.g., cubic polynomial, thin-plate) and the set of basis functions is called a *basis*. Each basis function is weighted by its coefficient and the resultant spline is the sum of these weighted basis functions (@fig-intro-gam\ignorespaces A). Splines coefficients are penalised (usually through the square of the smooth functions' second derivative) in a way that can be interpreted, in Bayesian terms, as a prior on the "wiggliness" of the function [@miller2025; @wood2017]. In other words, more complex (wiggly) basis functions are automatically penalised.

<!--

A detailed discussion of the technical aspects of smooth functions is beyond the scope of this article (see Wood, 2017a); or (Baayen, Vasishth, Kliegl, & Bates, 2017; van Rij, Vaci, Wurm, & Feldman, in press; Wieling, 2018, for an introduction), but in the context of this work, it can be thought of as a continuous, potentially wiggly but not abruptly changing line that is expressed over time.

Generalised additive models can be understood in a Bayesian framework [@miller2025; @wood2017]...

## Bayesian generalised additive multilevel models

There are debates among Bayesian practitioners as to whether prior distributions should encore subjective (personal) beliefs or... but these debates are outside the scope of the present paper and we therefore the interested reader to dedicated work (e.g., XX; YY)... In practice, weakly informative priors are often used as default priors in situations in which subjective priors are difficult to define/elicit... 

The Bayesian approach to statistical modelling is characterised by its reliance on probability theory to quantify uncertainty about parameter estimates [e.g., @gelman2020]. In this framework, all unknown entities are assigned probability distributions reflecting the uncertainty... These probability distributions are commonly referred to as "priors" and represent some state of knowledge about unknown quantities before seeing any data. Bayesian models are then fitted on empirical (actual or simulated) data to update prior states of knowledge to posterior states of knowledge using Bayes theorem, or in practise, sampling-based approximations of the posterior distribution...

-->

```{r fig-intro-gam, fig.width = 9, fig.cap = "Different types of GAMs. **A**: GAMs predictions are computed as the weigthed sum (in black) of basis functions (here thin-plate basis functions, in grey). **B**: Constant-effect GAM, with 5 participants in colours and the group-level prediction in black. **C**: Varying-intercept + varying-slope GAMM (with constant smoother). **D**: Varying-intercept + varying-slope + varying-smoother GAMM. In this model, each participant gets its own intercept, slope, and degree of 'wiggliness' (smoother)."}
########################
# simulating some data #
########################

set.seed(666)
n_subjects <- 5
n_per_subject <- 50
subject <- rep(paste0("S", 1:n_subjects), each = n_per_subject)
x <- rep(seq(0, 10, length.out = n_per_subject), times = n_subjects)

# subject-level varying effects
intercepts <- rnorm(n_subjects, 0, 1)
slopes <- rnorm(n_subjects, 0, 0.3)
wiggles <- rnorm(n_subjects, 0, 0.2)
subject_df <- tibble(
    subject = paste0("S", 1:n_subjects),
    intercept = intercepts,
    slope = slopes,
    wiggle = wiggles
    )

df <- tibble(x = x, subject = subject) %>%
    left_join(subject_df, by = "subject") %>%
    mutate(
        subject = as.factor(subject),
        y = sin(x) + intercept + slope * x + wiggle * sin(2 * x) + rnorm(n(), sd = 0.3)
        )

######################
# fitting the models #
######################

# fixed-effect GAM
gam_fixed <- gam(
    y ~ s(x),
    data = df
    )

# random intercept + slope
gam_int_slope <- gam(
    y ~ s(x) +
        s(subject, bs = "re") +
        s(x, subject, bs = "re"),
    data = df
    )

# random smooth per subject + global smooth
gam_rand_smooth <- gam(
    y ~ s(x) +
        s(subject, bs = "re") +
        s(x, subject, bs = "fs", m = 1),
    data = df
    )

# defining plotting functions
plot_basis_functions <- function (df, model) {
    
    # retrieving design matrix and coefficients
    Xp <- predict(model, type = "lpmatrix")
    coefs <- coef(model)
    
    # basis dataframe
    basis_df <- as.data.frame(Xp)
    colnames(basis_df) <- paste0("b", 1:ncol(basis_df) )
    basis_df$x <- df$x
    basis_long <- pivot_longer(basis_df, starts_with("b"), names_to = "basis", values_to = "value")
    
    # matching basis to weights
    weights <- data.frame(
        basis = paste0("b", seq_along(coefs) ),
        weight = coefs
        ) %>%
        mutate(label = paste0(basis, " × ", sprintf("%.2f", weight) ) )

    # adding labels to the end of each basis line
    label_df <- basis_long %>%
        group_by(basis) %>%
        slice_tail(n = 1) %>% # get last x point per basis
        left_join(weights, by = "basis") %>%
        mutate(label = paste0(basis, " × ", sprintf("%.2f", weight) ) )
  
    # fitted curve at the group level
    df$fit <- as.vector(Xp %*% coefs)
    
    # plotting everything
    ggplot() +
        geom_hline(yintercept = 0, linetype = 2) +
        geom_line(
            # data = basis_long, aes(x = x, y = value, color = basis),
            data = basis_long, aes(x = x, y = value, group = basis),
            colour = "grey50",
            alpha = 0.5,
            linewidth = 0.5
            ) +
        geom_line(
            data = df,
            aes(x = x, y = fit),
            color = "black",
            linewidth = 1.0
            ) +
        # geom_text(
        #     data = label_df,
        #     aes(x = x, y = value, label = label, color = basis),
        #     hjust = 0, nudge_x = 0.1, size = 2
        #     ) +
        # ggrepel::geom_text_repel(
        #     data = label_df,
        #     aes(x = x, y = value, label = label, color = basis),
        #     hjust = 0, nudge_x = 1,
        #     size = 2, direction = "y"
        #     ) +
        labs(x = "Predictor", y = "Response") +
        ylim(-2, +2) +
        theme(legend.position = "none")
    
    }

plot_model <- function (df, model) {
    
    # predictions fo the full model: includes all terms (fixed + random)
    df$fit_subject <- predict(model, newdata = df)
    
    # identify which smooth terms to exclude (those involving 'subject')
    smooth_labels <- sapply(model$smooth, function(s) s$label)
    subject_terms <- smooth_labels[grepl("subject", smooth_labels)]
    
    # group-level prediction + standard error (excludes subject-specific terms)
    pred_global <- predict(model, newdata = df, exclude = subject_terms, se.fit = TRUE)
    df$fit_global <- pred_global$fit
    df$fit_global_upper <- pred_global$fit + pred_global$se.fit
    df$fit_global_lower <- pred_global$fit - pred_global$se.fit
    
    # plotting the predictions
    df %>%
        ggplot(aes(x = x, y = y, colour = subject) ) +
        geom_hline(yintercept = 0, linetype = 2) +
        geom_point(
            size = 0.5,
            show.legend = FALSE
            ) +
        geom_line(
            aes(y = fit_subject, group = subject),
            alpha = 0.5,
            linewidth = 0.5,
            show.legend = FALSE
            ) +
        geom_ribbon(
            aes(x = x, y = y, ymin = fit_global_lower, ymax = fit_global_upper),
            fill = "black",
            alpha = 0.2,
            inherit.aes = FALSE
            ) +
        geom_line(
            aes(y = fit_global, group = 1),
            color = "black",
            linewidth = 1
            ) +
        ylim(-5, 5) +
        scale_colour_manual(values = met.brewer(name = "Johnson", n = n_subjects) ) +
        labs(x = "Predictor", y = "Response")
  
}

# plotting, note that the basis plot uses the fixed-effect model for simplicity
# df2 <- df %>% filter(subject %in% c(unique(df$subject)[1:4]) )
p1 <- plot_basis_functions(df, gam_fixed)
p2 <- plot_model(df, gam_fixed)
p3 <- plot_model(df, gam_int_slope)
p4 <- plot_model(df, gam_rand_smooth)

# combining plots with labels
(p1 | p2) / (p3 | p4) + plot_annotation(tag_levels = "A")
```

<!--

Or (Baayen, Vasishth, Kliegl, & Bates, 2017; van Rij, Vaci, Wurm, & Feldman, in press; Wieling, 2018, for an introduction), but in the context of this work, it can be thought of as a continuous, potentially wiggly but not abruptly changing line that is expressed over time.

This penalises complexity automatically [@meulman2023; @wieling2018]... ERP itself [@abugaber2023; @baayen2018; @meulman2015; @meulman2023]... See for instance these tutorials [@sóskuthy2017; @winter2016] or application to phonetic data [@wieling2018; @sóskuthy2021]... or this introduction [@baayen2020] or these reference books [@hastie2017; @wood2017]...

A detailed discussion of the technical aspects of GAMs is beyond the scope of this article [see reference books such as @hastie2017; @wood2017] but it should be noted that GAMs have been successfully been applied to various types of timeseries data throughout the cognitive sciences. For instance, it has been applied to pupillometry [e.g., @vanrij2019], articulography [e.g., @wieling2018], formant contours [e.g., @sóskuthy2021], neuroimaging data [e.g., @dinga2021] and ERPs [e.g., @abugaber2023; @baayen2018; @meulman2015; @meulman2023]. The appeal of GAMs for modelling surface (sensor-level) M/EEG data comes from their ability to deal with the complex shape of the ERP without overfitting it. Generalising to scale and shape or "distributional GAMs" [@rigby2005; @umlauf2018], allowing for the specification of linear predictors for both the shape and scale parameters of many distributions [for an application to neuroimaging data, see @dinga2021]...

Introduction to multilevel GAMs [@pedersen_hierarchical_2019]... Proper inclusion of varying/random effects in the model specification protects against overly wiggly curves [@baayen2020]... @fig-intro-gam\ignorespaces D... Instead of averaging, we obtain the smooth ERP signal from multilevel GAM... which has been shown to be less susceptible to outliers [@meulman2023]...

-->

A detailed treatment of the technical underpinnings of GAMs is beyond the scope of this article [see reference books such as @hastie2017; @wood2017]. However, it is worth emphasising that GAMs have been successfully applied to a wide range of time series data across the cognitive sciences, including pupillometry [e.g., @vanrij2019], articulography [e.g., @wieling2018], speech formant dynamics [e.g., @sóskuthy2021], neuroimaging data [e.g., @dinga2021], and event-related potentials [e.g., @abugaber2023; @baayen2018; @meulman2015; @meulman2023]. Their appeal for modelling M/EEG data lies in their ability to flexibly capture the complex shape of ERP waveforms without overfitting, through the use of smooth functions constrained by penalisation. Recent extensions, such as distributional GAMs [@rigby2005; @umlauf2018], allow researchers to model not only the mean structure but also the variance (or scale) and other distributional properties as functions of predictors, a feature that has proven useful in modelling neuroimaging data [e.g., @dinga2021]. Moreover, hierarchical or multilevel GAMs [@pedersen_hierarchical_2019] provide a principled way to account for the nested structure of M/EEG data (e.g., trials within participants), enabling the inclusion of varying intercepts, slopes, and smoothers (as illustrated in @fig-intro-gam\ignorespaces C-D). This approach mitigates the risk of overfitting and reduces the influence of outliers on smooth estimates [@baayen2020; @meulman2023].

## Objectives

<!--

Recently, @teichmann2022 provided a detailed tutorial on using Bayes factors (BFs) to analyse the 1D or 2D output from MVPA, that is, for testing, at every timestep, whether decoding performance is above chance level. However, this approach provides timeseries of BFs that ignores temporal dependencies... In the following...

As put by @rousselet_using_2025, concluding on the onset of effect based on a series of univariate tests... commits to three fallacies... here, we want to avoid these by introducing a model-based approach, which naturally take into account the temporal dependencies in the data to output a series of posterior probabilities...

Given the previously reported limitations of cluster-based inference to precisely identify the onset and offset of M/EEG effects (e.g., ERPs, decoding performance), we developed a model-based approach for estimating the onset and offset of M/EEG effects. To achieve this, we leveraged Bayesian generalised additive multilevel models fitted in `R` via the `brms` package and compared the performance of this approach to conventional methods on both simulated and actual M/EEG data.

-->

Cluster-based permutation tests are widely used in M/EEG research to identify statistically significant effects across time and space. However, these methods have notable limitations, particularly in accurately determining the precise onset and offset of neural effects. To address these limitations, we developed a model-based approach relying on Bayesian generalised additive multilevel models implemented in `R` via the `brms` package [@brms2017; @brms2018]. We evaluated the performance of this approach against conventional methods using both simulated and actual M/EEG data. Our findings demonstrate that this method provides more precise and reliable estimates of effects' onset and offset than conventional approaches such as cluster-based inference.

# Benchmarking with known ground truth

## Methods

### M/EEG data simulation

To assess the accuracy of group-level onset and offset estimation of our proposed method, we simulated EEG with known onset and offset values. Following the approach of @sassenhagen2019 and @rousselet_using_2025, we simulated EEG data stemming from two conditions, one with noise only, and the other with noise + signal. As in previous studies, the noise was generated by superimposing 50 sinusoids at different frequencies, following an EEG-like spectrum [see code in the online supplementary materials and details in @yeung2004]. As in @rousselet_using_2025, the signal was generated from a truncated Gaussian distribution with an objective onset at 160 ms, a peak at 250 ms, and an offset at 342 ms. We simulated this signal for 250 timesteps between 0 and 0.5s, akin to a 500 Hz sampling rate. We simulated data for a group of 20 participants (with variable true onset) with 50 trials per participant and condition (@fig-eeg). All figures and simulation results can be reproduced using the `R` code available online at: <https://github.com/lnalborczyk/brms_meeg>.

```{r fig-eeg, echo = FALSE, fig.width = 9, fig.cap = "Mean simulated EEG activity in two conditions with 50 trials each, for a group of 20 participants. The error band represents the mean +/- 1 standard error of the mean."}
# setting the seed for reproducible results
set.seed(111)

# importing R version of Matlab code from Yeung et al. (2004)
source("code/eeg_noise.R")

# importing the ERP template with true onset = 160 ms, F=81, and max at F=126
source("code/erp_template.R")

# EEG power information to use with the eeg_noise() function
meanpower <- unlist(read.table("code/meanpower.txt") )

# defining simulation parameters
n_trials <- 50 # number of trials
n_ppt <- 20 # number of participants
outvar <- 1 # noise variance
srate <- 500 # sampling rate in Hz
ronset <- seq(from = 150, to = 170, by = 2) # random onset for each participant

# removing raw_df if it already exists
if (exists("raw_df") ) rm(raw_df)

for (P in 1:n_ppt) { # for each participant
    
    # get random onset
    ponset <- sample(ronset, 1)
     
    # find starting point
    st <- which(Xf == ponset)
    
    # pad vector
    temp2 <- c(rep(0, st - 2), erp, rep(0, Nf - st - length(erp) + 2) )
    
    # initialising empty conditions
    cond1 <- matrix(0, nrow = n_trials, ncol = Nf)
    cond2 <- matrix(0, nrow = n_trials, ncol = Nf)
    
    for (T in 1:n_trials) { # for each trial
        
        cond1[T, ] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
        cond2[T, ] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
      
    }
    
    # converting results to dataframe
    temp_df <- data.frame(
        x = rep(Xf, 2*nrow(cond1) ),
        y = c(c(t(cond1) ), c(t(cond2) ) ),
        trial = c(rep(1:n_trials, each = length(Xf) ), rep(1:n_trials, each = length(Xf) ) ),
        condition = factor(rep(c("cond1", "cond2"), each = Nf * n_trials) ),
        participant = paste0("participant_", sprintf("%02d", P) )
        ) %>%
        select(participant, condition, trial, x, y)
    
    # and appending it to previous participants
    if (exists("raw_df") ) {
        
        raw_df <- bind_rows(raw_df, temp_df)
        
    } else {
        
        raw_df <- temp_df
        
    }
    
}

# converting time from ms to seconds
raw_df <- raw_df %>%
    # converting time from ms to seconds
    mutate(x = x / 1000) %>%
    # renaming columns
    rename(time = x, eeg = y)

# defining the true onset and offset in seconds
true_onset <- 0.160
true_peak <- 0.250
true_offset <- 0.342

# plotting the data
raw_df %>%
    summarise(
        eeg_mean = mean(eeg),
        eeg_se = sd(eeg) / sqrt(n() ),
        .by = c(participant, condition, time),
        ) %>%
    ggplot(aes(x = time, y = eeg_mean, colour = condition, fill = condition) ) +
    geom_hline(yintercept = 0, linetype = 2) +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_ribbon(
        aes(ymin = eeg_mean - eeg_se, ymax = eeg_mean + eeg_se, colour = NULL),
        alpha = 0.3, show.legend = FALSE
        ) +
    geom_line(show.legend = FALSE) +
    facet_wrap(~participant) +
    scale_colour_met_d(name = "Johnson") +
    labs(x = "Time (s)", y = "Simulated EEG signal (a.u.)")
```

### Model description and model fitting

We then fitted a Bayesian GAM (BGAM) using the `brms` package [@brms2017; @brms2018] and default priors (i.e., weakly informative priors). We ran eight Markov Chain Monte-Carlo (MCMC) to approximate the posterior distribution, including each 5000 iterations and a warmup of 2000 iterations, yielding a total of $8 \times (5000-2000) = 24000$ posterior samples to use for inference. Posterior convergence was assessed examining trace plots as well as the Gelman–Rubin statistic $\hat{R}$ [@gabry2019; @gelman2020]. The `brms` package uses the same syntax as `r pkrt("mgcv")` for specifying smooth effects. @fig-plot-post-slope shows the predictions of this model together with the raw data.

```{r gam, results = "hide"}
# computing the average EEG signals for each participant
ppt_df <- raw_df %>%
    group_by(participant, condition, time) %>%
    summarise(eeg_mean = mean(eeg) ) %>%
    ungroup()

# defining a contrast for condition
contrasts(ppt_df$condition) <- c(-0.5, 0.5)

# fitting the BGAM
gam <- brm(
    # thin-plate regression splines with k-1 basis functions
    eeg_mean ~ 1 + condition + s(time, bs = "tp", k = 20, by = condition),
    data = ppt_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 8,
    cores = 8,
    file = "models/gam.rds"
    )
```

However, the previous model only included constant (fixed) effects, thus not properly accounting for between-participant variability. We next fitted a multilevel version of the BGAM [BGAMM, for an introduction to Bayesian multilevel models in `brms`, see @nalborczyk2019] including a varying intercept and slope for participant (but with a constant smoother). Although it is possible to fit a BGAMM using data at the single-trial level, we present a computationally lighter version of the model that is fitted directly on by-participant summary statistics (mean and SD), similar to what is done in meta-analysis.

```{r meta-gam, results = "hide"}
# computing the mean and SD of EEG signals for each participant
summary_df <- raw_df %>%
    group_by(participant, condition, time) %>%
    summarise(eeg_mean = mean(eeg), eeg_sd = sd(eeg) ) %>%
    ungroup()

# defining a contrast for condition
contrasts(summary_df$condition) <- c(-0.5, 0.5)

# fitting the BGAMM
meta_gam <- brm(
    # using the by-participant mean and SD of ERPs
    eeg_mean | se(eeg_sd) ~
        1 + condition + (1 + condition | participant) +
        s(time, bs = "tp", k = 20, by = condition),
    data = summary_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 8,
    cores = 8,
    file = "models/meta_gam.rds"
    )
```

```{r meta-gamm, echo = FALSE, eval = FALSE}
# fitting the BGAMM
# gamm <- brm(
#     # thin-plate regression splines with k-1 basis functions
#     eeg_mean ~ 1 + condition + (1 + condition | participant) + s(time, bs = "tp", k = 20, by = condition),
#     data = ppt_df,
#     family = gaussian(),
#     warmup = 2000,
#     iter = 5000,
#     chains = 8,
#     cores = 8,
#     file = "models/gamm.rds"
#     )

# fitting the BGAMM
# meta_gam_k10 <- brm(
#     # using the by-participant SD of ERPs
#     eeg_mean | se(eeg_sd) ~
#         1 + condition + (1 + condition | participant) +
#         s(time, bs = "tp", k = 10, by = condition),
#     data = summary_df,
#     family = gaussian(),
#     warmup = 2000,
#     iter = 5000,
#     chains = 8,
#     cores = 8,
#     file = "models/meta_gam_k10.rds"
#     )

# comparing the two models
# loo(meta_gam, meta_gam_k10)
# library(easystats)
# model_comp <- compare_performance(meta_gam, meta_gam_k10)
# model_comp
# plot(model_comp)
```

We depict the posterior predictions together with the posterior estimate of the slope for `condition` at each timestep (@fig-plot-post-slope). This figure suggests that the BGAMM provides an adequate description of the simulated data (see further posterior predictive checks in @apx-basis).

```{r fig-plot-post-slope, echo = FALSE, fig.width = 8, fig.asp = 0.5, fig.cap = "Posterior estimate of the EEG activity in each condition (left) and posterior estimate of the difference in EEG activity (right) according to the BGAMM."}
# defining a function to plot posterior predictions
plot_post_preds <- function (model, data = NULL) {
    
    if (is.null(data) ) data <- model$data
    
    post_preds <- conditional_effects(
        x = model,
        effect = "time:condition",
        method = "posterior_epred",
        re_formula = NULL,
        prob = 0.95
        )[[1]]
  
    post_preds %>%
        ggplot(
            aes(
                x = time,
                y = eeg_mean,
                colour = condition, fill = condition
                )
            ) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_hline(yintercept = 0, linetype = 2) +
        geom_point(
            data = data %>% summarise(eeg_mean = mean(eeg_mean), .by = c(time, condition) ),
            aes(fill = NULL),
            pch = 21, show.legend = FALSE
            ) +
        geom_line(aes(y = estimate__), show.legend = FALSE) +
        geom_ribbon(
            aes(ymin = lower__, ymax = upper__, colour = NULL),
            alpha = 0.25, show.legend = FALSE
            ) +
        scale_colour_met_d(name = "Johnson") +
        scale_fill_met_d(name = "Johnson") +
        labs(x = "Time (s)", y = "EEG signal (a.u.)")
    
}

# defining a function to plot posterior slope
plot_post_slope <- function (model, data = NULL) {
    
    # if no data is specified, use model$data
    if (is.null(data) ) data <- model$data
    
    # computing the posterior probability for the slope over time
    post_prob_slope <- model$data %>%
        add_epred_draws(object = model) %>%
        data.frame() %>%
        dplyr::select(participant, time, condition, .epred, .draw) %>%
        pivot_wider(names_from = condition, values_from = .epred) %>%
        mutate(epred_diff = cond2 - cond1) %>%
        # computing mean posterior probability at the group level
        group_by(time) %>%
        summarise(
            post_prob = quantile(x = epred_diff, probs = 0.5),
            lower = quantile(x = epred_diff, probs = 0.025),
            upper = quantile(x = epred_diff, probs = 0.975)
            ) %>%
        ungroup()
    
    # plotting it
    post_prob_slope %>%
        ggplot(aes(x = time, y = post_prob) ) +
        geom_hline(yintercept = 0, linetype = 2) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
        geom_line() +
        labs(
            x = "Time (s)",
            y = expression(p(beta~"|"~data, model) )
            )
  
}

# plotting it
plot_post_preds(model = meta_gam) + plot_post_slope(model = meta_gam)
```

We then compute the posterior probability of the slope for `condition` being above $0$ (@fig-post-prob-ratio, left). From this quantity, we compute the ratio of posterior probabilities (i.e., $p/(1-p)$), or posterior odds, and visualise the timecourse of these odds superimposed with the conventional thresholds on evidence ratios (@fig-post-prob-ratio, right). A ratio of 10 means that the probability of the difference being above 0 is 10 times higher than the probability of the difference not being above 0, given the data, the priors, and other model's assumptions.[^1] Thresholding the posterior odds thus provides a model-based approach for estimating the onset and offset of M/EEG effects, whose properties will be assessed in the simulation study. An important advantage is that the proposed approach can be extended to virtually any model structure.

[^1]: These posterior odds are equivalent to a Bayes factor, assuming 1:1 prior odds.

```{r tests2, echo = FALSE, eval = FALSE}
# compute percentage in ROPE
library(easystats)
# rope(
#     x = gam,
#     range = "default",
#     ci = 0.95,
#     ci_method = "ETI",
#     effects = "fixed",
#     # component = c("conditional", "zi", "zero_inflated", "all"),
#     # parameters = NULL,
#     verbose = TRUE
#     )
# rope_range(x = gam)

# retrieving posterior draws for EEG over time
# gam <- full_model
# post_samples <- posterior_epred(object = gam, newdata = ppt_df, allow_new_levels = TRUE)
gam <- meta_gam
post_samples <- posterior_epred(object = gam, newdata = summary_df, allow_new_levels = TRUE)

# defining the ROPE as baseline SD
# rope_range <- c(-0.1, 0.1)
baseline_sd <- summary_df %>%
    pivot_wider(names_from = condition, values_from = eeg_mean) %>%
    mutate(cond_diff = abs(cond1 - cond2) ) %>%
    filter(time < 0.1) %>%
    # head()
    summarise(baseline_sd = sd(cond_diff) ) %>%
    data.frame()

rope_range <- c(-baseline_sd, +baseline_sd)

# computing the HDI for each time step
hdi_values <- apply(post_samples, 2, function (x) hdi(x, ci = 0.95) )

# computing the  proportion of the HDI that falls within the ROPE
prop_in_rope <- sapply(1:ncol(post_samples), function (i) {
    lower_hdi <- hdi_values[[i]][1]
    upper_hdi <- hdi_values[[i]][2]
    # computing the  proportion inside ROPE
    mean(lower_hdi >= rope_range[1] & upper_hdi <= rope_range[2])
    })

# storing the results in a dataframe
# rope_results <- data.frame(time = ppt_df$time, prop_in_rope = prop_in_rope)
rope_results <- data.frame(time = summary_df$time, prop_in_rope = prop_in_rope)

# plotting the results
rope_results %>%
    summarise(prop_in_rope = mean(prop_in_rope), .by = time) %>%
    ggplot(aes(x = time, y = prop_in_rope) ) +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_point()

# loading or fitting the full model
# full_model <- gam
# full_model <- readRDS("models/gam.rds")
full_model <- meta_gam

# fitting the null model (no 'condition' effect)
# null_model <- brm(
#     eeg ~ s(time, bs = "tp", k = 20),
#     data = ppt_df,
#     family = gaussian(),
#     warmup = 2000,
#     iter = 5000,
#     chains = 8,
#     cores = 8,
#     file = "models/null_gam.rds"
#     )

# fitting the BGAMM
null_model <- brm(
    # using by-participant SD of ERPs across trials
    eeg_mean | se(eeg_sd) ~
        1 + (1 | participant) +
        s(time, bs = "tp", k = 20),
    data = summary_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 8,
    cores = 8
    # file = "models/meta_gamm.rds"
    )

# computing the marginal likelihoods using bridge sampling
# library(bridgesampling)
# full_bridge <- bridge_sampler(full_model)
# null_bridge <- bridge_sampler(null_model)

# computing the Bayes Factor in favor of full model
# BF_10 <- bf(full_bridge, null_bridge)
# print(BF_10)

# extracting log-likelihoods for each model
log_lik_full <- log_lik(full_model)
log_lik_null <- log_lik(null_model)

# computing the pointwise log pointwise predictive density (LPD)
# averaging the log-likelihood across posterior draws
lpd_full <- colMeans(log_lik_full)
lpd_null <- colMeans(log_lik_null)

# computing the LPD difference (full model - null model)
lpd_diff <- lpd_full - lpd_null

# computing pointwise Bayes factors as exp(lpd_diff)
BF_10_pointwise <- exp(lpd_diff)

# storing the results in a dataframe
ppt_df <- full_model$data
lpd_results <- data.frame(time = ppt_df$time, LPD_diff = lpd_diff, BF_10 = BF_10_pointwise)

# viewing the results
head(lpd_results)

# plotting it
lpd_results %>%
    # pivot_longer(cols = LPD_diff:BF_10) %>%
    summarise(BF_10 = mean(LPD_diff), .by = time) %>%
    # summarise(BF_10 = mean(BF_10), .by = time) %>%
    # ggplot(aes(x = time, y = value, colour = name) ) +
    ggplot(aes(x = time, y = BF_10) ) +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_line() +
    geom_point()
```

```{r fig-post-prob-ratio, echo = FALSE, out.width = "100%", fig.width = 9, fig.asp = 0.5, fig.cap = "Left: Posterior probability of the EEG difference (slope) being above 0 according to the BGAMM. Right: Posterior odds according to the BGAMM (on a log10 scale). Timesteps above threshold (10) are highlighted in green. NB: the minimum and maximum possible posterior odds are determined (bounded) by the number of posterior samples in the model."}
# defining a function to compute and plot the posterior probability ratio
plot_post_test_ratio <- function (model, sesoi = 0, threshold = 10) {
    
    n_posterior_samples <- ndraws(model)
    
    prob_y_above_0 <- model$data %>%
        add_epred_draws(object = model) %>%
        data.frame() %>%
        dplyr::select(participant, time, condition, .epred, .draw) %>%
        pivot_wider(names_from = condition, values_from = .epred) %>%
        mutate(epred_diff = cond2 - cond1) %>%
        # computing mean posterior probability at the group level
        group_by(time) %>%
        summarise(m = mean(epred_diff > 0 + sesoi) ) %>%
        mutate(prob_ratio = m / (1 - m) ) %>%
        ungroup() %>%
        # ensuring there is no 0 or +Inf values
        # mutate(prob_ratio = ifelse(is.infinite(prob_ratio), n_posterior_samples, prob_ratio) ) %>%
        mutate(
            prob_ratio = ifelse(
                prob_ratio > n_posterior_samples, n_posterior_samples, prob_ratio
                )
            ) %>%
        mutate(prob_ratio = ifelse(prob_ratio == 0, 1 / n_posterior_samples, prob_ratio) )
        
    # plotting it
    post_plot <- prob_y_above_0 %>%
        ggplot(aes(x = time, y = m) ) +
        geom_hline(yintercept = 0.5, linetype = 2) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line() +
        ylim(c(0, 1) ) +
        # labs(x = "Time (s)", y = expression(Pr(beta>0~"|"~data, model) ) )
        labs(x = "Time (s)", y = expression(p(beta>0~"|"~data) ) )
    
    # plotting it
    ratio_plot <- prob_y_above_0 %>%
        mutate(above_thres = ifelse(prob_ratio >= threshold, 1, NA) ) %>%
        ggplot(aes(x = time, y = prob_ratio) ) +
        geom_hline(yintercept = 1, linetype = "dashed") +
        geom_hline(yintercept = 10, linetype = "dashed", color = "darkred") +
        geom_hline(yintercept = 1/10, linetype = "dashed", color = "darkred") +
        geom_hline(yintercept = 100, linetype = "dashed", color = "orangered") +
        geom_hline(yintercept = 1/100, linetype = "dashed", color = "orangered") +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line(linewidth = 0.8) +
        geom_point(
            data = . %>% dplyr::filter(!is.na(above_thres) ),
            aes(y = threshold),
            colour = "darkgreen",
            shape = 16,
            size = 2,
            na.rm = TRUE
            ) +
        scale_y_log10(
            labels = label_log(digits = 2),
            limits = c(1 / n_posterior_samples, n_posterior_samples),
            breaks = c(1/1e4, 1/1e3, 1/1e2, 1/10, 1, 10, 1e2, 1e3, 1e4)
            ) +
        labs(
            x = "Time (s)",
            y = expression(p(beta>0~"|"~data) / (1 - p(beta>0~"|"~data) ) )
            )
    
    # plotting both the posterior probability and the ratio...
    post_plot + ratio_plot
  
}

# plotting the meta_gam predictions
plot_post_test_ratio(model = meta_gam, threshold = 10)
```

```{r fig-bgamm-varying-effects, eval = FALSE, echo = FALSE, fig.width = 8, fig.asp = 0.5, fig.cap = "Posterior estimate of the EEG activity in each condition (left) and posterior estimate of the difference in EEG activity (right) according to the BGAMM."}
####################################################################
# TO-DO: plotting the onset/offset estimate per participant?
# see https://discourse.mc-stan.org/t/specifying-varying-effect-structures-with-hierarchical-generalised-additive-models-gams/35596
#################################################################

# fixed-effect GAM
gam_fixed <- gam(
    y ~ s(x),
    data = df
    )

# random intercept + slope
gam_int_slope <- gam(
    y ~ s(x) +
        s(subject, bs = "re") +
        s(x, subject, bs = "re"),
    data = df
    )

# random smooth per subject + global smooth
gam_rand_smooth <- gam(
    y ~ s(x) +
        s(subject, bs = "re") +
        s(x, subject, bs = "fs", m = 1),
    data = df
    )

# fitting the BGAMM
# see https://discourse.mc-stan.org/t/specifying-varying-effect-structures-with-hierarchical-generalised-additive-models-gams/35596/2
# tic()
# 5519.743 sec elapsed
full_meta_gamm <- brm(
    # using by-participant SD of ERPs across trials
    eeg_mean | se(eeg_sd) ~ 1 + condition +
        s(time, by = condition, bs = "tp", k = 20) +
        s(time, participant, by = condition, bs = "fs", xt = list(bs = "tp") ),
    data = summary_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 8,
    cores = 8,
    control = list(adapt_delta = 0.95)
    # file = "models/full_meta_gamm.rds"
    )
# toc()

# model <- meta_gamm
# plot_post_preds(model = meta_gamm) + plot_post_slope(model = meta_gamm)
# model <- full_meta_gamm
# plot_post_preds(model = full_meta_gamm) + plot_post_slope(model = full_meta_gamm)
# model <- meta_gam
# model <- gamm

plot_post_test_ratio_individual <- function (model, sesoi = 0, threshold = 10) {

    # retrieving the number of posterior samples
    n_posterior_samples <- ndraws(model)
    
    # retrieving posterior draws and compute the difference per participant & time
    prob_y_above_0_indiv <- model$data %>%
        add_epred_draws(object = model) %>%
        # add_epred_draws(object = model, ndraws = 1e2) %>%
        data.frame() %>%
        dplyr::select(participant, time, condition, .epred, .draw) %>%
        pivot_wider(names_from = condition, values_from = .epred) %>%
        # pivot_wider(id_cols = c(participant, time), names_from = condition, values_from = .epred) %>%
        mutate(epred_diff = cond2 - cond1) %>%
        group_by(participant, time) %>%
        # summarise(m = mean(epred_diff > 0 + sesoi), .groups = "drop") %>%
        summarise(m = mean(epred_diff > 0 + sesoi) ) %>%
        mutate(
            prob_ratio = m / (1 - m),
            prob_ratio = ifelse(is.infinite(prob_ratio), n_posterior_samples, prob_ratio),
            prob_ratio = ifelse(prob_ratio == 0, 1 / n_posterior_samples, prob_ratio),
            above_thres = ifelse(prob_ratio >= threshold, 1, NA)
            ) %>%
        ungroup()
    
    # some tests
    # model$data %>%
    #     add_epred_draws(object = model, ndraws = 1e2) %>%
    #     data.frame() %>%
    #     dplyr::select(participant, time, condition, .epred, .draw) %>%
    #     filter(participant == "participant_01" & time == 0 & .draw == 1)
    # 
    # prob_y_above_0_indiv %>%
    #     filter(participant == "participant_02" & time == 0 & .draw == 2)
    # 
    # prob_y_above_0_indiv %>%
    #     pivot_wider(names_from = condition, values_from = .epred) %>%
    #     filter(participant == "participant_02" & time == 0 & .draw == 2)

    # plotting posterior probabilities per participant
    post_plot_indiv <- prob_y_above_0_indiv %>%
        ggplot(aes(x = time, y = m, group = participant, color = participant) ) +
        geom_hline(yintercept = 0.5, linetype = 2) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line(alpha = 0.5) +
        labs(
            x = "Time (s)",
            # y = expression(Pr(beta>0~"|"~data, model) ),
            y = expression(p(beta>0~"|"~data) ),
            title = "Posterior probabilities per participant"
            ) +
        ylim(c(0, 1) ) +
        theme(legend.position = "none")
    
    # plotting posterior probability ratio per participant (log scale)
    ratio_plot_indiv <- prob_y_above_0_indiv %>%
        ggplot(aes(x = time, y = prob_ratio, group = participant, color = participant) ) +
        geom_hline(yintercept = 1, linetype = "dashed") +
        geom_hline(yintercept = 10, linetype = "dashed", color = "darkred") +
        geom_hline(yintercept = 1 / 10, linetype = "dashed", color = "darkred") +
        geom_hline(yintercept = 100, linetype = "dashed", color = "orangered") +
        geom_hline(yintercept = 1 / 100, linetype = "dashed", color = "orangered") +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line(alpha = 0.5) +
        geom_point(
            data = . %>% dplyr::filter(!is.na(above_thres) ),
            aes(y = threshold),
            colour = "darkgreen",
            shape = 16,
            size = 1.5,
            na.rm = TRUE
            ) +
        scale_y_log10(
            labels = label_log(digits = 2),
            limits = c(1 / n_posterior_samples, n_posterior_samples),
            breaks = c(1/1e4, 1/1e3, 1/1e2, 1/10, 1, 10, 1e2, 1e3, 1e4)
            ) +
        labs(
            x = "Time (s)",
            y = expression(p(beta>0~"|"~data) / (1 - p(beta>0~"|"~data) ) ),
            title = "Posterior probability ratios per participant"
            ) +
        theme(legend.position = "none")
    
    # combining plots
    post_plot_indiv + ratio_plot_indiv
    
}

# plotting the predictions
# plot_post_test_ratio_individual(model = full_meta_gamm)
# plot_post_test_ratio_individual(model = meta_gam)
plot_post_test_ratio_individual(model = gamm)

# plotting the predictions at the participant level
plot_post_preds_ppt <- function (model, data = NULL, true_onset = NULL, true_offset = NULL) {
    
    if (is.null(data) ) data <- model$data
    
    # retrieving predictions at the individual level
    # cond_effects <- conditional_effects(
    #     # defining the model
    #     x = model,
    #     # defining the interaction of interest
    #     effects = "time:condition",
    #     # defining the "conditions" (participant) on which computing predictions
    #     conditions = make_conditions(x = model$data, vars = "participant"),
    #     # getting predictions
    #     method = "posterior_epred",
    #     # including all random/varying effects
    #     re_formula = NULL
    #     )[[1]]
    
    # retrieving posterior draws and computing the difference per participant & time
    post_preds <- model$data %>%
        add_epred_draws(object = model, re_formula = NULL, ndraws = 500) %>%
        data.frame() %>%
        dplyr::select(participant, time, condition, .epred, .draw) %>%
        group_by(participant, time, condition) %>%
        summarise(
            .epred_mean = mean(.epred),
            .epred_lower = quantile(.epred, probs = 0.025),
            .epred_upper = quantile(.epred, probs = 0.975),
            .groups = "drop"
            ) %>%
        ungroup()

    # retrieving observed means (optional)
    obs_means <- data %>%
        group_by(participant, time, condition) %>%
        summarise(eeg_mean = mean(eeg_mean), .groups = "drop")
    
    # plotting
    post_preds %>%
        ggplot(aes(x = time, y = .epred_mean, colour = condition, fill = condition) ) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_hline(yintercept = 0, linetype = 2) +
        geom_point(
            data = obs_means, aes(y = eeg_mean),
            pch = 21, size = 1, alpha = 0.2,
            show.legend = FALSE
            ) +
        geom_line() +
        geom_ribbon(aes(ymin = .epred_lower, ymax = .epred_upper, colour = NULL), alpha = 0.2) +
        facet_wrap(~participant, scales = "free_y") +
        scale_colour_met_d(name = "Johnson") +
        scale_fill_met_d(name = "Johnson") +
        labs(x = "Time (s)", y = "EEG signal (a.u.)", title = "Posterior predictions by participant")
    
}

plot_post_preds_ppt(model = meta_gam)
```

<!--

\newpage

### Error properties of the proposed approach

We then assessed the performance of the proposed approach by computing the difference between the true and estimated onset/offset of the EEG difference according to various `k` (basis dimension) and `threshold` values. Remember that the EEG signal was generated from a truncated Gaussian with an objective onset at 160 ms, a maximum at 250 ms, and an offset at 342 ms. @fig-onset-error shows that the multilevel GAM can almost exactly recover the true onset and offset values, given some reasonable choice of `k` and `threshold` values. We provide more detailed recommendations on how to set `k` in @apx-basis. This figure further reveals that the optimal `k` and `threshold` values may differ for the onset and offset values, and there seems to exist a trade-off between these two parameters: lower `k` values lead to poorer estimations, but these poor estimations can be compensated (only to some extent) by higher `threshold` values (and reciprocally).

```{r fig-onset-error, eval = TRUE, echo = FALSE, results = "hide", dev = "png", dpi = 300, fig.width = 9, fig.asp = 0.5, fig.cap = "Average estimation error (RMSE) for the onset (left) and offset (right) according to various basis dimension and threshold values for the BGAM (computed from 100 simulated datasets)."}
# running the simulation on the HPC cluster
# source("code/meta_gam_errors_cluster.R")

# loading the simulation results
results <- readRDS(file = "results/meta_gam_error_properties_cluster.rds")

# defining the true onset and offset in seconds
true_onset <- 0.160
true_peak <- 0.250
true_offset <- 0.342

# plotting the results
# results %>%
#     group_by(kvalue, threshold, sim_id) %>%
#     summarise(
#         # value = if (onset_offset[1] == "onset") min(value) else max(value),
#         cluster_onset = min(cluster_onset),
#         cluster_offset = max(cluster_offset)
#         # .groups = "drop"
#         ) %>%
#     ungroup() %>%
#     group_by(kvalue, threshold) %>%
#     # summarise(across(estimated_onset:error_offset, median) ) %>%
#     # summarise(across(estimated_onset:error_offset, mean) ) %>%
#     summarise(
#         # error_onset = mean(abs(cluster_onset - true_onset) ),
#         # error_offset = mean(abs(cluster_offset - true_offset) )
#         error_onset = sqrt(mean(cluster_onset - true_onset)^2),
#         error_offset = sqrt(mean(cluster_offset - true_offset)^2)
#         ) %>%
#     ungroup() %>%
#     pivot_longer(cols = error_onset:error_offset) %>%
#     mutate(
#         name = factor(
#             x = name,
#             levels = c("error_onset", "error_offset"),
#             labels = c("onset", "offset")
#             )
#         ) %>%
#     ggplot(
#         aes(
#             x = as.factor(kvalue),
#             y = as.factor(threshold),
#             fill = value
#             )
#         ) +
#     geom_raster(interpolate = FALSE) +
#     # geom_tile() +
#     # geom_point(
#     #     data = results %>%
#     #         group_by(kvalue, threshold) %>%
#     #         summarise(across(estimated_onset:error_offset, mean) ) %>%
#     #         ungroup() %>%
#     #         na.omit() %>%
#     #         filter(error_onset == min(error_onset) ),
#     #     # aes(x = kvalue, y = threshold),
#     #     aes(x = kvalue, y = value),
#     #     color = "orangered", size = 4, shape = 4,
#     #     # show.legend = FALSE
#     #     show.legend = TRUE
#     #     ) +
#     facet_wrap(~name) +
#     scale_x_discrete(expand = c(0, 0) ) +
#     scale_y_discrete(expand = c(0, 0), breaks = c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50) ) +
#     scale_fill_gradientn(
#         colors = rev(met.brewer("Hokusai1") ),
#         transform = "sqrt"
#         ) +
#     labs(
#         x = "GAM basis dimension (k)",
#         y = "Threshold",
#         fill = "RMSE"
#         )

results %>%
    # filter(kvalue > 5) %>%
    group_by(kvalue, threshold, sim_id) %>%
    summarise(
        cluster_onset = min(cluster_onset),
        cluster_offset = max(cluster_offset),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    group_by(kvalue, threshold) %>%
    summarise(
        # error_onset = median(cluster_onset - true_onset),
        # error_offset = median(cluster_offset - true_offset)
        # error_onset = median(abs(cluster_onset - true_onset) ),
        # error_offset = median(abs(cluster_offset - true_offset) )
        error_onset = sqrt(mean(cluster_onset - true_onset)^2),
        error_offset = sqrt(mean(cluster_offset - true_offset)^2)
        ) %>%
    ungroup() %>%
    pivot_longer(cols = error_onset:error_offset) %>%
    mutate(
        name = factor(
            x = name,
            levels = c("error_onset", "error_offset"),
            labels = c("onset", "offset")
            )
        ) %>%
    ggplot(
        aes(
            x = threshold,
            y = value,
            colour = as.factor(kvalue),
            fill = as.factor(kvalue),
            group = as.factor(kvalue),
            )
        ) +
    geom_line(linewidth = 0.5) +
    geom_point(size = 0.8) +
    # geom_segment(
    #     data = . %>% filter(value == min(value), .by = name),
    #     aes(
    #         x = threshold,
    #         xend = threshold,
    #         y = 0.01,
    #         yend = 0,
    #         colour = as.factor(kvalue)
    #         ),
    #     arrow = arrow(length = unit(0.2, "cm"), type = "closed"),
    #     inherit.aes = FALSE,
    #     linewidth = 1.0,
    #     show.legend = FALSE
    #     ) +
    # geom_smooth(alpha = 0.1, linewidth = 0.8) +
    facet_wrap(~name) +
    scale_fill_viridis_d() +
    scale_colour_viridis_d() +
    scale_y_continuous(
        labels = scales::label_number(),
        transform = "sqrt"
        ) +
    labs(
        x = "BGAM threshold",
        y = "Estimation error (RMSE)",
        colour = "BGAM basis\ndimension (k)",
        fill = "BGAM basis\ndimension (k)"
        )
```

\newpage

-->

### Comparing the onset/offset estimates across approaches

We then compared the ability of the BGAM to accurately estimate the onset and offset of the ERP difference to other widely-used methods. First, we conducted mass-univariate t-tests (thus treating each timestep independently) and identified the onset and offset of the ERP difference as the first and last values crossing an arbitrary significance threshold ($\alpha = 0.05$). We then followed the same approach but after applying different forms of multiplicity correction to the $p$-values. We compared two methods that control the FDR [i.e., `BH95`, @benjamini1995; and `BY01`, @benjamini2001], one method that controls the FWER [i.e., Holm–Bonferroni method, @holm1979], and two cluster-based permutation methods [permutation with a single cluster-forming threshold and threshold-free cluster enhancement, `TFCE`, @smith2009]. The `BH95`, `BY01`, and `Holm` corrections were applied to the p-values using the `p.adjust()` function in `R`. The cluster-based inference was implemented using a cluster-sum statistic of squared $t$-values, as implemented in `MNE-Python` [@gramfort2013], called via `r pkrt("reticulate")`. We also compared these estimates to the onset and offset as estimated using the binary segmentation algorithm, as implemented in `r pkrt("changepoint")`, and applied directly to the squared $t$-values [as in @rousselet_using_2025].^[As in @rousselet_using_2025, we fixed the number of expected change points to two in the binary segmentation algorithm, thus producing always one cluster.] @fig-corrections illustrates the onsets and offsets estimated by each method on a single simulated dataset and shows that all methods systematically overestimate the true onset and underestimate the true offset. In addition, the `Raw p-value`, `FDR BH95`, and `FDR BY01` methods identify clusters well before the true onset and after the true offset.

```{r fig-corrections, echo = FALSE, results = "hide", fig.width = 8, fig.asp = 0.5, fig.cap = "Exemplary timecourse of squared t-values with true onset and offset (vertical black dashed lines) and onsets/offsets identified using the raw p-values, the corrected p-values (BH95, BY01, Holm), the cluster-based methods (Cluster mass, TFCE), or using the binary segmentation method (Change point)."}
# defining an arbitrary alpha threshold
aath <- 0.05

# summarising raw_data per participant
ppt_data <- raw_df %>%
    summarise(eeg = mean(eeg), .by = c(participant, condition, time) ) %>%
    pivot_wider(names_from = condition, values_from = eeg) %>%
    mutate(eeg_diff = cond2 - cond1)

# massive univariate t-tests
tests_results <- ppt_data %>%
    group_by(time) %>%
    summarise(
        tval = t.test(x = eeg_diff, mu = 0)$statistic^2,
        pval = t.test(x = eeg_diff, mu = 0)$p.value
        ) %>%
    mutate(
        pval_bh = p.adjust(p = pval, method = "BH"),
        pval_by = p.adjust(p = pval, method = "BY"),
        pval_holm = p.adjust(p = pval, method = "holm")
        ) %>%
    ungroup()

# defining a function to find onset and offset in timeseries
find_clusters <- function (df, threshold = aath) {
    
    df %>%
        select(-tval) %>%
        pivot_longer(cols = -time) %>%
        group_by(name) %>%
        mutate(above_threshold = value <= threshold) %>%
        mutate(cluster_change = c(TRUE, diff(above_threshold) != 0) ) %>%
        filter(above_threshold) %>%
        mutate(cluster_id = cumsum(cluster_change) ) %>%
        group_by(name, cluster_id) %>%
        summarise(cluster_onset = first(time), cluster_offset = last(time) ) %>%
        select(cluster_onset, cluster_offset)
    
}

# estimating onset and offset using the binary segmentation method
res <- cpt.meanvar(data = tests_results$tval, method = "BinSeg", Q = 2, penalty = "MBIC", minseglen = 2)
onset_cpt <- tests_results$time[res@cpts[1]]
offset_cpt <- tests_results$time[res@cpts[2]]

# sanity checks
# ts.plot(tests_results$tval)
# for (i in 1:length(res@cpts) ) {
#     
#     abline(v = res@cpts[i])
#     
# }

# running the cluster-based permutation tests
source(file = "code/mne_clusters.R")

# joining results in a common dataframe
methods_offset = data.frame(
    method = c("pval", "pval_bh", "pval_by", "pval_holm", "cluster_mass", "cluster_tfce", "cpt"),
    y_offset = rev(seq(from = -150, to = -20, length.out = 7) )
    )

clusters <- find_clusters(tests_results) %>%
    rename(method = name, onset = cluster_onset, offset = cluster_offset) %>%
    bind_rows(
        data.frame(
            method = "cpt",
            onset = onset_cpt,
            offset = offset_cpt
            )
        ) %>%
    data.frame() %>%
    bind_rows(data.frame(method = "cluster_mass", onset = onset_cluster_mass, offset = offset_cluster_mass) ) %>%
    bind_rows(data.frame(method = "cluster_tfce", onset = onset_cluster_mass_tfce, offset = offset_cluster_mass_tfce) ) %>%
    left_join(methods_offset, by = "method") %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "pval", "pval_bh", "pval_by", "pval_holm",
                "cluster_mass", "cluster_tfce", "cpt"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm",
                "Cluster mass", "TFCE", "Change point"
                ),
            )
        )

# plotting the results
tests_results %>%
    ggplot(aes(x = time, y = tval) ) +
    geom_hline(yintercept = 0, linetype = 1) +
    geom_area(position = "identity") +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_segment(
        # data = all_methods_onset_offset,
        data = clusters,
        aes(
            x = onset,
            xend = offset,
            y = y_offset,
            yend = y_offset,
            colour = method,
            ),
        linewidth = 1,
        lineend = "round",
        inherit.aes = FALSE
        ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(x = "Time (s)", y = "Squared t-value", colour = NULL)
```

### Simulation study

To assess the accuracy of group-level onset and offset estimation, all methods were compared by computing the bias (defined as the mean difference between the estimated and true value of the onset/offset), mean absolute error (MAE), root mean square error (RMSE), and variance of onset/offset estimates from 10,000 simulated datasets. Following @rousselet_using_2025, each participant was assigned a random onset between 150 and 170ms. Whereas the present article focuses on one-dimensional signals (e.g., one M/EEG channel), we provide a 2D application in @apx-2D.

<!--

## Results

This section is divided in two parts. First, we present the results from the simulation study, assessing the bias and variance of each method when applied to simulated data in which the ground truth is known. Second, we present the results obtained when applying the different methods to actual MEG data (decoding performance through time), assessing the reliability of the estimates provided by each method.

## Simulation study (bias and variance)

-->

## Results

@fig-simulation-results shows a summary of the simulation results, revealing that the proposed approach (`BGAM`) has the lowest error for both the onset and offset estimates. The `Cluster mass` and `Change point` methods also have good performance, but perhaps surprisingly, the `TFCE` method performs poorly for estimating the offset of the effect (with performance similar to the `Holm` method). Unsurprisingly, the `FDR BH95` and `Raw p-value` methods show the worst performance.

```{r fig-simulation-results, echo = FALSE, fig.width = 9, fig.asp = 0.6, fig.cap = "Mean error and variance of onset and offset estimates according to each method. Variance is plotted on a log10 scale for visual purposes."}
# loading all RDS files
files <- list.files(
    path = "results/errors_cluster_results", 
    pattern = ".rds", 
    full.names = TRUE
    )

# sorting files numerically by the number in the filename
files <- files[order(readr::parse_number(files) )]

# reading and binding all files
sim_results <- map2_dfr(
    .x = files,
    .y = seq_along(files),
    .f = ~ readRDS(.x)
    )

# sanity check
# n_distinct(sim_results$simulation_id)

# identifying the best brms results
# sim_results %>%
#     filter(method == "brms") %>%
#     pivot_wider(names_from = onset_offset, values_from = value) %>%
#     summarise(
#         bias_onset = mean(onset - true_onset),
#         MAE_onset = mean(abs(onset - true_onset) ),
#         variance_onset = var(onset),
#         bias_offset = mean(offset - true_offset),
#         MAE_offset = mean(abs(offset - true_offset) ),
#         variance_offset = var(offset),
#         .by = threshold
#         ) %>%
#     arrange(MAE_onset, MAE_offset)
#     # arrange(bias_onset, bias_offset)
#     # arrange(variance_onset, variance_offset)

# plotting the distribution of results
# sim_results %>%
#     # keeping only threshold = 20 for BGAM and threshold = 0.05 for frequentist results
#     filter(threshold %in% c(0.05, 20) ) %>%
#     mutate(
#         error = case_when(
#             # grepl("onset", onset_offset) ~ abs(value - true_onset),
#             # grepl("offset", onset_offset) ~ abs(value - true_offset)
#             grepl("onset", onset_offset) ~ value - true_onset,
#             grepl("offset", onset_offset) ~ value - true_offset
#             )
#         ) %>%
#     mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "raw_p", "pval_bh", "pval_by", "pval_holm",
#                 "cluster_mass", "cluster_tfce", "cpt", "brms"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "BGAM"
#                 ),
#             )
#         ) %>%
#     mutate(
#         plot_row = case_when(
#             method %in% c("Raw p-value", "Holm") ~ 1,
#             method %in% c("FDR BH95", "FDR BY01") ~ 2,
#             method %in% c("Cluster mass", "TFCE") ~ 3,
#             method %in% c("Change point", "BGAM") ~ 4
#             )
#         ) %>%
#     ggplot(
#         aes(
#             x = error,
#             colour = method,
#             fill = method
#             )
#         ) +
#     geom_vline(xintercept = 0, linetype = 2) +
#     geom_density(aes(fill = NULL) ) +
#     facet_grid(plot_row~onset_offset, scales = "free_x") +
#     theme_light(base_size = 12, base_family = "Open Sans") +
#     scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     labs(x = "Onset/Offset (s)", y = "Density")

# computing and plotting bias and variance
# bias := mean(sim_onset_offset_distrib - true_onset_offset)
# variance := var(sim_onset_offset_distrib - true_onset_offset)
sim_results %>%
    # keeping only threshold = 20 for BGAM and threshold = 0.05 for frequentist results
    filter(threshold %in% c(0.05, 20) ) %>%
    mutate(
        error = case_when(
            # grepl("onset", onset_offset) ~ abs(value - true_onset),
            # grepl("offset", onset_offset) ~ abs(value - true_offset)
            grepl("onset", onset_offset) ~ value - true_onset,
            grepl("offset", onset_offset) ~ value - true_offset
            )
        ) %>%
    group_by(method, onset_offset) %>%
    summarise(
        mean_error = mean(error, na.rm = TRUE),
        median_error = median(error, na.rm = TRUE),
        mean_abs_error = mean(abs(error), na.rm = TRUE),
        median_abs_error = median(abs(error), na.rm = TRUE),
        variance = var(error, na.rm = TRUE),
        mad = mad(value, na.rm = TRUE),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm",
                "cluster_mass", "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (our method)"
                ),
            )
        ) %>%
    ggplot(
        aes(
            x = mean_error,
            # y = mad,
            # y = median_abs_error,
            # y = median_abs_error,
            # x = median_abs_error,
            y = variance,
            # y = mean_abs_error,
            colour = method,
            fill = method
            )
        ) +
    geom_vline(xintercept = 0, linetype = 2) +
    # geom_errorbar(
    #     aes(
    #         xmin = median_error - mad, xmax = median_error + mad,
    #         #ymin = median_abs_error - mad, ymax = median_abs_error + mad
    #         )
    #     ) +
    #     geom_errorbar(
    #     aes(
    #         #xmin = median_error - mad, xmax = median_error + mad,
    #         ymin = median_abs_error - mad, ymax = median_abs_error + mad
    #         )
    #     ) +
    geom_point(
        size = 2,
        pch = 21,
        colour = "white",
        show.legend = FALSE
        ) +
    geom_label_repel(
        aes(label = method),
        colour = "white",
        size = 3,
        segment.color = NA,
        show.legend = FALSE
        ) +
    # scale_x_log10(guide = "axis_logticks") +
    scale_y_log10(
        guide = "axis_logticks",
        # breaks = trans_breaks("log10", function(x) 10^x),
        # labels = trans_format("log10", math_format(10^.x) )
        labels = label_log(digits = 2)
        ) +
    facet_wrap(~onset_offset) +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(x = "Mean error (s)", y = "Variance (log-scale)")

# or plotting only the error in descending order
# sim_results %>%
#     # keeping only threshold = 10 for BGAM and threshold = 0.05 for frequentist results
#     filter(threshold %in% c(0.05, 20) ) %>%
#     # filter(threshold %in% c(0.05, 10) ) %>%
#     mutate(
#         error = case_when(
#             # grepl("onset", onset_offset) ~ abs(value - true_onset),
#             # grepl("offset", onset_offset) ~ abs(value - true_offset)
#             grepl("onset", onset_offset) ~ value - true_onset,
#             grepl("offset", onset_offset) ~ value - true_offset
#             )
#         ) %>%
#     # converting to ms
#     # mutate(error = error * 1e3) %>%
#     group_by(method, onset_offset) %>%
#     summarise(
#         mean_error = mean(error, na.rm = TRUE),
#         median_error = median(error, na.rm = TRUE),
#         median_abs_error = median(abs(error), na.rm = TRUE),
#         variance = var(error, na.rm = TRUE),
#         # variance = var(value, na.rm = TRUE),
#         sem = sd(value, na.rm = TRUE) / sqrt(n() ),
#         mad = mad(value, na.rm = TRUE),
#         .groups = "drop"
#         ) %>%
#     ungroup() %>%
#     mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "raw_p", "pval_bh", "pval_by", "pval_holm",
#                 "cluster_mass", "cluster_tfce", "cpt", "brms"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "BGAM (our method)"
#                 ),
#             )
#         ) %>%
#     mutate(
#         method_reordered = tidytext::reorder_within(
#             x = method,
#             # by = 1 / error,
#             # fun = median,
#             # by = 1 / abs(median_error),
#             by = 1 / median_abs_error,
#             # by = 1 / abs(mean_error),
#             within = onset_offset
#             )
#         ) %>%
#     # mutate(
#     #     method_reordered = fct_reorder(
#     #         method, error, .fun = ~abs(median(.x) ), .na_rm = TRUE
#     #         )
#     #     ) %>%
#     # arrange(desc(error) ) %>%
#     # arrange(error) %>%
#     # head()
#     ggplot(
#         aes(
#             x = median_error,
#             # x = error,
#             xmin = median_error - mad,
#             xmax = median_error + mad,
#             # x = mean_error,
#             # xmin = mean_error - variance,
#             # xmax = mean_error + variance,
#             y = method_reordered,
#             colour = method,
#             fill = method
#             )
#         ) +
#     geom_vline(xintercept = 0, linetype = 2) +
#     geom_pointrange(
#         size = 0.25,
#         linewidth = 0.75,
#         show.legend = FALSE
#         ) +
#     # stat_histinterval(
#     #     # breaks = "FD",
#     #     # n = 1e3,
#     #     # point_interval = "median_qi",
#     #     # .width = c(0.5, 0.95),
#     #     .width = 0.1,
#     #     # colour = "black",
#     #     # slab_colour = "white",
#     #     # slab_linewidth = 0,
#     #     slab_alpha = 0.8,
#     #     # fill = "steelblue"
#     #     show.legend = FALSE
#     #     ) +
#     facet_wrap(~onset_offset, scales = "free") +
#     scale_y_reordered() +
#     scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     labs(
#         x = "Estimation error (s)",
#         # x = "Mean error (s)",
#         y = "Method"
#         )

# saving the plot
# ggsave(
#     # filename = "figures/simulation_results_median_error_variance.png",
#     # filename = "figures/simulation_results_bias_mae.png",
#     filename = "figures/simulation_results_mae_variance.png",
#     width = 12, height = 6, dpi = 300,
#     device = "png"
#     )

# plotting the distribution of results
# sim_results %>%
#     # keeping only threshold = 10 for BGAM and threshold = 0.05 for frequentist results
#     filter(threshold %in% c(0.05, 20) ) %>%
#     mutate(
#         error = case_when(
#             # grepl("onset", onset_offset) ~ abs(value - true_onset),
#             # grepl("offset", onset_offset) ~ abs(value - true_offset)
#             grepl("onset", onset_offset) ~ value - true_onset,
#             grepl("offset", onset_offset) ~ value - true_offset
#             )
#         ) %>%
#     # converting to ms
#     # mutate(error = error * 1e3) %>%
#     # group_by(method, onset_offset) %>%
#     # summarise(
#     #     mean_error = mean(error, na.rm = TRUE),
#     #     median_error = median(error, na.rm = TRUE),
#     #     median_abs_error = median(abs(error), na.rm = TRUE),
#     #     variance = var(error, na.rm = TRUE),
#     #     # variance = var(value, na.rm = TRUE),
#     #     sem = sd(value, na.rm = TRUE) / sqrt(n() ),
#     #     mad = mad(value, na.rm = TRUE),
#     #     .groups = "drop"
#     #     ) %>%
#     # ungroup() %>%
#     mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "raw_p", "pval_bh", "pval_by", "pval_holm",
#                 "cluster_mass", "cluster_tfce", "cpt", "brms"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "BGAM (our method)"
#                 ),
#             )
#         ) %>%
#     mutate(
#         method_reordered = tidytext::reorder_within(
#             x = method,
#             by = abs(error),
#             fun = median,
#             # by = 1 / abs(median_error),
#             # by = 1 / abs(mean_error),
#             within = onset_offset
#             )
#         ) %>%
#     # mutate(
#     #     method_reordered = fct_reorder(
#     #         method, error, .fun = ~abs(median(.x) ), .na_rm = TRUE
#     #         )
#     #     ) %>%
#     # arrange(desc(error) ) %>%
#     # arrange(error) %>%
#     # head()
#     ggplot(
#         aes(
#             # x = median_error,
#             x = error,
#             # xmin = median_error - mad,
#             # xmax = median_error + mad,
#             # x = mean_error,
#             # xmin = mean_error - variance,
#             # xmax = mean_error + variance,
#             y = method_reordered,
#             colour = method,
#             fill = method
#             )
#         ) +
#     geom_vline(xintercept = 0, linetype = 2) +
#     # geom_pointrange(
#     #     size = 0.25,
#     #     linewidth = 0.75,
#     #     show.legend = FALSE
#     #     ) +
#     stat_slabinterval(
#         n = 200,
#         normalize = "groups",
#         point_interval = "median_qi",
#         # .width = c(0.1, 0.9),
#         # .width = 0.2,
#         .width = 0,
#         interval_size_range = c(1, 1),
#         fatten_point = 1.25,
#         # slab_colour = "white",
#         # slab_linewidth = 0,
#         # density = "unbounded",
#         slab_alpha = 0.5,
#         show.legend = FALSE
#         ) +
#     facet_wrap(~onset_offset, scales = "free") +
#     scale_y_reordered() +
#     scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     labs(
#         x = "Estimation error (s)",
#         y = "Method"
#         )
```

These results are further summarised in @tbl-simulation-results, which shows that the `BGAM` method is almost perfectly unbiased (i.e., it has a bias of approximately 0.1ms for the onset and 2.4ms for the offset). The `Bias` column shows that all methods tend to estimate the onset later than the true onset and to estimate the offset earlier than the true offset. As can be seen from this table, the `BGAM` method has the best performance on all included metrics (except for the `Variance` of the offset estimate, where the `Change point` method performs better, presumably because it was constrained to identifying a single cluster).

```{r tbl-simulation-results, eval = TRUE, echo = FALSE, results = "asis"}
#| tbl-cap: "Summary statistics of onset/offset estimates for each method (in ms, ordered by the MAE)."
# summarising the results
sim_results_summary <- sim_results %>%
    # keeping only threshold = 20 for BGAM and threshold = 0.05 for frequentist results
    filter(threshold %in% c(0.05, 20) ) %>%
    # filter(threshold %in% c(0.05, 10) ) %>%
    mutate(
        error = case_when(
            # grepl("onset", onset_offset) ~ value - true_onset,
            # grepl("offset", onset_offset) ~ value - true_offset
            grepl("onset", onset_offset) ~ value * 1e3 - true_onset * 1e3,
            grepl("offset", onset_offset) ~ value * 1e3 - true_offset * 1e3
            )
        ) %>%
    # converting to ms
    # mutate(error = error * 1e3) %>%
    group_by(method, onset_offset) %>%
    summarise(
        mean_error = mean(error, na.rm = TRUE),
        # median_error = median(error, na.rm = TRUE),
        mean_abs_error = mean(abs(error), na.rm = TRUE),
        # median_abs_error = median(abs(error), na.rm = TRUE),
        rmse = sqrt(mean(error)^2),
        variance = var(error, na.rm = TRUE),
        # mad = mad(error, na.rm = TRUE),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm",
                "cluster_mass", "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (our method)"
                ),
            )
        ) %>%
    mutate(
        onset_offset = factor(x = onset_offset, levels = c("onset", "offset") )
        ) %>%
    data.frame()

# creating the table
sim_results_summary %>%
    mutate(
        onset_offset = factor(
            x = onset_offset,
            levels = c("offset", "onset")
            )
        ) %>%
    # arrange(abs(median_error) ) %>%
    # arrange(abs(mean_error) ) %>%
    # arrange(median_abs_error) %>%
    arrange(mean_abs_error) %>%
    group_by(onset_offset) %>%
    gt() %>%
    fmt_number(
        columns = everything(),
        decimals = 2
        ) %>%
    cols_align(
        align = "center",
        columns = everything()
        ) %>%
    cols_align(
        align = "left",
        columns = method
        ) %>%
    cols_label(
        method = "",
        # mean_error = "Bias_mean",
        # median_error = "Bias",
        mean_error = "Bias",
        # median_abs_error = "MAE",
        mean_abs_error = "MAE",
        rmse = "RMSE",
        variance = "Variance"
        # mad = "MAD"
        ) %>%
    tab_options(
        table.align = "center",
        # table.width = "75%",
        table.font.size = 12
        ) %>%
    as_latex()

```

# Application to actual MEG data

## Methods

To complement the simulation study, we evaluated the performance of all methods on actual MEG data [@nalborczyk:inprep]. In this study, the authors conducted time-resolved multivariate pattern analysis (MVPA, also known as decoding) of MEG data recorded in 32 human participants during a reading task. As a result, the authors obtained a timecourse of decoding accuray (ROC AUC), bounded between 0 and 1, for each participant. To test whether the group-level average decoding accuracy was above chance (i.e., 0.5) at each timestep, we fitted a BGAM as introduced previously with a basis dimension $k=50$ and retained all timesteps exceeding a posterior odds of 20.  To better distinguish signal from noise, we defined a region of practical equivalence [ROPE, @kruschke2017] as the upper 90% quantile of decoding performance during the baseline period (i.e., before stimulus onset). Although we chose a basis dimension of $k=50$, which seemed appropriate for the present data, this choice should be adapted according to the properties of the modelled data (e.g., signal-to-noise ratio, prior low-pass filtering, sampling rate) and should be assessed by the usual model checking tools (e.g., models comparison, posterior predictive checks, see @apx-basis).

```{r fig-decoding-data, cache = FALSE, echo = FALSE, eval = FALSE, out.width = "75%", fig.asp = 0.75, fig.cap = "Group-level average decoding performance (N=32) superimposed with the BGAM predictions (in blue) and the region of practical equivalence (ROPE, in orange) computed from the baseline period [data from @nalborczyk:inprep]. The blue horizontal markers indicate the timesteps at which the posterior probability ratio exceeds 20."}
# importing the reshaped MEG decoding results
decoding_df <- read.csv(file = "data/decoding_results_reshaped.csv") %>%
    # removing some participants
    mutate(participant_id = cur_group_id(), .by = participant) %>%
    dplyr::filter(participant_id < 33) %>%
    select(-participant_id)

# averaging across participants
decoding_summary_df <- decoding_df %>%
    summarise(
        auc_mean = mean(auc),
        auc_sd = sd(auc),
        .by = c(participant, time)
        )

# defining the chance level
chance_level <- 0.5

# computing the smallest effect size of interest (SESOI)
# as the upper 90% quantile of decoding performance during the baseline
sesoi <- decoding_summary_df %>%
    dplyr::filter(time < 0) %>%
    summarise(auc_mean = mean(auc_mean), .by = time) %>%
    summarise(sesoi = quantile(x = auc_mean, probs = 0.90) ) %>%
    # summarise(sesoi = quantile(x = auc_mean, probs = 0.95) ) %>%
    pull(sesoi) - chance_level

# computing the smallest effect size of interest (SESOI)
# as the SD of decoding performance during the baseline
# sesoi <- decoding_summary_df %>%
#     dplyr::filter(time < 0) %>%
#     summarise(auc_mean = mean(auc_mean), .by = time) %>%
#     summarise(sesoi = sd(auc_mean) ) %>%
#     pull(sesoi)

# computing the smallest effect size of interest (SESOI)
# as the 95% confidence interval during the baseline
# sesoi <- decoding_summary_df %>%
#     filter(time < 0) %>%
#     summarise(auc_mean = mean(auc_mean), .by = time) %>%
#     summarise(sesoi = 1.96 * (sd(auc_mean) / sqrt(n() ) ) ) %>%
#     pull(sesoi)

# fitting the GAMs
meg_decoding_gam_gaussian_k40 <- brm(
    auc_mean ~ s(time, bs = "cr", k = 40),
    data = decoding_summary_df,
    family = gaussian(),
    warmup = 1000,
    iter = 5000,
    chains = 4,
    cores = 4
    )

meg_decoding_gam_gaussian_k50 <- brm(
    auc_mean ~ s(time, bs = "cr", k = 50),
    data = decoding_summary_df,
    family = gaussian(),
    # warmup = 1000,
    # iter = 5000,
    warmup = 2000,
    iter = 10000,
    chains = 4,
    cores = 4
    )

meg_decoding_gam_gaussian_k60 <- brm(
    auc_mean ~ s(time, bs = "cr", k = 60),
    data = decoding_summary_df,
    family = gaussian(),
    warmup = 1000,
    iter = 5000,
    chains = 4,
    cores = 4
    )

meg_decoding_gam_beta_k40 <- brm(
    auc_mean ~ s(time, bs = "cr", k = 40),
    data = decoding_summary_df,
    family = Beta(),
    warmup = 1000,
    iter = 5000,
    chains = 4,
    cores = 4
    )

meg_decoding_gam_beta_k50 <- brm(
    auc_mean ~ s(time, bs = "cr", k = 50),
    data = decoding_summary_df,
    family = Beta(),
    warmup = 1000,
    iter = 5000,
    chains = 4,
    cores = 4
    )

meg_decoding_gam_beta_k60 <- brm(
    auc_mean ~ s(time, bs = "cr", k = 60),
    data = decoding_summary_df,
    family = Beta(),
    warmup = 1000,
    iter = 5000,
    chains = 4,
    cores = 4
    )

# PPCs
pp_check(meg_decoding_gam_gaussian_k40)
pp_check(meg_decoding_gam_gaussian_k50)
pp_check(meg_decoding_gam_gaussian_k60)
pp_check(meg_decoding_gam_beta_k40)
pp_check(meg_decoding_gam_beta_k50)
pp_check(meg_decoding_gam_beta_k60)

# models comparison
loo(
    meg_decoding_gam_gaussian_k40, meg_decoding_gam_gaussian_k50,
    meg_decoding_gam_gaussian_k60,
    meg_decoding_gam_beta_k40, meg_decoding_gam_beta_k50,
    meg_decoding_gam_beta_k60
    )

meg_decoding_gam_gaussian_k40 <- add_criterion(x = meg_decoding_gam_gaussian_k40, criterio = "loo")
meg_decoding_gam_gaussian_k50 <- add_criterion(x = meg_decoding_gam_gaussian_k50, criterio = "loo")
meg_decoding_gam_gaussian_k60 <- add_criterion(x = meg_decoding_gam_gaussian_k60, criterio = "loo")
meg_decoding_gam_beta_k40 <- add_criterion(x = meg_decoding_gam_beta_k30, criterio = "loo")
meg_decoding_gam_beta_k50 <- add_criterion(x = meg_decoding_gam_beta_k50, criterio = "loo")
meg_decoding_gam_beta_k60 <- add_criterion(x = meg_decoding_gam_beta_k60, criterio = "loo")

models_comp <- loo_compare(
    meg_decoding_gam_gaussian_k40, meg_decoding_gam_gaussian_k50,
    meg_decoding_gam_gaussian_k60,
    meg_decoding_gam_beta_k40, meg_decoding_gam_beta_k50,
    meg_decoding_gam_beta_k60
    )

# summary
print(x = models_comp, simplify = FALSE, digits = 3)

# computing the posterior probability
# prob_y_above_0 <- data.frame(time = unique(meg_decoding_gam$data$time) ) %>%
prob_y_above_0 <- data.frame(time = unique(meg_decoding_gam_gaussian_k50$data$time) ) %>%
    # retrieving the posterior samples
    # add_epred_draws(object = meg_decoding_gam) %>%
    add_epred_draws(object = meg_decoding_gam_gaussian_k50) %>%
    # converting to dataframe
    data.frame() %>%
    # computing mean posterior probability at the group level
    group_by(time) %>%
    summarise(m = mean(.epred > (0 + chance_level + sesoi) ) ) %>%
    # summarise(m = mean(.epred > (sesoi) ) ) %>%
    mutate(prob_ratio = m / (1 - m) ) %>%
    # ensuring there is no 0 or +Inf values
    mutate(prob_ratio = pmin(prob_ratio, ndraws(meg_decoding_gam_gaussian_k50) ) ) %>%
    mutate(prob_ratio = pmax(prob_ratio, 1 / ndraws(meg_decoding_gam_gaussian_k50) ) ) %>%
    ungroup()

# defining a function to find all onset and offset in timeseries
find_clusters <- function (df, threshold) {
    
    df %>%
        mutate(above_threshold = prob_ratio >= threshold) %>%
        mutate(cluster_change = c(TRUE, diff(above_threshold) != 0) ) %>%
        dplyr::filter(above_threshold) %>%
        mutate(cluster_id = cumsum(cluster_change) ) %>%
        group_by(cluster_id) %>%
        summarise(
            cluster_onset = first(time), cluster_offset = last(time),
            .groups = "drop"
            ) %>%
        select(cluster_onset, cluster_offset)
    
}

# defining a threshold
post_prob_ratio_threshold <- 20
clusters <- find_clusters(prob_y_above_0, threshold = post_prob_ratio_threshold)
    
# defining a colour for the BGAM method
brms_color <- met.brewer(name = "Johnson", n = 8)[8]

# checking model's predictions against raw data
plot(
    # conditional_effects(x = meg_decoding_gam),
    conditional_effects(x = meg_decoding_gam_gaussian_k50),
    line_args = list(colour = brms_color, fill = brms_color, alpha = 0.3),
    points = FALSE, plot = FALSE
    )[[1]] +
    geom_hline(yintercept = 0.5, linetype = 2) +
    geom_vline(xintercept = 0.0, linetype = 2) +
    annotate(
        geom = "rect",
        xmin = -Inf,
        xmax = Inf,
        ymin = chance_level - sesoi,
        ymax = chance_level + sesoi,
        # ymin = chance_level,
        # ymax = sesoi,
        fill ="orangered",
        alpha = 0.2
        ) +
    geom_segment(
        data = clusters,
        aes(
            x = cluster_onset,
            xend = cluster_offset,
            y = 0.48, yend = 0.48
            ),
        colour = brms_color,
        inherit.aes = FALSE,
        lineend = "round",
        linewidth = 2
        ) +
    geom_line(
        data = decoding_summary_df %>% summarise(auc_mean = mean(auc_mean), .by = time),
        aes(x = time, y = auc_mean), inherit.aes = FALSE
        ) +
    labs(x = "Time (s)", y = "Decoding accuracy (ROC AUC)")
```

```{r fig-post-prob-ratio-decoding, eval = FALSE, echo = FALSE, out.width = "100%", fig.width = 9, fig.asp = 0.5, fig.cap = "Left: Posterior probability of the EEG difference (slope) being above 0 according to the BGAMM. Right: Ratio of posterior probability according to the BGAMM (on a log10 scale). Timesteps above threshold (10) are highlighted in green. NB: the minimum and maximum possible ratio values are determined (bounded) by the number of posterior samples in the model."}
# Computing the EDF?
# From https://discourse.mc-stan.org/t/standard-operating-procedures-for-anova/3964/2
# If Kruschke has code that does what you want, then you can probably call his code on some columns of as.matrix(fit). If it really has to be in coda format, there is an As.mcmc.list function in the rstan package that will convert a stanfit object to a mcmc.list, but you probably have to call it on the $stanfit element of the brmsfit object.
# See the bamlss::samplestat() function at https://github.com/cran/bamlss/blob/0a2a4154e0c0099ba8ac0b372ae7460e57972741/R/BAMLSS.R#L2397
# Maybe export brms posterior samples to mcmc.list and then use this function?
# See https://mc-stan.org/rstan/reference/As.mcmc.list.html
library(bamlss)
# mcmc_samples <- as.mcmc(meg_decoding_gam)
mcmc_samples <- as.mcmc(meg_decoding_gam$fit)
mcmc_samples_stats <- samplestats(
    samples = mcmc_samples,
    x = bamlss.frame(formula = auc_mean ~ s(time, bs = "tp", k = 50), data = decoding_summary_df)$x,
    # y = bamlss.frame(formula = auc_mean ~ s(time, bs = "tp", k = 50), data = decoding_summary_df)$y,
    # family = beta_bamlss,
    # logLik = FALSE
    )
mcmc_samples_stats

# or using bamlss directly
b_model <- bamlss(
    formula = auc_mean ~ s(time, bs = "tp", k = 50),
    family = beta_bamlss,
    data = decoding_summary_df
    )

# computing the EDF (around 44.6)
samplestats(samples = b_model)

# comparing to p_WAIC (around 48.4)
waic(meg_decoding_gam)

# computing the posterior probability ratio (PPR)
model <- meg_decoding_gam
threshold <- 10

# defining a function to compute and plot the posterior odds
plot_posterior_odds <- function (model, threshold = 10) {
    
    # retrieving the number of posterior samples
    n_posterior_samples <- ndraws(model)
    
    # defiing the full time sequence
    # time_seq <- seq(min(model$data$time), max(model$data$time), length.out = 300)
    time_seg <- unique(model$data$time)
    
    # generating new data
    newdata <- data.frame(time = time_seq)
    
    # getting posterior expected values (marginal)
    posterior_preds <- posterior_epred(object = model, newdata = newdata, re_formula = NA)
    
    # computing posterior mean and 95% CI at each time
    posterior_preds_df <- as.data.frame(posterior_preds) %>%
        mutate(draw = row_number() ) %>%
        pivot_longer(-draw, names_to = "time_index", values_to = "auc") %>%
        mutate(time = rep(time_seq, each = nrow(posterior_preds) ) ) %>%
        group_by(time) %>%
        summarise(
            auc_mean = mean(auc),
            auc_lower = quantile(auc, 0.05),
            auc_upper = quantile(auc, 0.95)
            ) %>%
        ungroup()
    
    # computing the average upper 96% credible interval during baseline (i.e., before 0s)
    upper_baseline_ci <- posterior_preds_df %>%
        filter(time < 0) %>%
        summarise(upper_baseline_ci = mean(auc_upper) ) %>%
        pull(upper_baseline_ci)
    
    # computing the posterior probability of AUC > upper CrI of baseline
    prob_y_above_baseline <- model$data %>%
        add_epred_draws(object = model, ndraws = 100) %>%
        data.frame() %>%
        # head()
        # dplyr::select(participant, time, condition, .epred, .draw) %>%
        # pivot_wider(names_from = condition, values_from = .epred) %>%
        # mutate(epred_diff = cond2 - cond1) %>%
        # dplyr::select(time, baseline, .epred, .draw) %>%
        # pivot_wider(names_from = baseline, values_from = .epred) %>%
        # head()
        # mutate(epred_diff = `c - cond1) %>%
        # posteriors_preds %>%
        # computing the mean posterior probability at the group level
        group_by(time) %>%
        summarise(m = mean(.epred > upper_baseline_ci) ) %>%
        mutate(prob_ratio = m / (1 - m) ) %>%
        ungroup() %>%
        # bounding posterior probabilities by the number of posterior samples
        mutate(prob_ratio = pmin(prob_ratio, n_posterior_samples) ) %>%
        mutate(prob_ratio = pmax(prob_ratio, 1 / n_posterior_samples) ) %>%
        ungroup()
        
    # plotting it
    post_plot <- prob_y_above_baseline %>%
        ggplot(aes(x = time, y = m) ) +
        geom_hline(yintercept = 0.5, linetype = 2) +
        # geom_vline(xintercept = true_onset, linetype = 2) +
        # geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line() +
        ylim(c(0, 1) ) +
        labs(x = "Time (s)", y = expression(p(beta>0~"|"~data, model) ) )
    
    # plotting it
    ratio_plot <- prob_y_above_baseline %>%
        mutate(above_thres = ifelse(prob_ratio >= threshold, 1, NA) ) %>%
        ggplot(aes(x = time, y = prob_ratio) ) +
        geom_hline(yintercept = 1, linetype = "dashed") +
        geom_hline(yintercept = 10, linetype = "dashed", color = "darkred") +
        geom_hline(yintercept = 1/10, linetype = "dashed", color = "darkred") +
        geom_hline(yintercept = 100, linetype = "dashed", color = "orangered") +
        geom_hline(yintercept = 1/100, linetype = "dashed", color = "orangered") +
        # geom_vline(xintercept = true_onset, linetype = 2) +
        # geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line(linewidth = 0.8) +
        geom_point(
            data = . %>% dplyr::filter(!is.na(above_thres) ),
            aes(y = threshold),
            colour = "darkgreen",
            shape = 16,
            size = 2,
            na.rm = TRUE
            ) +
        scale_y_log10(
            labels = label_log(digits = 2),
            limits = c(1 / n_posterior_samples, n_posterior_samples),
            breaks = c(1/1e4, 1/1e3, 1/1e2, 1/10, 1, 10, 1e2, 1e3, 1e4)
            ) +
        labs(
            x = "Time (s)",
            y = expression(p(beta>0~"|"~data) / (1 - p(beta>0~"|"~data) ) )
            )
    
    # plotting both the posterior probability and the ratio...
    post_plot + ratio_plot
  
}

# plotting the posterior odds
plot_posterior_odds(model = meg_decoding_gam, threshold = 10)

####################################
# or testing an alternative method #
####################################

# defining the full time sequence
time_seq <- seq(min(decoding_summary_df$time), max(decoding_summary_df$time), length.out = 300)

# generating new data
newdata <- data.frame(time = time_seq)

# retrieving the posterior expected values (marginal)
posterior <- posterior_epred(object = meg_decoding_gam, newdata = newdata, re_formula = NA)

# computing posterior mean and 95% CI at each time
pred_df <- as.data.frame(posterior) %>%
    mutate(draw = row_number() ) %>%
    pivot_longer(-draw, names_to = "time_index", values_to = "auc") %>%
    mutate(time = rep(time_seq, each = nrow(posterior) ) ) %>%
    group_by(time) %>%
    summarise(
        auc_mean = mean(auc),
        auc_lower = quantile(auc, 0.025),
        auc_upper = quantile(auc, 0.975)
        )

# subsetting newdata to time < 0
baseline_indices <- which(time_seq < 0)
baseline_draws <- posterior[, baseline_indices]

# computing the average baseline AUC per draw
baseline_per_draw <- rowMeans(baseline_draws)

# for timepoints ≥ 0
post_indices <- which(time_seq >= 0)
time_post <- time_seq[post_indices]

# subsetting post-stimulus predictions
posterior_post <- posterior[, post_indices]

# computing posterior probability that AUC > pre-stim baseline
post_prob <- sapply(1:length(post_indices), function (i) {
    
    mean(posterior_post[, i] > baseline_per_draw)
    
})

# assembling into dataframe
post_df <- data.frame(
    time = time_post,
    prob_auc_gt_baseline = post_prob
    )

# plotting the results
n_posterior_samples <- ndraws(meg_decoding_gam)
post_df %>%
    ggplot(aes(x = time, y = prob_auc_gt_baseline / (1-prob_auc_gt_baseline) ) ) +
    geom_line(size = 1, color = "darkgreen") +
    geom_hline(yintercept = 0.95, linetype = "dashed", color = "gray") +
    scale_y_log10(
        labels = label_log(digits = 2),
        limits = c(1 / n_posterior_samples, n_posterior_samples),
        breaks = c(1/1e4, 1/1e3, 1/1e2, 1/10, 1, 10, 1e2, 1e3, 1e4)
        ) +
    labs(
        title = "Posterior Probability AUC > Pre-Stimulus Baseline",
        x = "Time (s)", y = "P(AUC > baseline)"
        ) +
    theme_bw()

# or using yet another approach
estimate_edf_brms <- function (
    model, 
    term,           # e.g., "s(time)"
    time_seq = NULL,
    n = 200,        # number of time points
    re_formula = NA # exclude group-level effects
    ) {

  # Extract time variable name from term, e.g., "s(time)" → "time"
  time_var <- gsub("s\\(([^,]+).*\\)", "\\1", term)

  # Create prediction grid
  if (is.null(time_seq)) {
    time_range <- range(model$data[[time_var]], na.rm = TRUE)
    time_seq <- seq(time_range[1], time_range[2], length.out = n)
  }

  # newdata <- data.frame(!!time_var := time_seq)
  newdata <- data.frame(time_seq)
  names(newdata) <- time_var

  # Extract posterior smooth draws only (excludes intercept & fixed effects)
  smooth_draws <- posterior_smooths(
    model,
    newdata = newdata,
    smooth = term,
    re_formula = re_formula,
    resp = NULL
  )

  # Compute posterior mean at each timepoint
  posterior_mean <- colMeans(smooth_draws)

  # Compute variance of the posterior mean across time
  var_mean <- var(posterior_mean)

  # Compute variance at each timepoint across draws
  pointwise_var <- apply(smooth_draws, 2, var)

  # Mean pointwise variance
  mean_pointwise_var <- mean(pointwise_var)

  # EDF estimate
  edf_estimate <- var_mean / mean_pointwise_var

  return(list(
    edf = edf_estimate,
    var_mean = var_mean,
    mean_pointwise_var = mean_pointwise_var,
    time = time_seq,
    smooth_mean = posterior_mean,
    smooth_sd = sqrt(pointwise_var)
  ))
}

edf_result <- estimate_edf_brms(
  model = meg_decoding_gam,
  #term = "s(time)"
  term = 's(time,bs="tp",k=50)'
)

print(paste("Estimated EDF:", round(edf_result$edf, 2)))

plot_df <- data.frame(
  time = edf_result$time,
  mean = edf_result$smooth_mean,
  sd = edf_result$smooth_sd
)

ggplot(plot_df, aes(x = time, y = mean)) +
  geom_line(color = "blue") +
  geom_ribbon(aes(ymin = mean - sd, ymax = mean + sd), alpha = 0.2) +
  labs(
    title = paste("Smooth with Posterior SD | Estimated EDF ≈", round(edf_result$edf, 2)),
    x = "Time", y = "Smooth Estimate"
  ) +
  theme_bw()
```

## Results

@fig-onset-offset shows the group-level average decoding performance through time superimposed with onset and offset estimates from each method. Overall, this figure shows that both the `Raw p-value` and `FDR BH95` methods are extremely lenient, identifying clusters of above-chance decoding accuracy before the onset of the stimulus (false positive) and until the end of the trial. The `Change point` method seems to be the most conservative one, identifying a single cluster spanning from approximately +60ms to +450ms. The `Holm`, `Cluster mass`, `TFCE`, and `BGAM` methods produce roughly similar estimates of onset and offset, ranging from approximately +60ms to +650ms (considering only the first and last identified timesteps), although the `BGAM` method seems to result in fewer clusters.

```{r fig-onset-offset, echo = FALSE, results = "hide", fig.width = 8, fig.asp = 0.6, fig.cap = "Group-level average decoding performance through time with clusters of higher-than-chance decoding performance as identified by each method [data from @nalborczyk:inprep]."}
# importing the reshaped MEG decoding results
decoding_df <- read.csv(file = "data/decoding_results_reshaped.csv") %>%
    # removing the last participant (to get a round number of participants)
    mutate(participant_id = cur_group_id(), .by = participant) %>%
    dplyr::filter(participant_id < 33) %>%
    select(-participant_id)

# averaging across participants
decoding_summary_df <- decoding_df %>%
    summarise(
        auc_mean = mean(auc),
        auc_sd = sd(auc),
        .by = c(participant, time)
        )

# defining the chance level
chance_level <- 0.5

# computing the SESOI as the baseline upper 95% quantile
full_sesoi <- decoding_summary_df %>%
    filter(time < 0) %>%
    summarise(auc_mean = mean(auc_mean), .by = time) %>%
    summarise(quantile(x = auc_mean, probs = 0.90) ) %>%
    pull() - chance_level

# importing Python modules
use_condaenv("r-reticulate3", conda = "~/miniforge3/bin/conda", required = TRUE)

# fitting the final GAM
full_gam <- brm(
    auc_mean ~ s(time, bs = "cr", k = 50),
    data = decoding_summary_df,
    family = gaussian(),
    warmup = 2000,
    iter = 10000,
    chains = 8,
    cores = 8,
    file = "models/full_decoding_gam.rds"
    )

# sanity check
cat("\nNumber of posterior samples in the full model:", ndraws(full_gam) )

# computing the posterior probability
prob_y_above_0 <- data.frame(time = unique(full_gam$data$time) ) %>%
    # retrieving the posterior samples
    add_epred_draws(object = full_gam) %>%
    # using a subset of posterior samples
    # add_epred_draws(object = full_gam, ndraws = n_post_samples) |>
    # converting to dataframe
    data.frame() %>%
    # computing mean posterior probability at the group level
    group_by(time) %>%
    summarise(m = mean(.epred > (0 + chance_level + full_sesoi) ) ) %>%
    # summarise(m = mean(.epred >= full_sesoi) ) |>
    mutate(prob_ratio = m / (1 - m) ) %>%
    # ensuring there is no 0 or +Inf values
    mutate(prob_ratio = pmin(prob_ratio, ndraws(full_gam) ) ) %>%
    mutate(prob_ratio = pmax(prob_ratio, 1 / ndraws(full_gam) ) ) %>%
    ungroup()

# importing home-made helper functions
source("code/utils.R")

# importing Python modules
np <- import("numpy")
mne <- import("mne")
        
# defining alpha and posterior odds thresholds        
alpha_level <- 0.05
post_prob_ratio_threshold <- 20

# finding clusters
onset_offset_brms <- find_clusters(
    data = prob_y_above_0 %>% select(time, value = prob_ratio),
    threshold = post_prob_ratio_threshold
    ) %>%
    mutate(split_id = "full") %>%
    select(split_id, cluster_id, onset = cluster_onset, offset = cluster_offset) %>%
    mutate(method = "brms") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

# converting data to a matrix (for later use in MNE functions)
data_matrix <- matrix(
    data = decoding_summary_df$auc_mean,
    ncol = length(unique(decoding_summary_df$time) ),
    byrow = TRUE
    )

# massive univariate t-tests
tests_results <- decoding_summary_df %>%
    group_by(time) %>%
    summarise(
        tval = t.test(x = auc_mean, mu = chance_level)$statistic^2,
        pval = t.test(x = auc_mean, mu = chance_level)$p.value
        ) %>%
    mutate(
        pval_bh = p.adjust(p = pval, method = "BH"),
        pval_by = p.adjust(p = pval, method = "BY"),
        pval_holm = p.adjust(p = pval, method = "holm")
        ) %>%
    ungroup()

# using the binary segmentation method to identify onsets and offsets
res <- cpt.meanvar(data = tests_results$tval, method = "BinSeg", Q = 2)

onset_offset_cpt <- data.frame(
    split_id = "full",
    cluster_id = 1,
    onset = tests_results$time[res@cpts[1]],
    offset = tests_results$time[res@cpts[2]]
    ) %>%
    mutate(method = "cpt") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

# finding clusters
onset_offset_raw_p <- find_clusters(
    data = tests_results %>% mutate(pval = pval * (-1) ) |> select(time, value = pval),
    threshold = -alpha_level
    ) %>%
    mutate(split_id = "full") %>%
    select(split_id, cluster_id, onset = cluster_onset, offset = cluster_offset) %>%
    mutate(method = "raw_p") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

onset_offset_pval_bh <- find_clusters(
    data = tests_results %>% mutate(pval = pval_bh * (-1) ) |> select(time, value = pval),
    threshold = -alpha_level
    ) %>%
    mutate(split_id = "full") %>%
    select(split_id, cluster_id, onset = cluster_onset, offset = cluster_offset) %>%
    mutate(method = "pval_bh") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

onset_offset_pval_by <- find_clusters(
    data = tests_results %>% mutate(pval = pval_by * (-1) ) |> select(time, value = pval),
    threshold = -alpha_level
    ) %>%
    mutate(split_id = "full") %>%
    select(split_id, cluster_id, onset = cluster_onset, offset = cluster_offset) %>%
    mutate(method = "pval_by") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

onset_offset_pval_holm <- find_clusters(
    data = tests_results %>% mutate(pval = pval_holm * (-1) ) |> select(time, value = pval),
    threshold = -alpha_level
    ) %>%
    mutate(split_id = "full") %>%
    select(split_id, cluster_id, onset = cluster_onset, offset = cluster_offset) %>%
    mutate(method = "pval_holm") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

# running the MNE cluster-based permutation
p_values_cluster_mass <- freq_stats_cluster_matrix(
    X = data_matrix - chance_level,
    timesteps = unique(decoding_summary_df$time),
    cluster_type = "mass",
    alpha_level = alpha_level,
    verbose = FALSE
    )
p_values_tfce <- freq_stats_cluster_matrix(
    X = data_matrix - chance_level,
    timesteps = unique(decoding_summary_df$time),
    cluster_type = "tfce",
    alpha_level = alpha_level,
    verbose = FALSE
    )

onset_offset_cluster_mass <- find_clusters(
    data = p_values_cluster_mass %>% mutate(pval = pval*(-1) ) %>% select(time, value = pval),
    threshold = -alpha_level
    ) %>%
    mutate(split_id = "full") %>%
    select(split_id, cluster_id, onset = cluster_onset, offset = cluster_offset) %>%
    mutate(method = "cluster_mass") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

onset_offset_cluster_tfce <- find_clusters(
    data = p_values_tfce %>% mutate(pval = pval*(-1) ) %>% select(time, value = pval),
    threshold = -alpha_level
    ) %>%
    mutate(split_id = "full") %>%
    select(split_id, cluster_id, onset = cluster_onset, offset = cluster_offset) %>%
    mutate(method = "cluster_tfce") %>%
    pivot_longer(cols = onset:offset) %>%
    select(split_id, method, cluster_id, onset_offset = name, value) %>%
    data.frame()

# binding all results together
full_reliability_results <- bind_rows(
    onset_offset_brms, onset_offset_raw_p, onset_offset_pval_bh,
    onset_offset_pval_by, onset_offset_pval_holm, onset_offset_cpt,
    onset_offset_cluster_mass, onset_offset_cluster_tfce
    ) %>%
    mutate(sesoi = full_sesoi) %>%
    pivot_wider(names_from = onset_offset, values_from = value) %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
                "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (our method)"
                )
            )
        ) %>%
    mutate(y_offset = 0.5 - as.numeric(factor(method) ) * 0.01 - 0.01)

# importing results
# reliability_results <- readRDS(file = "results/reliability_results_100splits.rds")

# loading all RDS files
# files <- list.files(
#     path = "results/reliability_cluster_results",
#     pattern = ".rds", 
#     full.names = TRUE
#     )
# files <- list.files(
#     # path = "reliability_cluster_results",
#     path = "reliability_jaccard_cluster_results",
#     pattern = ".rds", 
#     full.names = TRUE
#     )

# importing the data splits
# data_splits <- list.files(
#     path = "reliability_data_splits",
#     pattern = ".csv",
#     full.names = TRUE
#     )

# sorting files numerically by the number in the filename
# files <- files[order(readr::parse_number(files) )]
# data_splits <- data_splits[order(readr::parse_number(data_splits) )]

# reading and binding all files
# reliability_results <- map2_dfr(
#     .x = files,
#     .y = seq_along(files),
#     .f = ~ readRDS(.x)
#     # .f = ~ readRDS(.x) |> mutate(sim_id = .y)
#     # .f = function (file, index) {
#     #     offset <- (index - 1) * splits_per_file
#     #     readRDS(file) %>% mutate(split_id = as.numeric(split_id) ) %>%
#     #         mutate(split_id = split_id + offset) %>%
#     #         mutate(split_id = formatC(x = split_id, width = 3, flag = 0) ) 
#     #     }
#     )

# checking errors
# reliability_results %>%
#     filter(!is.na(error) )

# removing NAs
# reliability_results <- reliability_results %>%
#     # select(-error, -stringAsFactors) %>%
#     na.omit()

# sanity checks (N=1122)
# unique(reliability_results$split_id)
# n_distinct(reliability_results$split_id)

# reading and binding all files, adding a split_id
# splits_results <- map2_dfr(
#     .x = data_splits,
#     .y = seq_along(data_splits),
#     .f = ~ read.csv(.x)
#     ) %>%
#     # mutate(array_id = as.numeric(array_id) ) %>%
#     mutate(split_id = paste(array_id, sprintf("s%03d", split_number), sep = "_") ) %>%
#     # removing unused columns
#     select(-split_number, -array_id)

# or reading splits list directly
# splits_results <- read.csv(file = "uniform_jaccard_data_splits.csv")
# splits_results <- read.csv(file = "uniform_jaccard_data_splits_100pairs.csv")
# splits_results <- read.csv(file = "data/uniform_jaccard_data_splits_1000pairs.csv")

# extracting the full data estimates
# full_reliability_results <- reliability_results %>%
#     # filter(split_id == max(split_id) ) %>%
#     # filter(str_detect(string = split_id, pattern = "_s051") ) %>%
#     # dplyr::filter(split_id == "a001_s051") %>%
#     dplyr::filter(split_id == "1_full") %>%
#     # keeping only the first and last timestep
#     # group_by(split_id, method, onset_offset) %>%
#     # summarise(
#     #     value = if (onset_offset[1] == "onset") min(value) else max(value),
#     #     .groups = "drop"
#     #     ) %>%
#     # ungroup() %>%
#     # pivot_longer(cols = -split_id, names_to = "measure", values_to = "value") %>%
#     # data.frame() %>%
#     # select(-split_id) %>%
#     # separate(measure, into = c("type", "method"), sep = "_", extra = "merge") %>%
#     pivot_wider(names_from = onset_offset, values_from = value) %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
#                 "cluster_tfce", "cpt", "brms"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "BGAM (our method)"
#                 )
#             )
#         ) %>%
#     mutate(y_offset = 0.5 - as.numeric(factor(method) ) * 0.01 - 0.01)

# checking model's predictions against raw data
decoding_df %>%
    summarise(
        auc_mean = mean(auc),
        auc_sd = sd(auc),
        auc_se = sd(auc) / sqrt(n() ),
        .by = time
        ) %>%
    ggplot(aes(x = time, y = auc_mean) ) +
    geom_hline(yintercept = 0.5, linetype = 2) +
    geom_vline(xintercept = 0.0, linetype = 2) +
    # annotate(
    #     geom = "rect",
    #     xmin = -Inf, xmax = Inf,
    #     ymin = chance_level - sesoi, ymax = chance_level + sesoi,
    #     fill ="orangered",
    #     alpha = 0.2
    #     ) +
    geom_segment(
        data = full_reliability_results,
        aes(
            x = onset, xend = offset,
            y = y_offset, yend = y_offset,
            colour = method,
            ),
        inherit.aes = FALSE,
        lineend = "round",
        linewidth = 1.5
        ) +
    geom_ribbon(
        aes(ymin = auc_mean - auc_se, ymax = auc_mean + auc_se),
        alpha = 0.2,
        ) +
    geom_line(
        data = decoding_summary_df %>%
            summarise(auc_mean = mean(auc_mean), .by = time),
        aes(x = time, y = auc_mean), inherit.aes = FALSE
        ) +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(x = "Time (s)", y = "Decoding accuracy (ROC AUC)", colour = NULL)

# saving the plot
# ggsave(
#     filename = "figures/meg_decoding_results_all_methods.png",
#     width = 12, height = 6, dpi = 300,
#     device = "png"
#     )
```

We then assessed the sensitivity of the various methods using a form of permutation-based sensitivity study, which consisted of the following steps. First, we created a large number of split halves of the data, that is, subsets of the dataset containing only 16 out of 32 participants. For each possible pair of subsets, we have 16 possible levels of overlap/similarity that can be quantified using the Jaccard index, ranging from 0 (perfectly disjoints subsets) to $\approx0.88$ (identical subsets except one participant). For each of these 16 levels of Jaccard similarity, we created 1,000 pairs of subsets, resulting in 16,000 pairs of subsets in total. For each of these pairs, we estimated the onset and offset according to each method and computed the absolute difference in onset/offset estimates. Finally, we estimated the Spearman's rank correlation coefficient (which quantifies the strength of a monotonic relation between two variables) between the Jaccard similarity and the absolute difference in onset/offset estimates. The rational for this procedure is that sensitive methods should produce similar onset/offset estimates for similar subsets and dissimilar onset/offset estimates for dissimilar subsets. The results of this procedure are summarised in @fig-splits-similarity.

<!--

Finally, we fitted a polynomial (cubic) regression to predict the absolute difference in onset/offset estimates according to the Jaccard similarity. This analysis allowed us to disentangle two properties of each method: i) whether it produces similar onset/offset estimates for similar subsets (in the limit case where subsets are identical, this is reflected by the intercept parameter), and ii) how sensitive each method is to variation in subsets' similarity (which is reflected by the two slope parameters, for the linear and cubic terms, respectively). In simple terms, sensitive methods should produce similar onset/offset estimates for similar subsets and dissimilar onset/offset estimates for dissimilar subsets. The results of this procedure are summarised in @fig-splits-similarity.

This figure reveals that the `BGAM` *onset and offset* estimates on each split are the closest to the estimates from the full dataset on average (0ms difference for the onset estimate and 5ms difference for the offset estimate). The `Raw p-value` method has similar performance for the offset, but given the aberrant estimates it produces (cf. @fig-onset-offset), the fact that it is consistent between data splits and the full dataset is not convincing on its own. The `Change point` method also has a very good performance (i.e., very low difference between split estimates and full estimates), but produces too short cluster of significant decoding performance (cf. @fig-onset-offset).[^3] Overall, the figure reveals that for all other methods, split datasets produce later onset estimates and earlier offset estimates (compared to the estimates from the model fitted on the full dataset). These results highlight some desirable properties for a method aiming to precisely and reliably estimate the onset and offset of M/EEG effects, namely, it should i) have good asymptotic properties on simulated data, ii) provide sensible identified clusters in actual data, and iii) provide reliable/stable estimates across various partitions of the data. All these desiderata seem to be fulfilled by the BGAM approach.

-->

```{r fig-splits-similarity, eval = TRUE, echo = FALSE, fig.width = 12, fig.asp = 0.6, fig.cap = "Relation between data subsets' dissimilarity (x-axis) and difference in onset (blue) and offset (orange) estimates (y-axis) according to each method."}
# reading splits list directly
# splits_results <- read.csv(file = "uniform_jaccard_data_splits.csv")
# splits_results <- read.csv(file = "uniform_jaccard_data_splits_100pairs.csv")
splits_results <- read.csv(file = "data/uniform_jaccard_data_splits_1000pairs.csv")

# listing all RDS files
files <- list.files(
    path = "results/reliability_jaccard_cluster_results",
    pattern = ".rds", 
    full.names = TRUE
    )

# sorting files numerically by the number in the filename
files <- files[order(readr::parse_number(files) )]

# reading and binding all files
reliability_results <- map2_dfr(
    .x = files,
    .y = seq_along(files),
    .f = ~ readRDS(.x)
    )

# checking errors
# reliability_results %>%
#     filter(!is.na(error) )

# removing NAs
reliability_results <- reliability_results %>%
    # select(-error, -stringAsFactors) %>%
    na.omit()

# This analysis assesses whether the method is "self-consistent" across data splits...
# Step 1: creating a list of participants per split_id
# split_participants <- splits_results %>%
#     rename(split_id = split_number) %>%
#     group_by(split_id) %>%
#     dplyr::filter(group == "A") %>%
#     summarise(participants = list(participant) ) %>%
#     ungroup()

# Step 2: creating all pairwise combinations of split_ids
# split_pairs <- expand.grid(
#     split_id1 = split_participants$split_id,
#     split_id2 = split_participants$split_id,
#     stringsAsFactors = FALSE
#     ) %>%
#     # avoiding duplicates and self-comparison
#     dplyr::filter(split_id1 < split_id2) 

# Step 3: joining the participant lists for each pair
# split_pairs <- split_pairs %>%
#     left_join(split_participants, by = c("split_id1" = "split_id") ) %>%
#     rename(participants1 = participants) %>%
#     left_join(split_participants, by = c("split_id2" = "split_id") ) %>%
#     rename(participants2 = participants)

# Step 4: computing overlap as size of intersection / union
# see also https://www.rdocumentation.org/packages/genieclust/versions/1.1.6/topics/compare_partitions
# split_pairs <- split_pairs %>%
#     mutate(
#         intersection = map2_int(participants1, participants2, ~length(intersect(.x, .y) ) ),
#         union = map2_int(participants1, participants2, ~length(union(.x, .y) ) ),
#         jaccard_similarity = intersection / union,
#         # jaccard_similarity = map2_dbl(participants1, participants2, ~jaccard(.x, .y) )
#         # ari = map2(participants1, participants2, ~ARI(.x[[1]], .y[[1]]) ),
#         # nmi = map2_dbl(participants1, participants2, ~NMI(.x[[1]], .y[[1]]) ),
#         # ami = map2_dbl(participants1, participants2, ~AMI(.x[[1]], .y[[1]]), .progress = TRUE)
#         )

# checking the results
# sort(unique(split_pairs$intersection) )
# hist(split_pairs$intersection)

# plotting it
# split_pairs %>%
#     ggplot(aes(x = intersection, y = jaccard_similarity) ) +
#     geom_point() +
#     geom_smooth(method = "lm") +
#     theme_bw()

##########################################################
# now computing similarity of onset and offset estimates #
##########################################################

# Step 1: creating a list of participants per split_id
# reliability_splits <- reliability_results %>%
#     # filter(str_detect(string = split_id, pattern = "full", negate = TRUE) ) %>%
#     # mutate(split_id = str_sub(string = split_id, start = 6, end = -1) )
#     dplyr::filter(split_id != "a001_s051") %>%
#     # keeping only the first and last timestep
#     group_by(split_id, method, onset_offset) %>%
#     summarise(
#         value = if (onset_offset[1] == "onset") min(value) else max(value),
#         .groups = "drop"
#         ) %>%
#     ungroup()

# Step 2: creating all pairwise combinations of split_ids
# and computing the absolute difference of estimated onset/offset
# reliability_splits_results <- reliability_splits %>%
#   # joining the data frame with itself by method and onset_offset
#   inner_join(
#       reliability_splits,
#       by = c("method", "onset_offset"),
#       suffix = c("_1", "_2")
#       ) %>%
#   # keeping only unique pairs (avoid self-pairs and duplicates)
#   dplyr::filter(split_id_1 < split_id_2) %>%
#   # computing the absolute difference in value
#   mutate(value_diff = abs(value_1 - value_2) ) %>%
#   select(method, onset_offset, split_id_1, split_id_2, value_diff)

# joining these results with splits similarity
# head(reliability_splits_results)
# full_df <- split_pairs %>%
#     select(split_id_1 = split_id1, split_id_2 = split_id2, jaccard_similarity) %>%
#     # head()
#     inner_join(reliability_splits_results, by = c("split_id_1", "split_id_2") )

# checking similarity values and effectives
# full_df %>%
#     summarise(
#         effective_sim = n(),
#         .by = c(onset_offset, method, jaccard_similarity)
#         ) %>%
#     arrange(jaccard_similarity)

########################################################################
# or working directly from pre-existing jaccard similarity csv file
######################################################################

reliability_results2 <- reliability_results %>%
    # dplyr::filter(str_detect(string = split_id, pattern = "full", negate = TRUE) ) %>%
    # keeping only the first and last timestep
    group_by(split_id, method, onset_offset) %>%
    summarise(
        value = if (onset_offset[1] == "onset") min(value) else max(value),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    mutate(split_id = str_sub(string = split_id, start = 6, end = -3) ) %>%
    # removing split with error
    # filter(split_id != "s00171") %>%
    group_by(split_id, method, onset_offset) %>%
    mutate(value_diff = abs(diff(value) ) ) %>%
    # mutate(value_diff = diff(value) ) %>%
    # mutate(value_diff = diff(value)^2) %>%
    # rmse = sqrt(mean(error)^2),
    ungroup() %>%
    select(-value) %>%
    distinct()

splits_results2 <- splits_results %>%
    mutate(split_id = formatC(x = split_id, width = 5, flag = 0) ) %>%
    mutate(split_id = paste0("s", split_id) ) %>%
    select(-group, -participant) %>%
    distinct()

full_df <- full_join(reliability_results2, splits_results2, by = "split_id")

# sanity checks
# table(full_df$jaccard)
# unique(full_df$jaccard)
# n_distinct(full_df$jaccard)
# unique(splits_results2$jaccard)
# unique(reliability_results2$split_id)

# find rows with NAs
# full_df[!complete.cases(full_df), ] %>% data.frame()
# reliability_results2 %>% filter(split_id == "s00088")
# reliability_results %>% filter(str_detect(string = split_id, pattern = "s00088") )

# custom function to implement min-max scaling (i.e., between 0 and 1)
# minMax <- function (x) {
#     
#     std_score <- (x - min(x) ) / (max(x) - min(x) )
#     
#     return (std_score)
#     
# }

# polynomial regression degree
# poly_reg_degree <- 2

# or fitting a GAM for each combination
# library(ggpmisc)
# library(broom)
# library(glue)
# library(mgcv)
# 
# # 1. Nest data by method & onset_offset
# model_df <- full_df %>%
#     na.omit() %>%
#     group_by(method, onset_offset, jaccard) %>%
#     summarise(
#         mean_value_diff = mean(x = value_diff, na.rm = TRUE),
#         sem = sd(x = value_diff, na.rm = TRUE) / sqrt(n() )
#         ) %>%
#     ungroup() %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
#                 "cluster_tfce", "cpt", "brms"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "BGAM (our method)"
#                 )
#             )
#         ) %>%
#     # mutate(jaccard = 1-jaccard) %>%
#     filter(!is.na(mean_value_diff), !is.na(jaccard) ) %>%
#     group_by(method, onset_offset) %>%
#     nest()
# 
# # 2. Fit a GAM to each subset
# model_df <- model_df %>%
#     mutate(
#         gam_model = map(
#             data, ~ gam(mean_value_diff ~ s(jaccard),
#                         # family = Gamma(link = "log"),
#                         data = .x)
#             )
#         )
# 
# # 3. Generate pretty label with edf and R² for plotting
# model_df <- model_df %>%
#     mutate(
#         eq_label = map2_chr(
#             gam_model,
#             data,
#             ~ {
#                 r2 <- summary(.x)$r.sq
#                 edf <- summary(.x)$s.table[, "edf"] %>% round(2)
#                 glue("edf = {edf}, R² = {round(r2, 2)}")
#                 })
#         )
# 
# # 4. Create predictions for plotting
# model_df <- model_df %>%
#   mutate(
#     preds = map2(data, gam_model, ~ {
#       tibble(
#         jaccard = seq(min(.x$jaccard), max(.x$jaccard), length.out = 100)
#       ) %>%
#         mutate(mean_value_diff = predict(.y, newdata = ., type = "response"))
#     })
#   )
# 
# # 5. Unnest everything for ggplot
# plot_data <- model_df %>%
#   select(method, onset_offset, eq_label, preds) %>%
#   unnest(preds)
# 
# # 6. Create base plot with data points and fitted GAM lines
# full_df %>%
#     na.omit() %>%
#     # nrow()
#     # dplyr::filter(jaccard < 1 & jaccard > 0) %>%
#     group_by(method, onset_offset, jaccard) %>%
#     summarise(
#         mean_value_diff = mean(x = value_diff, na.rm = TRUE),
#         # mean_value_diff = sqrt(mean(x = value_diff, na.rm = TRUE) ),
#         sem = sd(x = value_diff, na.rm = TRUE) / sqrt(n() )
#         ) %>%
#     ungroup() %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
#                 "cluster_tfce", "cpt", "brms"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "BGAM (our method)"
#                 )
#             )
#         ) %>%
#     ggplot(aes(x = 1-jaccard, y = mean_value_diff, colour = onset_offset) ) +
#     geom_point(alpha = 0.6) +
#     geom_line(data = plot_data, aes(y = mean_value_diff), size = 1.2) +
#     geom_label(
#         data = plot_data %>% group_by(method, onset_offset) %>% slice(1),
#         aes(label = eq_label, x = 0.05, y = Inf),
#         vjust = 1.2, hjust = 0,
#         fill = "white", colour = "black",
#         size = 3, inherit.aes = FALSE
#         ) +
#     facet_wrap(~method, nrow = 2) +
#     theme_bw() +
#         labs(
#         x = "Jaccard dissimilarity",
#         y = "Average absolute difference in onset/offset estimate (s)",
#         colour = ""
#         )

# plotting the results
full_df %>%
    na.omit() %>%
    mutate(jaccard_dis = 1-jaccard) %>%
    # dplyr::filter(jaccard < 1 & jaccard > 0) %>%
    # dplyr::filter(1-jaccard > 0 & 1-jaccard < 1) %>%
    dplyr::filter(jaccard_dis > 0) %>%
    group_by(method, onset_offset, jaccard_dis) %>%
    summarise(
        mean_value_diff = mean(x = value_diff, na.rm = TRUE),
        sem = sd(x = value_diff, na.rm = TRUE) / sqrt(n() )
        ) %>%
    ungroup() %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
                "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (our method)"
                )
            )
        ) %>%
    ggplot(
        aes(
            # x = minMax(1-jaccard_similarity),
            # x = 1-jaccard_similarity,
            # x = max(jaccard)-jaccard,
            x = jaccard_dis,
            y = mean_value_diff,
            fill = onset_offset,
            colour = onset_offset,
            group = onset_offset
            )
        ) +
    # geom_point(size = 0.5, alpha = 0.05) +
    geom_smooth(
        # method = "glm",
        # method.args = list(family = Gamma(link = "inverse") ),
        # method.args = list(family = Gamma(link = "log") ),
        # method.args = list(family = inverse.gaussian() ),
        method = "lm",
        # method = "gam",
        # formula = y ~ stats::poly(x, degree = 3, raw = TRUE),
        # formula = y ~ stats::poly(x, degree = poly_reg_degree, raw = TRUE),
        # formula = rank(y) ~ rank(x),
        # formula = y ~ logit_scaled(x),
        # method.args = list(family = "binomial"),
        alpha = 0.2,
        fullrange = TRUE
        ) +
    # geom_point() +
    geom_pointrange(
        aes(ymin = mean_value_diff - sem, ymax = mean_value_diff + sem),
        fatten = 1
        ) +
    stat_cor(
        # showing the R and R^2
        # aes(label = paste(..r.label.., ..rr.label.., sep = "~`,`~") ),
        aes(label = paste(after_stat(r.label), after_stat(rr.label), sep = "~`,`~") ),
        method = "spearman",
        # method = "pearson",
        # method = "kendall",
        digits = 3,
        # label.x = 0,
        # label.y = 1,
        label.x.npc = "left",
        label.y.npc = "top",
        geom = "label",
        colour = "white",
        size = 3.5
        ) +
    # stat_regline_equation(
    #     aes(label = paste(..eq.label.., ..rr.label.., sep = "~`,`~") ),
    #     # formula = rank(y) ~rank(x),
    #     # formula = y ~ logit_scaled(x),
    #     formula = y ~ stats::poly(x = x, degree = poly_reg_degree, raw = TRUE),
    #     # method = "glm",
    #     # method.args = list(family = Gamma(link = "log")),
    #     label.x = 0,
    #     label.y.npc = "top",
    #     geom = "label",
    #     colour = "white",
    #     size = 3.5
    #     ) +
    # scale_y_log10(guide = "axis_logticks", limits = c(NA, 0.5) ) +
    scale_y_log10(guide = "axis_logticks", limits = c(NA, 1) ) +
    facet_wrap(~method, nrow = 2) +
    theme_bw(base_size = 12) +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 2) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 2) ) +
    labs(
        x = "Jaccard dissimilarity",
        y = "Average absolute difference in onset/offset estimate (s)",
        fill = "",
        colour = ""
        )

# saving the plot
# ggsave(
#     filename = "figures/splits_summary_similarity_100pairs.png",
#     width = 16, height = 8, dpi = 200,
#     device = "png"
#     )
```

This figure shows that, among the methods that performed best in the simulation study (i.e., `Cluster mass`, `Change point`, and `BGAM`), onset estimates remain highly stable across subsets of participants with varying Jaccard similarity, it varies from around 1ms for most similar subsets to around 10ms for most dissimilar subsets. Additionally, for both onset and offset estimates, the average pairwise difference increases monotonically with Jaccard dissimilarity, as indicated by the Spearman's rank correlation coefficient. For the onset estimates, the `Holm`, `Cluster mass`, `TFCE`, and `BGAM` methods exhibit the strongest monotonic relation with subset similarity (all $\rho\text{s} > 0.9$), whereas for offset estimates, all methods demonstrate excellent performance (all $\rho\text{s} > 0.9$) with the `BGAM` method showing the highest sensitivity ($\rho\approx0.994$). However, given the aberrant clusters identified by the `Raw p-value`, `FDR BH95`, and `FDR BY01` methods (see @fig-onset-offset), their sensitivity to variation in subset similarity is not meaningful.

<!--

@fig-reliability shows the median difference between the onset and offset estimates from each data split and the onset and offset estimates from the full dataset (x-axis) along with the variance of its onset and offset estimates across data splits (error bar). This figure reveals that the `BGAM` *onset and offset* estimates on each split are the closest to the estimates from the full dataset on average (0ms difference for the onset estimate and 5ms difference for the offset estimate). The `Raw p-value` method has similar performance for the offset, but given the aberrant estimates it produces (cf. @fig-onset-offset), the fact that it is consistent between data splits and the full dataset is not convincing on its own. The `Change point` method also has a very good performance (i.e., very low difference between split estimates and full estimates), but produces too short cluster of significant decoding performance (cf. @fig-onset-offset).[^3] Overall, the figure reveals that for all other methods, split datasets produce later onset estimates and earlier offset estimates (compared to the estimates from the model fitted on the full dataset). These results highlight some desirable properties for a method aiming to precisely and reliably estimate the onset and offset of M/EEG effects, namely, it should i) have good asymptotic properties on simulated data, ii) provide sensible identified clusters in actual data, and iii) provide reliable/stable estimates across various partitions of the data. All these desiderata seem to be fulfilled by the BGAM approach.

[^3]: As in @rousselet_using_2025, we fixed the number of expected change points to two in the binary segmentation algorithm, thus producing always one cluster.

```{r fig-reliability, eval = FALSE, echo = FALSE, cache = FALSE, fig.width = 8, fig.asp = 0.5, fig.cap = "Median error and median absolute deviation of the error for the onset (left) and offset (right) estimates according to each method. Methods are ordered from smallest (top) to largest (bottom) median absolute error separately for the onset and offset estimates."}
# extracting the full data estimates
full_reliability_results <- reliability_results %>%
    # filter(split_id == max(split_id) ) %>%
    filter(split_id == "a001_s051") %>%
    # keeping only the first and last timestep
    group_by(split_id, method, onset_offset) %>%
    summarise(
        value = if (onset_offset[1] == "onset") min(value) else max(value),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    data.frame() %>%
    select(-split_id)

# analysing the split results
# reliability_results %>%
#     slice(1:nrow(.)-1) %>%
#     pivot_longer(cols = -split_id, names_to = "measure", values_to = "value") %>%
#     data.frame() %>%
#     left_join(
#         full_reliability_results,
#         by = "measure",
#         suffix = c("_split", "_full")
#         ) %>%
#     # mutate(error = abs(value_split - value_full) ) %>%
#     mutate(error = value_split - value_full) %>%
#     # mutate(error = (value_split - value_full)^2) %>%
#     separate(measure, into = c("type", "method"), sep = "_", extra = "merge") %>%
#     group_by(type, method) %>%
#     summarise(
#         MAE = median(error, na.rm = TRUE),
#         # variance = var(error, na.rm = TRUE),
#         variance = var(value_split, na.rm = TRUE)
#         # variance = sum(error, na.rm = TRUE)
#         # .groups = "drop"
#         ) %>%
#     ungroup() %>%
#     # now plotting
#     mutate(type = factor(x = type, levels = c("onset", "offset") ) ) %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "p", "bh", "by", "holm", "cluster_mass",
#                 "cluster_tfce", "cpt", "brms", "brms_full"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "brms", "brms_full"
#                 ),
#             )
#         ) %>%
#     ggplot(
#         aes(
#             x = MAE,
#             y = variance,
#             colour = method,
#             fill = method
#             )
#         ) +
#     geom_vline(xintercept = 0, linetype = 2) +
#     geom_point(
#         size = 2,
#         pch = 21,
#         colour = "white",
#         show.legend = FALSE
#         ) +
#     geom_label_repel(
#         aes(label = method),
#         colour = "white",
#         size = 3,
#         segment.color = NA,
#         max.overlaps = 20,
#         show.legend = FALSE
#         ) +
#     scale_y_log10() +
#     facet_wrap(~type, scales = "free") +
#     scale_fill_manual(values = met.brewer(name = "Johnson", n = 9) ) +
#     scale_colour_manual(values = met.brewer(name = "Johnson", n = 9) ) +
#     labs(
#         x = "Average (median) difference to full dataset (s)",
#         y = "Variance across splits"
#         )

# defining a consistent colour palette using original method levels
original_methods <- c(
    "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
    "TFCE", "Change point", "BGAM (our method)"
    )

color_palette <- setNames(
    met.brewer(name = "Johnson", n = length(original_methods) ),
    original_methods
    )

# sanity check
# reliability_results %>%
#     filter(str_detect(string = split_id, pattern = "s051", negate = TRUE) ) %>%
#     pull(split_id) %>%
#     n_distinct()

# plotting the error in descending order
reliability_results %>%
    filter(str_detect(string = split_id, pattern = "s051", negate = TRUE) ) %>%
    data.frame() %>%
    # keeping only the first and last timestep
    group_by(split_id, method, onset_offset) %>%
    summarise(
        value = if (onset_offset[1] == "onset") min(value) else max(value),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    select(-split_id) %>%
    left_join(
        full_reliability_results,
        # by = "measure",
        by = c("method", "onset_offset"),
        suffix = c("_split", "_full")
        ) %>%
    group_by(method, onset_offset) %>%
    # mutate(error = abs(value_split - value_full) ) %>%
    mutate(error = value_split - value_full) %>%
    summarise(
        # mean_error = mean(error, na.rm = TRUE),
        # variance = var(error, na.rm = TRUE),
        # variance = var(value_split, na.rm = TRUE)
        # variance = sd(error, na.rm = TRUE) / sqrt(n() )
        # variance = sum(error, na.rm = TRUE)
        mean_error = mean(error, na.rm = TRUE),
        median_error = median(error, na.rm = TRUE),
        # variance = IQR(error, na.rm = TRUE)
        mad = mad(error, na.rm = TRUE),
        variance = var(error, na.rm = TRUE)
        # .groups = "drop"
        ) %>%
    ungroup() %>%
    # head()
    mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
                "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (our method)"
                ),
            )
        ) %>%
    mutate(
        method_reordered = tidytext::reorder_within(
            x = method,
            by = 1 / abs(median_error),
            # by = 1 / abs(mean_error),
            within = onset_offset
            )
        ) %>%
    ggplot(
        aes(
            x = median_error,
            xmin = median_error - mad,
            xmax = median_error + mad,
            # x = mean_error,
            # xmin = mean_error - variance,
            # xmax = mean_error + variance,
            y = method_reordered,
            colour = method,
            fill = method
            )
        ) +
    geom_vline(xintercept = 0, linetype = 2) +
    geom_pointrange(
        size = 0.25,
        linewidth = 0.75,
        show.legend = FALSE
        ) +
    facet_wrap(~onset_offset, scales = "free") +
    scale_y_reordered() +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(
        x = "Difference to full dataset (s)",
        y = "Method"
        )

# plotting the distribution of results
# reliability_results %>%
#     # filter(split_id < max(split_id) ) %>%
#     filter(str_detect(string = split_id, pattern = "s051", negate = TRUE) ) %>%
#     data.frame() %>%
#     # keeping only the first and last timestep
#     group_by(split_id, method, onset_offset) %>%
#     summarise(
#         value = if (onset_offset[1] == "onset") min(value) else max(value),
#         .groups = "drop"
#         ) %>%
#     ungroup() %>%
#     select(-split_id) %>%
#     left_join(
#         full_reliability_results,
#         # by = "measure",
#         by = c("method", "onset_offset"),
#         suffix = c("_split", "_full")
#         ) %>%
#     group_by(method, onset_offset) %>%
#     # mutate(error = abs(value_split - value_full) ) %>%
#     mutate(error = value_split - value_full) %>%
#     mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "raw_p", "pval_bh", "pval_by", "pval_holm",
#                 "cluster_mass", "cluster_tfce", "cpt", "brms"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "BGAM (our method)"
#                 ),
#             )
#         ) %>%
#     mutate(
#         method_reordered = tidytext::reorder_within(
#             x = method,
#             by = abs(error),
#             fun = median,
#             # by = 1 / abs(median_error),
#             # by = 1 / abs(mean_error),
#             within = onset_offset
#             )
#         ) %>%
#     # mutate(
#     #     method_reordered = fct_reorder(
#     #         method, error, .fun = ~abs(median(.x) ), .na_rm = TRUE
#     #         )
#     #     ) %>%
#     # arrange(desc(error) ) %>%
#     # arrange(error) %>%
#     # head()
#     ggplot(
#         aes(
#             # x = median_error,
#             x = error,
#             # xmin = median_error - mad,
#             # xmax = median_error + mad,
#             # x = mean_error,
#             # xmin = mean_error - variance,
#             # xmax = mean_error + variance,
#             y = method_reordered,
#             colour = method,
#             fill = method
#             )
#         ) +
#     geom_vline(xintercept = 0, linetype = 2) +
#     # geom_pointrange(
#     #     size = 0.25,
#     #     linewidth = 0.75,
#     #     show.legend = FALSE
#     #     ) +
#     stat_slabinterval(
#         n = 200,
#         normalize = "groups",
#         point_interval = "median_qi",
#         # .width = c(0.1, 0.9),
#         # .width = 0.2,
#         .width = 0,
#         interval_size_range = c(1, 1),
#         fatten_point = 1.25,
#         # slab_colour = "white",
#         # slab_linewidth = 0,
#         # density = "unbounded",
#         slab_alpha = 0.5,
#         show.legend = FALSE
#         ) +
#     facet_wrap(~onset_offset, scales = "free") +
#     scale_y_reordered() +
#     scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
#     labs(
#         x = "Estimation error (s)",
#         y = "Method"
#         )
```

```{r tbl-reliability-results, eval = TRUE, echo = FALSE, results = "asis"}
#| tbl-cap: "Summary statistics of the onset and offset estimates for each method (in ms, ordered by the MAE) according to the reliability simulation study."

# extracting the full data estimates
reliability_errors <- reliability_results %>%
    data.frame() %>%
    # keeping only the first and last timestep
    group_by(split_id, method, onset_offset) %>%
    summarise(
        value = if (onset_offset[1] == "onset") min(value) else max(value),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    select(-split_id) %>%
    left_join(
        full_reliability_results,
        by = c("method", "onset_offset"),
        suffix = c("_split", "_full")
        ) %>%
    group_by(method, onset_offset) %>%
    mutate(error = value_split - value_full) %>%
    ungroup()

# summarising the results
reliability_summary <- reliability_errors %>%
    # mutate(
    #     error = case_when(
    #         grepl("onset", onset_offset) ~ value - true_onset,
    #         grepl("offset", onset_offset) ~ value - true_offset
    #         )
    #     ) %>%
    group_by(method, onset_offset) %>%
    mutate(error = 1e3 * error) %>%
    summarise(
        # mean_error = mean(error, na.rm = TRUE),
        median_error = median(error, na.rm = TRUE),
        median_abs_error = median(abs(error), na.rm = TRUE),
        rmse = sqrt(mean(error)^2),
        variance = var(error, na.rm = TRUE),
        mad = mad(error, na.rm = TRUE),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm",
                "cluster_mass", "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (our method)"
                ),
            )
        ) %>%
    mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
    data.frame()

# creating the table
reliability_summary %>%
    mutate(
        onset_offset = factor(
            x = onset_offset,
            levels = c("offset", "onset")
            )
        ) %>%
    # arrange(median_abs_error) %>%
    # arrange(abs(median_error) ) %>%
    # arrange(median_abs_error) %>%
    arrange(abs(median_error) ) %>%
    group_by(onset_offset) %>%
    gt() %>%
    # gt(groupname_col = "onset_offset") %>%
    fmt_number(
        columns = everything(),
        decimals = 2
        ) %>%
    cols_align(
        align = "center",
        columns = everything()
        ) %>%
    cols_align(
        align = "left",
        columns = method
        ) %>%
    cols_label(
        method = "",
        # mean_error = "Bias_mean",
        median_error = "Bias",
        median_abs_error = "MAE",
        rmse = "RMSE",
        variance = "Variance",
        mad = "MAD"
        ) %>%
    tab_options(
        table.align = "center",
        # table.width = "75%",
        table.font.size = 12
        ) %>%
    as_latex()
```

\newpage

-->

# Discussion

In brief, our results show that the model-based approach we introduced outperforms conventional methods such as cluster-based inference in identifying the onset and offset of M/EEG effects. Performance was assessed both on simulated data--allowing us to evaluate the method's ability to recover ground-truth onset and offset values--and on actual MEG data--allowing us to assess its sensitivity to realistic data properties (subset similarity). Together, these results highlight desirable properties for any method aiming to precisely and reliably estimate the onset and offset of M/EEG effects: it should i) recover true onsets and offsets in simulation (good asymptotic behaviour), ii) identify clusters that are interpretable and consistent in empirical data, and iii) show sensitivity to subtle changes in the data. Our approach meets all three of these desiderata.

As with previous simulation studies [e.g., @rousselet2008; @sassenhagen2019], results inevitably depend on design choices, including the specific cluster-forming algorithm and threshold (for cluster-based methods), the signal-to-noise ratio, and the potential degradation of temporal resolution introduced by preprocessing steps such as low-pass filtering. However, these constraints apply equally to all methods tested, so relative differences in performance remain meaningful.

**I am not sure what to do with this paragraph (not sure how to discuss these results/observations):** Interestingly, the TFCE method performed worse than the traditional cluster-sum approach, consistent with the predictions of @rousselet_using_2025 based on the original findings of @smith2009. We also found a striking overlap in the clusters identified by the `Holm` and `TFCE` procedures (cf. @fig-onset-offset and @fig-splits-similarity). Whereas these two approaches are conceptually distinct; `Holm` controlling the family-wise error rate through sequential *p*-value adjustment, `TFCE` enhancing signal based on spatiotemporal support; their similarity in practice may arise because both effectively prioritise extended, moderately strong effects over isolated high-intensity points. This convergence warrants further methodological work. We chose not to include the cluster-depth algorithm [@frossard2022], as prior work already shows its performance to be inferior to cluster-mass approaches in similar contexts [@rousselet_using_2025].

A critical consideration for any model-based approach is that the model must adequately capture the underlying data-generating process. Misspecified models are likely to produce biased or unreliable onset/offset estimates. This underscores the importance of thorough model diagnostics, including posterior predictive checks, fit assessments, and model comparison [@gelman2020]. As with any inferential modelling framework, transparency in assumptions and rigorous validation procedures are indispensable to ensure replicability and generalisability of the results.

Another important methodological consideration concerns the selection of model hyperparameters, such as the threshold for posterior odds and the number of basis functions. Although our simulations suggest that these parameters influence the precision and reliability of onset and offset estimates, optimal values may vary depending on the signal's temporal dynamics and signal-to-noise characteristics. Future work could explore principled approaches to hyperparameter tuning, including cross-validation or fully Bayesian model selection using tools such as leave-one-out cross-validation (LOO-CV) or Bayes factors [@gelman2020]. We provide initial guidance in @apx-basis and advocate for future development of adaptive heuristics to support flexible yet parsimonious model specification.

Currently, our approach estimates temporal effects independently at each sensor (1D temporal data). Extending the current framework to incorporate additional temporal (see @apx-2D) or spatial dimensions would improve both sensitivity and interpretability. Such extensions could draw on methods from spatial epidemiology and geostatistics using either GAMMs or approximate Gaussian process regression [e.g., @rasmussen2005; @riutort-mayol_practical_2023], depending on computational feasibility.

To facilitate adoption, we developed the `neurogram` open-source `R` package [@neurogam], which implements the proposed method using `brms`. The package integrates seamlessly with `MNE-Python` [@gramfort2013], enabling researchers to process M/EEG data in `Python` and import them directly into `R` for model-based inference without cumbersome data export. This interoperability, described in @apx-package, is designed to encourage broader use of model-based approaches in cognitive neuroscience.

In conclusion, we introduced a model-based approach for estimating the onset and offset of M/EEG effects. Across simulated and empirical datasets, we showed that the method yields more precise and sensitive estimates than conventional cluster-based approaches. These results highlight the potential of flexible, model-based alternatives for characterising time-resolved neural dynamics, particularly in applications where accurate temporal localisation is critical.

\newpage

\setcounter{secnumdepth}{0}

# Data and code availability

The simulation results as well as the `R` code to reproduce the simulations are available at: <https://github.com/lnalborczyk/brms_meeg>. The `neurogam R` package is available at <https://github.com/lnalborczyk/neurogam>.

# Packages

```{r, eval = TRUE, echo = FALSE, cache = FALSE}
# checking the included packages
# grateful::scan_packages(pkgs = "Session")

# citing the R packages we have used
cite_packages(
    output = "paragraph",
    omit = c("bamlss", "changepoint", "permuco", "reticulate"),
    out.dir = getwd(),
    dependencies = FALSE,
    quiet = TRUE
    # progress = FALSE,
    # errors = "ignored"
    )
```

# Acknolwedgements

Centre de Calcul Intensif d'Aix-Marseille is acknowledged for granting access to its high performance computing resources.

\newpage

# References

::: {#refs}
:::

\newpage

# Application to 2D time-resolved decoding results (cross-temporal generalisation) {#apx-2D}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

We conducted a cross-temporal generalisation analysis of the decoding data from @nalborczyk:inprep, in which we assessed the performance of classifiers trained and tested at various timesteps of the trial [@king2014]. This analysis was performed at the participant level, resulting in a 2D matrix where each element contains the decoding accuracy (ROC AUC) of a classifier trained at timestep $\text{training}_{i}$ and tested at timestep $\text{testing}_{j}$ for each participant (@fig-timegen).

```{r fig-timegen, eval = TRUE, echo = FALSE, dpi = 300, out.width = "75%", fig.cap = "Group-level average cross-temporal generalisation matrix of decoding performance [data from @nalborczyk:inprep]."}
# importing the MEG decoding data
library(reticulate)
use_condaenv("r-reticulate3", conda = "~/miniforge3/bin/conda", required = TRUE)
np <- import("numpy")
data <- np$load("data/group_level_decoding_4l_2s_vs_7l_2s_visual_blocks_temporal_generalisation_all.npy")

# converting to R array
data_r <- py_to_r(data)

# sanity checks
# str(data_r)
# dim(data_r)

# original time vector (before decimation)
time_points <- seq(from = -0.5, to = 1.0, length.out = dim(data_r)[2])

# removing the first 300ms before stimulus onset
start_idx <- which.min(abs(time_points - (-0.2) ) )
end_idx <- which.min(abs(time_points - 1.0) )

# cutting data_r to keep only time points between -0.2s and 1.0s
data_cut <- data_r[, start_idx:end_idx, start_idx:end_idx]

# updating time vector accordingly
time_points_cut <- time_points[start_idx:end_idx]

# checking new dimensions
# dim(data_cut)
# length(time_points_cut)

# defining a decimation factor
decim <- 4

# selecting every 4th time point
data_cut_decim <- data_cut[, seq(1, dim(data_cut)[2], by = decim), seq(1, dim(data_cut)[3], by = decim)]

# retrieving the number of participants, time points
n_participants <- dim(data_cut_decim)[1]
n_times <- dim(data_cut_decim)[2]

# defining a time vector (if not already defined)
time_points <- seq(from = -0.2, to = 1.0, length.out = n_times)

# converting array to long format using `as.data.frame.table`
timegen_data <- as.data.frame.table(data_cut_decim)

# renaming and converting indices
colnames(timegen_data) <- c("participant", "train_idx", "test_idx", "auc")

timegen_data <- timegen_data %>%
    mutate(
    participant = as.integer(participant),
    train_idx = as.integer(train_idx),
    test_idx = as.integer(test_idx),
    train_time = time_points[train_idx],
    test_time = time_points[test_idx]
    ) %>%
    select(train_time, test_time, auc, participant) %>%
    filter(participant < 11)

# plotting the cross-temporal generalisation matrix
timegen_data %>%
    summarise(auc = mean(auc), .by = c(train_time, test_time) ) %>%
    ggplot(aes(x = train_time, y = test_time, fill = auc) ) +
    geom_tile() +
    geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
    geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
    geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
    scale_x_continuous(expand = c(0, 0) ) +
    scale_y_continuous(expand = c(0, 0) ) + 
    coord_fixed() +
    scale_fill_scico(palette = "vik", midpoint = 0.5, name = "AUC") +
    labs(x = "Testing time (s)", y = "Training time (s)")
```

```{r fig-sim-timegen, eval = FALSE, echo = FALSE, dpi = 300, out.width = "75%", fig.cap = "Exemplary (simulated) group-level average cross-temporal generalisation matrix of decoding performance (ROC AUC)."}
# number of participants
n_participants <- 10

# number of timepoints
n_timepoints <- 60 # (1.2s at 50Hz)
timepoints <- seq(from = -0.2, to = 1, length.out = n_timepoints)

# smooth onset and offset parameters
decode_onset <- 0.2
decode_offset <- 0.3

# smoothness of onset/offset
sigma_smooth <- 0.05

# generating participant-specific cross-temporal generalisation matrices
timegen_data <- data.frame()

# for each participant
for (p in 1:n_participants) {
  
    # adding variability in peak amplitude and width per participant
    peak_variability <- runif(1, 30, 50) # random peak strength (higher = stronger decoding)
    smooth_variability <- runif(1, 0.04, 0.06) # random smoothness variation
    onset_variability <- runif(1, -0.05, 0.05) # small jitter in onset time
    offset_variability <- runif(1, -0.05, 0.05) # small jitter in offset time
    
    # computing smooth transition function (soft onset and offset with participant variation)
    decode_window <- outer(timepoints, timepoints, function (t1, t2) {
        
        # smooth onset
        onset_factor <- 1 /
            (1 + exp(-(t1 - (decode_onset + onset_variability) ) / sigma_smooth) )
        
        # smooth offset
        offset_factor <- 1 /
            (1 + exp((t1 - (decode_offset + offset_variability) ) / sigma_smooth) )
        
        # diagonal Gaussian
        onset_factor * offset_factor * exp(-((t1 - t2)^2) / (2 * smooth_variability^2) )
        
    })
    
    # defining smooth alpha and beta parameters for Beta distribution
    base_alpha <- 10
    peak_alpha <- peak_variability
    alpha_matrix <- base_alpha + peak_alpha * decode_window
    beta_matrix <- base_alpha
    
    # generating AUC values using the Beta distribution
    auc_matrix_beta <- matrix(
        rbeta(n_timepoints^2, shape1 = alpha_matrix, shape2 = beta_matrix),
        nrow = n_timepoints, ncol = n_timepoints
        )
    
    # converting to long format for plotting
    temp_data <- expand.grid(
        train_time = timepoints, test_time = timepoints
        ) %>%
        mutate(auc = as.vector(auc_matrix_beta), participant = p)
    
    # storing results
    timegen_data <- bind_rows(timegen_data, temp_data)
    
}

# plotting the cross-temporal generalisation matrix
timegen_data %>%
    summarise(auc = mean(auc), .by = c(train_time, test_time) ) %>%
    ggplot(aes(x = train_time, y = test_time, fill = auc) ) +
    geom_tile() +
    geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
    geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
    geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
    scale_x_continuous(expand = c(0, 0) ) +
    scale_y_continuous(expand = c(0, 0) ) + 
    coord_fixed() +
    scale_fill_scico(palette = "vik", midpoint = 0.5, name = "AUC") +
    labs(x = "Testing time (s)", y = "Training time (s)")
```

```{r mne-timegen, eval = FALSE, echo = FALSE}
# defining the function in R (it will be executed in Python)
mne_timegen <- function (X, labels, ncores = 8) {
    
    # converting R dataframe to NumPy array (reshaping if needed)
    # X should be a matrix before conversion
    X_np <- np$array(X)
    
    if (length(dim(X_np) ) == 2) {

        # adding a second dimension (channels) if missing
        X_np <- np$expand_dims(X_np, axis = as.integer(1) )

    }
  
    # defining the classifier
    clf <- sklearn$linear_model$LogisticRegression(solver = "liblinear")
    
    # sliding the estimator on all time frames
    time_gen <- mne$decoding$GeneralizingEstimator(
        clf,
        n_jobs = as.integer(ncores),
        scoring = "roc_auc",
        verbose = FALSE
        )
    
    # or using N-fold cross-validation
    scores <- mne$decoding$cross_val_multiscore(
        time_gen,
        X_np,
        labels,
        cv = as.integer(4),
        n_jobs = as.integer(ncores),
        verbose = FALSE
        )
    
    # returning the scores
    return (scores)
    
}

# listing all participants
participants <- unique(raw_df$participant)

# initialising empty decoding results
group_timegen_scores <- data.frame()

# running decoding for each participant
for (ppt in participants) {
    
    # printing progress
    print(ppt)
    
    # retrieve data from one participant
    ppt_data <- raw_df %>%
        filter(participant == ppt) %>%
        select(-participant) %>%
        pivot_wider(names_from = time, values_from = eeg) %>%
        select(-condition, -trial)
        
    # extracting the labels
    labels <- raw_df %>%
        filter(participant == ppt) %>%
        select(-participant) %>%
        pivot_wider(names_from = time, values_from = eeg) %>%
        pull(condition) %>%
        as.numeric()
    
    # extracting the timesteps
    timesteps <- raw_df %>%
        filter(participant == ppt) %>%
        select(-participant) %>%
        pull(time) %>%
        unique()
    
    # running the decoding
    timegen_scores <- mne_timegen(
        X = ppt_data,
        labels = labels-1
        )
    
    # computing the average over CV folds
    # a <- timegen_scores %>%
    #     apply(MARGIN = c(2, 3), FUN = mean) %>%
    #     data.frame()
    timegen_scores <- data.frame(np$mean(timegen_scores, axis = as.integer(0) ) )
    
    # sanity check
    # image(as.matrix(timegen_scores) )
    
    # appending to previous results
    group_timegen_scores <- bind_rows(group_timegen_scores, timegen_scores)
    
}

# saving the scores
saveRDS(object = group_timegen_scores, file = "results/timegen_scores.rds")

# reshaping the data
colnames(group_timegen_scores) <- unique(raw_df$time)
timegen_data <- group_timegen_scores %>%
    mutate(participant = rep(x = participants, each = n_distinct(raw_df$time) ) ) %>%
    mutate(train_time = rep(x = unique(raw_df$time), times = length(participants) ) ) %>%
    pivot_longer(
        cols = -c(participant, train_time),
        names_to = "test_time",
        values_to = "auc"
        ) %>%
    mutate(test_time = as.numeric(test_time) )
```

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

To model cross-temporal generalisation matrices of decoding performance (ROC AUC), we extended our initial BGAM to take into account the bivariate temporal distribution of AUC values, thus producing naturally smoothed estimates (timecourses) of AUC values and posterior probabilities. This model can be written as follows:

$$
\begin{aligned}
\text{AUC}_{i} &\sim \mathrm{Beta}(\mu_{i}, \phi)\\
g(\mu_{i}) &= f \left(\text{train}_{i}, \text{test}_{i} \right)\\
\end{aligned}
$$

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

where we assume that AUC values come from a $\mathrm{Beta}$ distribution with two parameters $\mu$ and $\phi$. We can think of $f \left(\text{train}_{i}, \text{test}_{i} \right)$ as a surface (a smooth function of two variables) that we can model using a 2-dimensional splines. Let $\mathbf{s}_{i} = \left(\text{train}_{i}, \text{test}_{i} \right)$ be some pair of training and testing samples, and let $\mathbf{k}_{m} = \left(\text{train}_{m}, \text{test}_{m} \right)$ denote the $m^{\text{th}}$ knot in the domain of $\text{train}_{i}$ and $\text{test}_{i}$. We can then express the smooth function as:

$$
f \left(\text{train}_{i}, \text{test}_{i} \right) = \alpha + \sum_{m=1}^M \beta_{m} b_{m} \left(\tilde{s}_{i}, \tilde{k}_{m} \right)
$$

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

A popular bivariate basis function uses *thin-plate splines* [@wood2003]. These splines are designed to interpolate and approximate smooth surfaces over two dimensions (hence the "bivariate" term). For $d=2$ dimensions and $l=2$ (smoothness penalty involving second order derivative):

$$
f \left(\tilde{s}_{i} \right) = \alpha + \beta_{1} x_{i} + \beta_{2} z_{i} +\sum_{m=1}^{M} \beta_{2+m} b_m\left(\tilde{s}_i, \tilde{k}_m\right)
$$

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

using the the radial basis function given by:

$$
b_m\left(\tilde{s}_i, \tilde{k}_m\right)=\left\|\tilde{s}_i-\tilde{k}_m\right\|^2 \log \left\|\tilde{s}_i-\tilde{k}_m\right\|
$$

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

where $\left\|\mathbf{s}_i-\mathbf{k}_{m}\right\|$ is the Euclidean distance between the covariate $\mathbf{s}_{i}$ and the knot location $\mathbf{k}_{m}$. We fitted this model using `brms` and the `t2()` tensor product smooth constructor with full penalties [@pedersen_hierarchical_2019]. We ran eight MCMCs to approximate the posterior distribution, including each 5000 iterations and a warmup of 2000 iterations, yielding a total of $8 \times (5000-2000) = 24000$ posterior samples to be used for inference.

```{r gam-timegen, eval = TRUE, echo = TRUE, results = "hide"}
# fitting a GAM with two temporal dimensions
timegen_gam <- brm(
    # 2D thin-plate spline (tp) with full penalties
    auc ~ t2(train_time, test_time, bs = "tp", k = 30, full = TRUE),
    data = timegen_data,
    family = brms::Beta(),
    warmup = 1000,
    iter = 5000,
    chains = 8,
    cores = 8,
    control = list(adapt_delta = 0.95, max_treedepth = 15),
    backend = "cmdstanr",
    stan_model_args = list(stanc_options = list("O1") ),
    file = "models/timegen_gam_meg_t2_k30_full.rds"
    )
```

```{r fig-gam-timegen-post-preds, eval = TRUE, echo = FALSE, cache = FALSE, fig.width = 9, fig.asp = 0.5, fig.cap = "Predicted AUC values with threshold (left) and posterior odds of decoding accuracy being above chance (right) according to the bivariate BGAM."}
# defining a function to plot posterior predictions
plot_post_timegen <- function (
        model, data = NULL, chance_level = 0.5,
        sesoi = 0, thresholds = c(10, 100)
        ) {
    
    # if no data is specified, use model$data
    if (is.null(data) ) data <- model$data
    
    # defining a sequence of x values to make predictions
    preds_conds <- crossing(
        train_time = unique(data$train_time),
        test_time = unique(data$test_time)
        )
    
    # retrieving posterior predictions
    post_preds <- conditional_effects(
        x = model,
        conditions = preds_conds,
        method = "posterior_epred",
        prob = 0.95,
        ndraws = 1e2,
        # ndraws = 1e3,
        resolution = n_distinct(data$train_time)
        )[[1]]
    
    # sanity checks
    # nrow(preds_conds)
    # unique(post_preds$train_time)
    # unique(post_preds$test_time)
    # post_preds %>%
    #     summarise(auc = mean(estimate__), .by = c(train_time, test_time) ) %>%
    #     # head()
    #     nrow()
    
    # combining AUC ranges from both plots
    all_aucs <- c(
        post_preds %>% summarise(auc = mean(estimate__), .by = c(train_time, test_time) ) %>% pull(auc),
        data %>% summarise(auc = mean(auc), .by = c(train_time, test_time) ) %>% pull(auc)
        )
    
    # determining common limits
    fill_limits <- range(all_aucs, na.rm = TRUE)
    
    # retrieving posterior samples
    posterior_samples <- posterior_epred(object = model, newdata = preds_conds)
    
    # computing the probability that y is above 0
    prob_y_above_0 <- preds_conds %>%
        mutate(post_prob = colMeans(posterior_samples) ) %>%
        mutate(m = colMeans(posterior_samples > (chance_level + sesoi) ) ) %>%
        mutate(prob_ratio = m / (1 - m) ) %>%
        mutate(prob_ratio = pmin(prob_ratio, ndraws(model) ) ) %>%
        mutate(prob_ratio = pmax(prob_ratio, 1 / ndraws(model) ) )
    
    # plotting cross-temporal generalisation matrix
    # data_plot <- data %>%
    #     summarise(auc = mean(auc), .by = c(train_time, test_time) ) %>%
    #     ggplot(aes(x = train_time, y = test_time, fill = auc) ) +
    #     geom_tile() +
    #     geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
    #     geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
    #     geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
    #     scale_x_continuous(expand = c(0, 0) ) +
    #     scale_y_continuous(expand = c(0, 0) ) + 
    #     coord_fixed() +
    #     scale_fill_scico(palette = "vik", midpoint = 0.5, name = "AUC", limits = fill_limits) +
    #     labs(x = "Testing time (s)", y = "Training time (s)")
    
    # plotting cross-temporal generalization matrix
    post_preds_plot <- post_preds %>%
        summarise(auc = mean(estimate__), .by = c(train_time, test_time) ) %>%
        ggplot(aes(x = train_time, y = test_time, fill = auc) ) +
        geom_tile() +
        geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
        geom_contour(
            data = prob_y_above_0,
            aes(x = train_time, y = test_time, z = prob_ratio),
            breaks = thresholds,
            color = "black",
            linewidth = 1,
            inherit.aes = FALSE
            ) +
        scale_x_continuous(expand = c(0, 0) ) +
        scale_y_continuous(expand = c(0, 0) ) + 
        coord_fixed() +
        scale_fill_scico(palette = "vik", midpoint = 0.5, name = "AUC", limits = fill_limits) +
        labs(x = "Testing time (s)", y = "Training time (s)")

    # plotting it
    prob_above_chance_plot <- prob_y_above_0 %>%
        ggplot(aes(x = train_time, y = test_time, fill = prob_ratio) ) +
        geom_tile() +
        geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
        scale_x_continuous(expand = c(0, 0) ) +
        scale_y_continuous(expand = c(0, 0) ) + 
        coord_fixed() +
        scale_fill_scico(
            palette = "vik",
            midpoint = 0,
            name = "Posterior odds",
            trans = "log10"
            ) +
        labs(x = "Testing time (s)", y = "Training time (s)")
    
    # combining the plots
    post_preds_plot + prob_above_chance_plot
  
}

# computing the SESOI as the baseline SD
# baseline_sd <- timegen_data %>%
#     filter(train_time < 0 & test_time < 0) %>%
#     summarise(sd(auc) ) %>%
#     pull()

# computing the SESOI as the baseline upper 99% quantile
baseline_upper_quantile <- timegen_data %>%
    filter(train_time < 0 & test_time < 0) %>%
    summarise(quantile(x = auc, probs = 0.99) ) %>%
    pull()

# computing predictions and posterior odds contours
post_preds_plot <- plot_post_timegen(
    model = timegen_gam,
    data = timegen_data,
    # chance_level = 0.5,
    # sesoi = baseline_sd,
    chance_level = baseline_upper_quantile,
    sesoi = 0,
    # thresholds = c(20, 50, 100)
    thresholds = 10
    )

# showing the plot
post_preds_plot
```

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

@fig-gam-timegen-post-preds shows the predictions from the model (left) superimposed with the identified cluster as defined by thresholding the posterior odds (right). Notably, this model could be extended to a multilevel bivariate GAM via `t2(train_time, test_time, participant, bs = c("tp", "tp", "re"), m = 2, full = TRUE)` and could be generalised to account for both spatial (`x1` and `x2`) and temporal (`time`) dimensions with formulas such as `te(x, y, time, d = c(2, 1) )`. The performance of such models should be assessed in future work.

<!--

\newpage

# Alternative to GAMs: Approximate Gaussian Process regression

A Gaussian process (GP) is a stochastic process that defines the distribution over a collection of random variables indexed by a continuous variable, that is $\{f(t): t \in \mathcal{T}\}$ for some index set $\mathcal{T}$ [@riutort-mayol_practical_2023; @rasmussen2005]. Whereas Bayesian linear regression outputs a distribution over the parameters of some predefined parametric model, the GP approach, in contrast, is a non-parametric approach, in that it finds a distribution over the possible functions that are consistent with the observed data. However, "nonparametric" does not mean there aren't parameters, it means that there are infinitely many parameters.

From [brms documentation](https://www.rdocumentation.org/packages/brms/versions/2.22.0/topics/gp): A GP is a stochastic process, which describes the relation between one or more predictors $x=\left(x_{1}, \ldots, x_{d}\right)$ and a response $f(x)$, where $d$ is the number of predictors. A GP is the generalization of the multivariate normal distribution to an infinite number of dimensions. Thus, it can be interpreted as a prior over functions. The values of $f()$ at any finite set of locations are jointly multivariate normal, with a covariance matrix defined by the covariance kernel $k_p\left(x_i, x_j\right)$, where $p$ is the vector of parameters of the GP:

$$
\left(f\left(x_{1}\right), \ldots f\left(x_{n}\right) \sim \operatorname{MVN}\left(0, \left(k_p\left(x_{i}, x_{j}\right)\right)_{i, j=1}^{n}\right)\right.
$$

The smoothness and general behaviour of the function $f$ depends only on the choice of covariance kernel, which ensures that values that are close together in the input space will be mapped to similar output values...

From this perspective, $f$ is a realisation of an infinite dimensional normal distribution:

$$
f \sim \mathrm{Normal}(0, C(\lambda))
$$

where $C$ is a covariance kernel with hyperparameters $\lambda$ that defines the covariance between two function values $f\left(t_1\right)$ and $f\left(t_2\right)$ for two time points $t_1$ and $t_2$ [@rasmussen2005]. Similar to the different choices of the basis function for splines, different choices of the covariance kernel lead to different GPs. In this article, we consider the squared-exponential (a.k.a. radial basis function) kernel, which computes the squared distance between points and converts it into a measure of similarity. It is defined as:

$$
C(\lambda) := C\left(t_1, t_2, \sigma, \gamma\right) := \sigma^2 \exp \left(-\frac{||t_1-t_2||^{2}}{2 \gamma^2}\right)
$$

with hyperparameters $\lambda = (\sigma, \gamma)$, expressing the overall scale of GP and the length-scale, respectively [@rasmussen2005]. The advantages of this kernel are that it is computationally efficient and (infinitely) smooth making it a reasonable choice for the purposes of the present article. Here again, $\lambda$ hyperparameters are estimated from the data, along with all other model parameters.

Taken from <https://michael-franke.github.io/Bayesian-Regression/practice-sheets/10c-Gaussian-processes.html>: For a given vector $\mathbf{x}$, we can use the kernel to construct finite multi-variate normal distribution associated with it like so:

$$
\mathbf{x} \mapsto_{G P} \operatorname{MVNormal}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}))
$$

where $m$ is a function that specifies the mean for the distribution associated with $\mathbf{x}$. This mapping is essentially the Gaussian process: a systematic association of vectors of arbitrary length with a suitable multi-variate normal distribution.

Low-rank approximate Gaussian processes are of main interest in machine learning and statistics due to the high computational demands of exact Gaussian process models [@riutort-mayol_practical_2023]...

```{r gp, echo = FALSE, eval = FALSE, message = FALSE}
# See also <https://jtimonen.github.io/lgpr-usage/articles/math.html>...
# Now we fit the GP regression model...
# computing eeg_diff and averaging across participants
# group_df <- raw_df %>%
#     group_by(participant, time) %>%
#     pivot_wider(names_from = condition, values_from = eeg, values_fn = mean) %>%
#     mutate(eeg_diff = cond2 - cond1) %>%
#     summarise(eeg_diff = mean(eeg_diff) ) %>%
#     ungroup()

# see https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html
# and https://betanalpha.github.io/assets/case_studies/gaussian_processes.html
gp_priors <- c(
    set_prior("normal(0, 1)", class = "Intercept"),
    set_prior("normal(0, 1)", class = "b"),
    set_prior("exponential(1)", class = "sigma"),
    set_prior("exponential(1)", class = "sdgp")
    )

gp_model <- brm(
    # k refers to the number of basis functions for
    # computing Hilbert-space approximate GPs
    # if k = NA (default), exact GPs are computed
    # eeg ~ gp(time, by = condition),
    # eeg ~ condition + gp(time, by = condition, k = 20, cov = "exp_quad"),
    eeg ~ condition + gp(time, by = condition, k = 20, cov = "matern32"),
    data = ppt_df,
    family = gaussian(),
    # prior = gp_priors,
    control = list(adapt_delta = 0.99, max_treedepth = 15),
    iter = 2000,
    chains = 4,
    cores = 4,
    file = "models/gp.rds"
    )
```

```{r meta-gp, echo = FALSE, eval = FALSE, message = FALSE}
# fitting the GP
meta_gp <- brm(
    # using by-participant SD of ERPs across trials
    eeg_mean | se(eeg_sd) ~
        condition + gp(time, k = 20, by = condition) +
        (1 | participant),
    data = summary_df,
    family = gaussian(),
    control = list(adapt_delta = 0.99),
    iter = 5000,
    chains = 4,
    cores = 4,
    file = "models/meta_gp.rds"
    )
```

-->

\newpage

# How to choose the GAM basis dimension? {#apx-basis}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

There is no universal recommendation for choosing the optimal value of `k`, as it depends on several factors, including the sampling rate, preprocessing steps (e.g., signal-to-noise ratio, low-pass filtering), and the underlying neural dynamics of the phenomenon under investigation. One strategy is to set `k` as high as computational constraints allow, as suggested by previous authors [e.g., @pedersen_hierarchical_2019]. Alternatively, one can fit a series of models with different `k` values and compare them using information criteria such as LOOIC or WAIC, alongside posterior predictive checks (PPCs), to select the model that best captures the structure of the data. We illustrate this approach below.

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

@fig-choose-k presents the posterior predictions and two forms of posterior predictive checks (PPCs) for each GAM fit using different numbers of basis functions ($k \in \{5, 10, 20, 40, 80\}$). With the exception of the $k=5$ model, all other fits yield satisfactory PPCs, indicating that the predicted data closely resemble the empirical observations. However, model comparison using the leave-one-out information crtierio (LOOIC), as summarised in @tbl-simulation-results, identifies the $k=40$ model as the best-performing one in terms of LOOIC, closely followed by the $k=80$ model. This suggests that the optimal number of basis functions likely lies between these two values. Future simulation studies could further investigate how such model selection criteria relate to the precision of onset and offset estimates.

```{r fig-choose-k1, results = "hide", echo = FALSE, fig.width = 10, fig.asp = 1.25, fig.cap = "Posterior predictions and posterior predictive checks for the GAM with varying k (in rows)."}
# defining a list of possible k values
# k_values <- c(5, 10, 20, 40, 80)

# initialising lists to store models and plots
# gam_models <- list()
# post_pred_plots <- list()
# ppc_dens_plots <- list()
# ppc_ecdf_plots <- list()
# ppc_stat2d_plots <- list()

# looping over k values
# for (k_value in k_values) {
# 
#     # printing progress
#     cat("Processing k =", k_value, "\n")
# 
#     # fitting the GAM
#     gam <- brm(
#         eeg_mean ~ condition + s(time, bs = "tp", k = k_value, by = condition),
#         data = ppt_df,
#         family = gaussian(),
#         warmup = 1000,
#         iter = 5000,
#         chains = 4,
#         cores = 4,
#         file = paste0("results/gam_k", as.character(k_value), ".rds")
#         )
#     
#     # computing model comparison indices
#     gam <- add_criterion(
#         x = gam,
#         # criterion = c("loo", "waic", "bayes_R2", "loo_R2", "marglik"),
#         criterion = c("loo", "waic", "bayes_R2"),
#         model_name = paste0("gam_k", as.character(k_value) )
#         )
#     
#     # storing the model
#     gam_models[[as.character(k_value)]] <- gam
# 
#     # generating posterior predictions
#     post_pred_plots[[as.character(k_value)]] <- plot_post_preds(model = gam) +
#         ggtitle(paste("Posterior predictions (k =", k_value, ")") )
# 
#     # generating posterior predictive checks
#     ppc_dens_plots[[as.character(k_value)]] <- pp_check(gam, ndraws = 100, type = "dens_overlay") +
#         labs(x = "EEG signal", y = "Density") +
#         ggtitle(paste("Density overlay (k =", k_value, ")") ) +
#         theme(legend.position = "none")
# 
#     ppc_ecdf_plots[[as.character(k_value)]] <- pp_check(gam, ndraws = 100, type = "ecdf_overlay") +
#         labs(x = "EEG signal", y = "Cumulative density") +
#         ggtitle(paste("ECDF overlay (k =", k_value, ")") ) +
#         theme(legend.position = "none")
# 
#     ppc_stat2d_plots[[as.character(k_value)]] <- pp_check(gam, ndraws = 500, type = "stat_2d", stat = c("mean", "sd") ) +
#         labs(x = "Mean", y = "SD") +
#         ggtitle(paste("Mean vs. SD (k =", k_value, ")") )
# 
# }

# saving the models
# saveRDS(object = gam_models, file = "results/gam_k_models.rds")

# saving the results
# saveRDS(object = post_pred_plots, file = "results/post_pred_plots.rds")
# saveRDS(object = ppc_dens_plots, file = "results/ppc_dens_plots.rds")
# saveRDS(object = ppc_ecdf_plots, file = "results/ppc_ecdf_plots.rds")
# saveRDS(object = ppc_stat2d_plots, file = "results/ppc_stat2d_plots.rds")

# importing the models
gam_models <- readRDS(file = "results/gam_k_models.rds")

# importing the results
post_pred_plots <- readRDS(file = "results/post_pred_plots.rds")
ppc_dens_plots <- readRDS(file = "results/ppc_dens_plots.rds")
ppc_ecdf_plots <- readRDS(file = "results/ppc_ecdf_plots.rds")
ppc_stat2d_plots <- readRDS(file = "results/ppc_stat2d_plots.rds")

# pp_check(gam, ndraws = 100, type = "ecdf_overlay") + 
#     labs(x = "EEG signal", y = "Cumulative density") + ggtitle(paste("ECDF overlay (k =", k_value, ")") ) + theme(legend.position = "none")
# pp_check(gam, ndraws = 200, type = "stat_2d", stat = c("median", "mad") ) + 
#     labs(x = "Mean", y = "SD") + ggtitle(paste("Mean vs. SD (k =", k_value, ")") )

# arranging plots using patchwork
final_plot <- (
    post_pred_plots[["5"]] + ppc_dens_plots[["5"]] + ppc_stat2d_plots[["5"]] +
    post_pred_plots[["10"]] + ppc_dens_plots[["10"]] + ppc_stat2d_plots[["10"]] +
    post_pred_plots[["20"]] + ppc_dens_plots[["20"]] + ppc_stat2d_plots[["20"]] +
    post_pred_plots[["40"]] + ppc_dens_plots[["40"]] + ppc_stat2d_plots[["40"]] +
    post_pred_plots[["80"]] + ppc_dens_plots[["80"]] + ppc_stat2d_plots[["80"]]
    ) + plot_layout(ncol = 3, nrow = 5)

# displaying the final plot
# print(final_plot)

# saving the plot
# ggsave(
#     filename = "figures/k_value_ppc.png",
#     width = 20, height = 15, dpi = 200,
#     device = "png"
#     )

# combining posterior predictions and some posterior predictive checks
# post_preds <- plot_post_preds(model = gam)
# ppc_dens <- pp_check(object = gam, ndraws = 100, type = "dens_overlay") + labs(x = "EEG signal", y = "Density")
# ppc_ecdf <- pp_check(object = gam, ndraws = 100, type = "ecdf_overlay") + labs(x = "EEG signal", y = "CDF")
# ppc_stat2d <- pp_check(object = gam, ndraws = 200, type = "stat_2d", stat = c("mean", "sd") ) + labs(x = "Mean", y = "SD")
# post_preds + ppc_dens + ppc_ecdf + ppc_stat2d

# some posterior predictive checks
# pp_check(gam, prefix = "ppd")
# pp_check(gam, type = "error_hist")
# pp_check(object = gam, ndraws = 10, type = "intervals", x = "time")
# pp_check(object = gam, ndraws = 10, type = "scatter_avg")
# pp_check(object = gam, ndraws = 20, type = "hist")
# pp_check(object = gam, ndraws = 50, type = "stat_2d")
# pp_check(object = gam, ndraws = 50, type = "ecdf_overlay")
# pp_check(object = gam, ndraws = 200, type = "stat", stat = "mean")
# pp_check(object = gam, ndraws = 200, type = "stat", stat = "median")
# pp_check(object = gam, ndraws = 200, type = "stat_2d", stat = c("mean", "sd") )
# pp_check(object = gam, ndraws = 200, type = "stat_2d", stat = c("median", "mad") )
```

```{r tbl-k-results, eval = TRUE, echo = FALSE, results = "asis", fig.pos = "h", out.extra = ''}
#| tbl-cap: "Models comparison with LOOIC. Models are arranged by the diffence in expected log-pointwise density (ELPD) to the best model (i.e., k=40)."
#| # comparing the models
models_comp <- brms::loo_compare(
    gam_models[["5"]],
    gam_models[["10"]],
    gam_models[["20"]],
    gam_models[["40"]],
    gam_models[["80"]],
    criterion = "loo"
    )

data.frame(models_comp) %>%
    rownames_to_column(var = "k") %>%
    mutate(k = str_extract(string = k, pattern = '"[^"]*"') ) %>%
    mutate(k = str_remove_all(string = k, pattern = '^"|"$') ) %>%
    # mutate(across(.cols = where(is.numeric), .fns = ~round(.x, 2) ) ) %>%
    gt() %>%
    fmt_number(
        columns = everything(),
        decimals = 2
        ) %>%
    cols_align(
        align = "center",
        columns = everything()
        ) %>%
    cols_align(
        align = "left",
        columns = k
        ) %>%
    # cols_label(
    #     # elpd_diff = "\\$Delta$ diff"
    #     ) %>%
    tab_options(
        table.align = "center",
        table.font.size = 10
        ) %>%
    as_latex()
```

```{r fig-choose-k, results = "hide", echo = FALSE, fig.width = 10, fig.asp = 1.25, fig.cap = "Posterior predictions and posterior predictive checks for the GAM with varying k (in rows)."}
# displaying the final plot
print(final_plot)
```

\newpage

# `R` package and integration with `MNE-Python` {#apx-package}

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

For readers who are already familiar with `brms`, the recommended pipeline is to use the code provided in the main paper (available at <https://github.com/lnalborczyk/brms_meeg>). It is also possible to call functions from the `neurogam R` package (available at <https://github.com/lnalborczyk/neurogam>) which come with sensible defaults.

```{r, eval = FALSE, echo = TRUE}
# installing (if needed) and loading the neurogam R package
# remotes::install_github("https://github.com/lnalborczyk/neurogam")
library(neurogam)

# using the testing_through_time() function from the neurogam package
# this may take a few minutes (or hours depending on the machine's
# performance and the size of the dataset)...
gam_onset_offset <- testing_through_time(
    # dataframe with M/EEG data in long format
    data = raw_df,
    # the *_id arguments are used to specify the relevant columns in data 
    participant_id = "participant", meeg_id = "eeg",
    time_id = "time", predictor_id = "condition",
    # posterior odds threshold for defining clusters (20 by default)
    threshold = 20,
    # number of warmup MCMC iterations
    warmup = 1000,
    # total number of MCMC iterations
    iter = 5000,
    # number of MCMCs
    chains = 4,
    # number of parallel cores to use for running the MCMCs
    cores = 4
    )

# displaying the results
gam_onset_offset$clusters
```

\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}

The `neurogam` package can also be called from `Python` using the `rpy2` module, and can easily be integrated into `MNE-Python` pipelines. For example, we use it below to estimate the onset and offset of effects for one EEG channel from an MNE evoked object. The code used to reshape the `sample MNE` dataset is available in the online supplementary materials, and we further refer to the [MNE documentation](https://mne.tools/stable/auto_tutorials/epochs/50_epochs_to_data_frame.html) about converting `MNE` epochs to `Pandas` dataframes in long format (i.e., with one observation per row).

```{python, eval = FALSE, echo = TRUE}
# loading the Python modules
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr
from rpy2.robjects import pandas2ri
from rpy2.robjects.conversion import localconverter

# importing the "neurogam" R package
neurogam = importr("neurogam")

# activating automatic pandas-R conversion
pandas2ri.activate()

# assuming reshaped_df is some M/EEG data reshaped in long format
with localconverter(robjects.default_converter + pandas2ri.converter):
    
    reshaped_df_r = robjects.conversion.py2rpy(reshaped_df)
    

# using the testing_through_time() function from the neurogam R package
gam_onset_offset = neurogam.testing_through_time(
    data=reshaped_df_r,
    threshold=20,
    multilevel=False
    )

# displaying the results
print(list(gam_onset_offset) )
```

```{python, eval = FALSE, echo = FALSE}
# loading the Python modules
import mne
from mne.datasets import sample
import numpy as np
import pandas as pd

# defining the path to the "sample" dataset
path = sample.data_path()

# loading the evoked data
evokeds = mne.read_evokeds(path / "MEG" / "sample" / "sample_audvis-ave.fif")

# defining a function to reshape the data
def reshape_eeg_channels_as_participants(evokeds, condition_labels):
    
    all_dfs = []

    for evoked, condition in zip(evokeds, condition_labels):
        
        # selecting only EEG channels
        picks = mne.pick_types(evoked.info, meg=False, eeg=True)
        data = evoked.data[picks, :]
        channel_names = np.array(evoked.ch_names)[picks]

        n_channels, n_times = data.shape
        # repeating time for each channel
        times = np.tile(evoked.times, n_channels)
        meeg_values = data.flatten()
        pseudo_participant_ids = np.repeat(channel_names, n_times)
        
        # converting to dataframe
        df = pd.DataFrame({
            "participant": pseudo_participant_ids,
            "time": times,
            "eeg": meeg_values,
            "condition": condition
            })

        all_dfs.append(df)

    return pd.concat(all_dfs, ignore_index=True)

# picking two evoked conditions
faces = evokeds[0]
scrambled = evokeds[1]

# reshaping data by pretending each EEG channel is a participant
reshaped_df = reshape_eeg_channels_as_participants(
    evokeds=[faces, scrambled],
    condition_labels=["faces", "scrambled"]
    )
```
