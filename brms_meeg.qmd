---
title: "Precise temporal localisation of M/EEG effects with Bayesian generalised additive multilevel models"
shorttitle: "Bayesian modelling of M/EEG data"
author:
  - name: Ladislas Nalborczyk
    corresponding: true
    orcid: 0000-0002-7419-9855
    email: ladislas.nalborczyk@cnrs.fr
    url: https://lnalborczyk.github.io
    affiliations:
      - id: id1
        name: "Aix Marseille Univ, CNRS, LPL"
        address: "5 avenue Pasteur"
        city: "13100 Aix-en-Provence"
        country: France
  - name: Paul Bürkner
    orcid: 0000-0001-5765-8995
    url: https://paulbuerkner.com
    affiliations:
      - name: "TU Dortmund University, Department of Statistics"
author-note:
  disclosures:
    conflict-of-interest: "The authors have no conflicts of interest to disclose."
abstract: "Time-resolved electrophysiological measurements such as those obtained through magneto- or electro-encephalography (M/EEG) offer a unique window into the neural activity underlying cognitive processes. Researchers are often interested in determining whether and when these signals differ across experimental conditions or participant groups. The conventional approach involves mass-univariate statistical testing across time, followed by corrections for multiple comparisons (e.g., FDR, FWER) or cluster-based inference. While effective for controlling error rates, cluster-based methods come with a significant limitation: they shift the focus of inference from individual time points to clusters, making it difficult to draw precise conclusions about the onset or offset of observed effects. Here, we introduce a *model-based* approch for analysing M/EEG timeseries such as event-related potentials (ERPs) or decoding performance over time. Our method leverages Bayesian generalised additive multilevel models (GAMMs), providing posterior probabilities that an effect is above zero (or above chance) at each time point, while naturally accounting for temporal dependencies and between-subject variability. Using both simulated and actual M/EEG datasets, we demonstrate that this approach substantially outperforms conventional methods in estimating the onset and offset of neural effects, yielding more precise and reliable results. We also provide an R package implementing the method and show how it can be seamlessly integrated into M/EEG analysis pipelines using MNE-Python."
keywords: [EEG, MEG, generalised additive models, mixed-effects models, multilevel models, Bayesian statistics, brms]
floatsintext: true
numbered-lines: true
bibliography:
  - meeg_modelling.bib
  - grateful-refs.bib
suppress-title-page: false
link-citations: true
# see https://www.overleaf.com/learn/latex/Font_typefaces
# and systemfonts::system_fonts()
# pdf-engine: xelatex
# monofont: Roboto
mainfont: CMU Serif
fontsize: 10pt
draftfirst: false
draftall: false
a4paper: true
donotrepeattitle: true
lang: en
toc: true
language:
  citation-last-author-separator: "&"
  email: "email"
  title-block-author-note: "Author note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
format:
  # apaquarto-html: default
  apaquarto-pdf:
    latex-engine: xelatex
    documentmode: doc # man, jou, doc, stu
    keep-tex: true
    include-in-header:
      - text: |
          \usepackage{mathtools}
---

```{r setup, include = FALSE}
# loading packages
library(changepoint)
library(tidyverse)
library(tidybayes)
library(patchwork)
library(MetBrewer)
library(tidytext)
library(grateful)
library(ggrepel)
library(scales)
library(pakret)
library(scico)
library(knitr)
library(brms)

# setting knitr options
knitr::opts_chunk$set(
    cache = TRUE,
    eval = TRUE,
    echo = TRUE,
    warning = FALSE,
    message = FALSE,
    fig.align = "center",
    out.width = "100%",
    fig.asp = 0.75,
    fig.pos = "!htb",
    # dev = "cairo_pdf",
    dev = "png",
    dpi = 300
    )

# setting a default ggplot2 theme
# theme_set(theme_light(base_size = 10, base_family = "Open Sans") )
theme_set(theme_bw(base_size = 10, base_family = "Open Sans") )

# template for citing R packages
# pkrt_set(pkg = "the `R` package `:pkg` [:ref]")
pkrt_set(pkg = "the `R` package `:pkg` v :ver [:ref]")
```

# Introduction

Understanding the temporal dynamics of cognitive processes requires methods that can capture fast-changing neural activity with high temporal resolution. Magnetoencephalography (MEG) and electroencephalography (EEG) are two such methods, widely used in cognitive neuroscience for their ability to track brain activity at the millisecond scale. These techniques provide rich time series data that reflect how neural responses unfold in response to stimuli or tasks. A central goal in many M/EEG studies is to determine whether and when neural responses differ across experimental conditions or participant groups.

Here are some useful references to be discussed [@combrisson_exceeding_2015; @ehinger_unfold_2019; @hayasaka_validating_2003; @luck_how_2017; @frossard2021; @frossard2022; @gramfort2013; @maris2007; @pernet2015]... See also @maris2011 and @rosenblatt2018 (history of cluster-based approaches)... Clusters failures [@eklund2016]...

## Problem statement

Description of cluster-based approaches [see @sassenhagen2019]...

However, as pointed by the original authors themselves [@maris2007], "there is a conflict between this interest in localized effects and our choice for a global null hypothesis: by controlling the FA \[false alarm\] rate under this global null hypothesis, one cannot quantify the uncertainty in the spatiotemporal localization of the effect" [@maris2007; @sassenhagen2019]. In contrast, the proposed approach, based on Bayesian generalised additive multilevel models, allows quantifying the posterior probability of effects being above chance at the level of timesteps, voxels, sensors (etc), while naturally taking into account spatiotemporal dependencies present in M/EEG timeseries.

## Related work

Recent example of GLM for EEG [@wüllhorst2025; @fischer2013]... See also [@rousselet2008; @hauk2006]... Example of two-stage regression analysis [i.e., individual-level then group-level, @dunagan2024]...

See also the rERP framework [@smith2014a; @smith2014b] and @tremblay2014...

From @dimigen2021: Recently, spline regression has been applied to ERPs [e.g., @hendrix2017; @kryuchkova2012]... GAMMs for EEG data [@abugaber2023; @meulman2015; @meulman2023]...

Disentangling overlapping processes [@skukies_brain_2024; @skukies_modelling_2021]... Weighting single trials [@pernet2022]... The LIMO toolbox [@pernet2011]...

Recently, @teichmann2022 provided a detailed tutorial on using Bayes factors (BFs) to analyse the 1D or 2D output from MVPA, that is, for testing, at every timestep, whether decoding performance is above chance level. However, this approach provides timeseries of BFs that ignores temporal dependencies..

## Bayesian regression modelling

Short intro/recap about Bayesian (linear and generalised) regression models...

## Generalised additive models

See for instance these tutorials [@sóskuthy2017; @winter2016] or application to phonetic data [@wieling2018; @sóskuthy2021] or this introduction [@baayen2020] or these reference books [@hastie2017; @wood2017]... application to pupillometry [@vanrij2019]... GAMLSS for neuroimaging data [@dinga2021]... Modelling auto-correlation in GAMMs + EEG example [@baayen2018]...

In generalised additive models (GAMs), the functional relationship between predictors and response variable is decomposed into a sum of low-dimensional non-parametric functions. A typical GAM has the following form:

$$
\begin{aligned} 
y_{i} &\sim \mathrm{EF}\left(\mu_{i}, \phi\right)\\
g\left(\mu_i\right) &= \underbrace{\mathbf{A}_{i} \gamma}_{\text{parametric part}} + \underbrace{\sum_{j=1}^{J} f_{j}\left(x_{ij}\right)}_{\text{non-parametric part}}
\end{aligned}
$$

where $y_{i} \sim \mathrm{EF}\left(\mu_{i}, \phi\right)$ denotes that the observations $y_{i}$ are distributed as some member of the exponential family of distributions (e.g., Gaussian, Gamma, Beta, Poisson) with mean $\mu_{i}$ and scale parameter $\phi$; $g(\cdot)$ is the link function, $\mathbf{A}_{i}$ is the $i$th row of a known parametric model matrix, $\gamma$ is a vector of parameters for the parametric terms (to be estimated), $f_{j}$ is a smooth function of covariate $x_{j}$ (to be estimated as well). The smooth functions $f_{j}$ are represented in the model via penalised splines basis expansions of the covariates, that are a weighted sum of $K$ simpler, basis functions:

$$
f_{j}\left(x_{i j}\right) = \sum_{k=1}^K \beta_{jk} b_{jk}\left(x_{ij}\right)
$$

where $\beta_{jk}$ is the weight (coefficient) associated with the $k$th basis function $b_{jk}()$ evaluated at the covariate value $x_{ij}$ for the $j$th smooth function $f_{j}$. To clarify the terminology at this stage: *splines* are functions composed of simpler functions. These simpler functions are basis functions (e.g., a cubic polynomial) and the set of basis functions is a *basis*. Each basis function gets its coefficient and the resultant spline is the sum of these weighted basis functions. Splines coefficients are penalised (usually through the squared of the smooth functions' second derivative) in a way that can be interpreted, in Bayesian terms, as a prior on the "wiggliness" of the function. In other words, more complex (wiggly) basis function are penalised.

## Bayesian generalised additive multilevel models

Introduction to multilevel GAMs [@pedersen_hierarchical_2019]...

Now describe the Bayesian GAMM [@miller2025]... Proper inclusion of varying/random effects in the model specification protects against overly wiggly curves [@baayen2020]... Generalising to scale and shape or "distributional GAMs" [@rigby2005; @umlauf2018] and applied to neuroimaging data [@dinga2021]...

Instead of averaging, obtain the smooth ERP signal from multilevel GAM... less susceptible to outliers [@meulman2023]...

## Objectives

Given the previously reported limitations of conventional methods to precisely identify the onset and offset of M/EEG effects (e.g., ERPs, decoding performance), we aimed to provide a model-based approach for estimating the onset and offset of such effects. To achieve this, we leveraged Bayesian generalised additive multilevel models (BGAMMs) fitted via the `brms R` package and compared the performance of this approach to conventional methods on both simulated and actual M/EEG data.

# Methods

## M/EEG data simulation

Following the approach of @sassenhagen2019 and @rousselet_using_2025, we simulated EEG data stemming from two conditions, one with noise only, and the other with noise + signal. As in previous studies, the noise was generated by superimposing 50 sinusoids at different frequencies, following an EEG-like spectrum [see code in the online supplementary materials and details in @yeung2004]. As in @rousselet_using_2025, the signal was generated from truncated Gaussian with an objective onset at 160 ms, a peak at 250 ms, and an offset at 342 ms. We simulated this signal for 250 timesteps between 0 and 0.5s, akin to a 500 Hz sampling rate. We simulated data for a group of 20 participants with 50 trials per participant and condition (@fig-eeg).

```{r fig-eeg, echo = FALSE, fig.width = 9, fig.cap = "Mean simulated EEG activity in two conditions with 50 trials each, for a group of 20 participants. The error band represents the mean +/- 1 standard error of the mean."}
# importing R version of Matlab code from Yeung et al. (2004)
source("code/eeg_noise.R")

# importing the ERP template with true onset = 160 ms, F=81, and max at F=126
source("code/erp_template.R")

# importing helper functions
# source("code/functions.R")

# EEG power information to use with the eeg_noise() function
meanpower <- unlist(read.table("code/meanpower.txt") )

# defining simulation parameters
n_trials <- 50 # number of trials
n_ppt <- 20 # number of participants
outvar <- 1 # noise variance
srate <- 500 # sampling rate in Hz
ronset <- seq(from = 150, to = 170, by = 2) # random onset for each participant

# removing raw_df if it already exists
if (exists("raw_df") ) rm(raw_df)

for (P in 1:n_ppt) { # for each participant
    
    # get random onset
    ponset <- sample(ronset, 1)
     
    # find starting point
    st <- which(Xf == ponset)
    
    # pad vector
    temp2 <- c(rep(0, st - 2), erp, rep(0, Nf - st - length(erp) + 2) )
    
    # initialising empty conditions
    cond1 <- matrix(0, nrow = n_trials, ncol = Nf)
    cond2 <- matrix(0, nrow = n_trials, ncol = Nf)
    
    for (T in 1:n_trials) { # for each trial
        
        cond1[T, ] <- temp1 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
        cond2[T, ] <- temp2 + eeg_noise(frames = Nf, srate = srate, outvar = outvar, meanpower)
      
    }
    
    # converting results to dataframe
    temp_df <- data.frame(
        x = rep(Xf, 2*nrow(cond1) ),
        y = c(c(t(cond1) ), c(t(cond2) ) ),
        trial = c(rep(1:n_trials, each = length(Xf) ), rep(1:n_trials, each = length(Xf) ) ),
        condition = factor(rep(c("cond1", "cond2"), each = Nf * n_trials) ),
        participant = paste0("participant_", sprintf("%02d", P) )
        ) %>%
        select(participant, condition, trial, x, y)
    
    # and appending it to previous participants
    if (exists("raw_df") ) {
        
        raw_df <- bind_rows(raw_df, temp_df)
        
    } else {
        
        raw_df <- temp_df
        
    }
    
}

# converting time from ms to seconds
raw_df <- raw_df %>%
    # converting time from ms to seconds
    mutate(x = x / 1000) %>%
    # renaming columns
    rename(time = x, eeg = y)

# defining the true onset and offset in seconds
true_onset <- 0.160
true_peak <- 0.250
true_offset <- 0.342

# plotting the data
raw_df %>%
    summarise(
        eeg_mean = mean(eeg),
        eeg_se = sd(eeg) / sqrt(n() ),
        .by = c(participant, condition, time),
        ) %>%
    ggplot(aes(x = time, y = eeg_mean, colour = condition, fill = condition) ) +
    geom_hline(yintercept = 0, linetype = 2) +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_ribbon(
        aes(ymin = eeg_mean - eeg_se, ymax = eeg_mean + eeg_se, colour = NULL),
        alpha = 0.3, show.legend = FALSE
        ) +
    geom_line(show.legend = FALSE) +
    facet_wrap(~participant) +
    scale_colour_met_d(name = "Johnson") +
    labs(x = "Time (s)", y = "EEG signal (a.u.)")
```

````{=html}
<!--

We computed the average of the ERP difference (@fig-erp)...

```{r fig-erp, eval = FALSE, echo = FALSE, out.width = "75%", fig.cap = "Group-level average difference between conditions (mean +/- standard error of the mean). The 'true' onset and offset are indicated by the vertical dashed lines."}
# averaging across participants
raw_df %>%
    pivot_wider(names_from = condition, values_from = eeg, values_fn = mean) %>%
    mutate(eeg_diff = cond2 - cond1) %>%
    summarise(
        eeg_mean = mean(eeg_diff),
        eeg_sem = sd(eeg_diff) / sqrt(n() ),
        .by = time
        ) %>%
    mutate(lower = eeg_mean - eeg_sem, upper = eeg_mean + eeg_sem) %>%
    # plotting the data
    ggplot(aes(x = time, y = eeg_mean) ) +
    geom_hline(yintercept = 0, linetype = 2) +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_ribbon(
        aes(ymin = lower, ymax = upper, colour = NULL),
        alpha = 0.3, show.legend = FALSE
        ) +
    geom_line(show.legend = FALSE) +
    labs(x = "Time (s)", y = "EEG difference (a.u.)")
```

-->
````

## Model fitting

We then fitted a Bayesian GAM using the `brms` package [@brms2017; @brms2018; @nalborczyk2019]. We used the default priors in `brms` (i.e., weakly informative priors). We ran eight Markov Chain Monte-Carlo (MCMC) to approximate the posterior distribution, including each 5000 iterations and a warmup of 2000 iterations, yielding a total of $8 \times (5000-2000) = 24000$ posterior samples to use for inference. Posterior convergence was assessed examining trace plots as well as the Gelman–Rubin statistic $\hat{R}$ [@gabry2019; @gelman2020]. The `brms` package uses the same syntax as `r pkrt("mgcv")` for specifying smooth effects. @fig-plot-post-slope shows the predictions of this model together with the raw data.

```{r gam, results = "hide"}
# averaging across participants
ppt_df <- raw_df %>%
    group_by(participant, condition, time) %>%
    summarise(eeg = mean(eeg) ) %>%
    ungroup()

# defining a contrast for condition
contrasts(ppt_df$condition) <- c(-0.5, 0.5)

# timing the model fitting
# around 600s for 2000 iterations on 4 chains and 4 cores
# around 190s for 2000 iterations on 4 chains and 4 cores + cmdstanr and 01
# around 440s for 2000 iterations on 4 chains and 4 cores + threading(2), cmdstanr and 01
# library(tictoc)
# tic()

# fitting the GAM
gam <- brm(
    # cubic regression splines with k-1 basis functions
    # eeg ~ condition + s(time, bs = "cr", k = 20, by = condition),
    # thin-plate regression splines with k-1 basis functions
    eeg ~ condition + s(time, bs = "tp", k = 20, by = condition),
    data = ppt_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 8,
    cores = 8,
    file = "models/gam.rds"
    #################################
    # speeding up the model fitting #
    #################################
    # backend = "cmdstanr",
    # threads = threading(2),
    # stan_model_args = list(stanc_options = list("O1") )
    )

# stop timing
# toc()
```

```{r gamm, echo = FALSE, eval = FALSE, results = "hide"}
# defining a contrast for condition
contrasts(raw_df$condition) <- c(-0.5, 0.5)

# fitting the GAMM with factor smooth (per participant)
gamm <- brm(
    # cubic regression splines with k-1 basis functions
    eeg ~ condition +
        s(time, bs = "cr", k = 20, by = condition) +
        s(time, participant, bs = "fs"),
    data = raw_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 4,
    cores = 4,
    file = "models/gamm.rds"
    )
```

```{r model-preds, echo = FALSE}
# defining a function to plot posterior predictions
plot_post_preds <- function (model, data = NULL) {
    
    if (is.null(data) ) data <- model$data
    
    post_preds <- conditional_effects(
        x = model,
        effect = "time:condition",
        method = "posterior_epred",
        re_formula = NULL,
        prob = 0.99
        )[[1]]
  
    post_preds %>%
        ggplot(aes(x = time, y = eeg, colour = condition, fill = condition) ) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_hline(yintercept = 0, linetype = 2) +
        geom_point(
            data = data %>% summarise(eeg = mean(eeg), .by = c(time, condition) ),
            aes(fill = NULL),
            pch = 21, show.legend = FALSE
            ) +
        geom_line(aes(y = estimate__), show.legend = FALSE) +
        geom_ribbon(
            aes(ymin = lower__, ymax = upper__, colour = NULL),
            alpha = 0.25, show.legend = FALSE
            ) +
        scale_colour_met_d(name = "Johnson") +
        scale_fill_met_d(name = "Johnson") +
        labs(x = "Time (s)", y = "EEG signal (a.u.)")
    
}
```

```{r gam-preds, echo = FALSE, eval = FALSE, fig.width = 8, fig.asp = 0.5, fig.cap = "Posterior predictions of the GAM (left) and GP (right) models."}
# plotting the posterior predictions
plot_post_preds(model = gam) + plot_post_preds(model = gp_model)
```

We depict the posterior predictions together with the posterior estimate of the slope for `condition` at each timestep (@fig-plot-post-slope).

```{r fig-plot-post-slope, echo = FALSE, fig.width = 8, fig.asp = 0.5, fig.cap = "Posterior estimate of the EEG activity in each condition (left) and posterior estimate of the difference in EEG activity (right) according to the GAM."}
# defining a function to plot posterior slope
plot_post_slope <- function (model, data = NULL) {
  
    # if no data is specified, use model$data
    if (is.null(data) ) data <- model$data
    
    # defining a sequence of time values to make predictions
    time_seq <- crossing(
        time = seq(min(data$time), max(data$time), length.out = 100),
        condition = c("cond1", "cond2")
        ) %>%
        arrange(condition)
    
    # retrieving posterior samples
    posterior_samples <- posterior_epred(object = model, newdata = time_seq)
    
    # computing the difference between cond2 and cond1
    posterior_samples <- posterior_samples[, 101:200] - posterior_samples[, 1:100] 
    
    # computing the probability that y is above 0
    prob_y_above_0 <- data.frame(
        time = seq(min(data$time), max(data$time), length.out = 100)
        ) %>%
        mutate(
          m = colMeans(posterior_samples),
          lower = apply(
              X = posterior_samples,
              MARGIN = 2, FUN = quantile, probs = 0.025
              ),
          upper = apply(
              X = posterior_samples,
              MARGIN = 2, FUN = quantile, probs = 0.975
              )
          )
    
    # plotting it
    prob_y_above_0 %>%
        ggplot(aes(x = time, y = m) ) +
        geom_hline(yintercept = 0, linetype = 2) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line() +
        geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
        scale_colour_met_d(name = "Johnson") +
        scale_fill_met_d(name = "Johnson") +
        labs(
            x = "Time (s)",
            y = expression(p(beta~"|"~data, model) )
            )
  
}

# plotting it
# plot_post_slope(model = gam, eps = 0.1) + plot_post_slope(model = gp_model, eps = 0.1)
plot_post_preds(model = gam) + plot_post_slope(model = gam)
```

We then compute the posterior probability of the slope for `condition` being above $0$ (@fig-post-prob-ratio, left). We can also express this as the ratio of posterior probabilities (i.e., $p/(1-p)$) and visualise the timecourse of this ratio superimposed with the conventional thresholds on evidence ratios (@fig-post-prob-ratio, right). A ratio of 10 means that the probability of the difference being above 0 is 10 times higher than the probability of the difference not being above 0, given the data, the priors, and other model's assumptions.

```{r fig-post-prob-test, echo = FALSE, out.width = "75%", fig.asp = 0.75, fig.cap = "Posterior probability of the EEG difference (slope) being above 0 according to the GAM."}
# defining a function to plot posterior prob of slope above 0
plot_post_test <- function (model, data = NULL, eps = 0.1, n_approx = 1e3, n_draws = 1e3) {
    
    # if no data is specified, use model$data
    if (is.null(data) ) data <- model$data
    
    # defining a sequence of time values to make predictions
    time_seq <- crossing(
        time = seq(min(data$time), max(data$time), length.out = n_approx),
        # time = unique(data$time),
        condition = c("cond1", "cond2")
        ) %>%
        arrange(condition)
    
    # retrieving posterior samples
    # posterior_samples <- posterior_epred(object = model, newdata = time_seq)
    
    # computing the difference between cond2 and cond1
    # posterior_samples <- posterior_samples[, (nrow(time_seq)/2+1):nrow(time_seq)] -
    #     posterior_samples[, 1:(nrow(time_seq)/2)]
    
    # computing the probability that y is above 0
    # prob_y_above_0 <- data.frame(
    #     time = time_seq$time,
    #     m = colMeans(posterior_samples > (0 + eps) )
    #     )
    
    # computing probability metrics with the current eps value
    prob_y_above_0 <- time_seq %>%
        add_epred_draws(object = model, ndraws = n_draws) %>%
        data.frame() %>%
        dplyr::select(time, condition, .epred, .draw) %>%
        pivot_wider(names_from = condition, values_from = .epred) %>%
        mutate(epred_diff = cond2 - cond1) %>%
        group_by(time) %>%
        summarise(m = mean(epred_diff > 0 + eps) ) %>%
        ungroup()
        
    # plotting it
    prob_y_above_0 %>%
        ggplot(aes(x = time, y = m) ) +
        geom_hline(yintercept = 0.5, linetype = 2) +
        geom_vline(xintercept = true_onset, linetype = 2) +
        geom_vline(xintercept = true_offset, linetype = 2) +
        geom_line() +
        ylim(c(0, 1) ) +
        labs(x = "Time (s)", y = expression(Pr(beta>0~"|"~data, model) ) )
  
}

# using the function with the two models
# plot_post_test(model = gam, eps = 0.1)
```

```{r tests, echo = FALSE, eval = FALSE}
# compute percentage in ROPE
library(easystats)
# rope(
#     x = gam,
#     range = "default",
#     ci = 0.95,
#     ci_method = "ETI",
#     effects = "fixed",
#     # component = c("conditional", "zi", "zero_inflated", "all"),
#     # parameters = NULL,
#     verbose = TRUE
#     )
# rope_range(x = gam)

# retrieving posterior draws for EEG over time
gam <- full_model
post_samples <- posterior_epred(object = gam, newdata = ppt_df, allow_new_levels = TRUE)

# defining the ROPE as baseline SD
# rope_range <- c(-0.1, 0.1)
baseline_sd <- ppt_df %>%
    pivot_wider(names_from = condition, values_from = eeg) %>%
    mutate(cond_diff = abs(cond1 - cond2) ) %>%
    filter(time < 0.1) %>%
    # head()
    summarise(baseline_sd = sd(cond_diff) ) %>%
    data.frame()

rope_range <- c(-baseline_sd, +baseline_sd)

# computing the HDI for each time step
hdi_values <- apply(post_samples, 2, function (x) hdi(x, ci = 0.95) )

# computing the  proportion of the HDI that falls within the ROPE
prop_in_rope <- sapply(1:ncol(post_samples), function (i) {
    lower_hdi <- hdi_values[[i]][1]
    upper_hdi <- hdi_values[[i]][2]
    # computing the  proportion inside ROPE
    mean(lower_hdi >= rope_range[1] & upper_hdi <= rope_range[2])
    })

# storing the results in a dataframe
rope_results <- data.frame(time = ppt_df$time, prop_in_rope = prop_in_rope)

# plotting the results
rope_results %>%
    summarise(prop_in_rope = mean(prop_in_rope), .by = time) %>%
    ggplot(aes(x = time, y = prop_in_rope) ) +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_point()

# loading or fitting the full model
# full_model <- gam
full_model <- readRDS("models/gam.rds")

# fitting the null model (no 'condition' effect)
null_model <- brm(
    eeg ~ s(time, bs = "tp", k = 20),
    data = ppt_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 8,
    cores = 8,
    file = "models/null_gam.rds"
    )

# computing the marginal likelihoods using bridge sampling
# library(bridgesampling)
# full_bridge <- bridge_sampler(full_model)
# null_bridge <- bridge_sampler(null_model)

# computing the Bayes Factor in favor of full model
# BF_10 <- bf(full_bridge, null_bridge)
# print(BF_10)

# extracting log-likelihoods for each model
log_lik_full <- log_lik(full_model)
log_lik_null <- log_lik(null_model)

# computing the pointwise log pointwise predictive density (LPD)
# averaging the log-likelihood across posterior draws
lpd_full <- colMeans(log_lik_full)
lpd_null <- colMeans(log_lik_null)

# computing the LPD difference (full model - null model)
lpd_diff <- lpd_full - lpd_null

# computing pointwise Bayes Factors as exp(lpd_diff)
BF_10_pointwise <- exp(lpd_diff)

# storing the results in a dataframe
ppt_df <- full_model$data
lpd_results <- data.frame(time = ppt_df$time, LPD_diff = lpd_diff, BF_10 = BF_10_pointwise)

# viewing the results
head(lpd_results)

# plotting it
lpd_results %>%
    # pivot_longer(cols = LPD_diff:BF_10) %>%
    summarise(BF_10 = mean(LPD_diff), .by = time) %>%
    # ggplot(aes(x = time, y = value, colour = name) ) +
    ggplot(aes(x = time, y = BF_10) ) +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_line() +
    geom_point()
```

```{r fig-post-prob-ratio, echo = FALSE, out.width = "100%", fig.width = 9, fig.asp = 0.5, fig.cap = "Left: Posterior probability of the EEG difference (slope) being above 0 according to the GAM. Right: Ratio of posterior probability according to the GAM (on a log10 scale). Timesteps above threshold (10) are highlighted in green."}
# defining some parameters
n_approx <- 1e3
eps <- 0.05
threshold <- 10


# defining a sequence of x values to make predictions
time_seq <- crossing(
    time = seq(min(ppt_df$time), max(ppt_df$time), length.out = n_approx),
    # time = unique(ppt_df$time),
    condition = c("cond1", "cond2")
    ) %>%
    arrange(condition)

# computing the posterior probability that y is above 0
n_posterior_samples <- ndraws(gam)
prob_y_above_0 <- time_seq %>%
    add_epred_draws(object = gam) %>%
    data.frame() %>%
    dplyr::select(time, condition, .epred, .draw) %>%
    pivot_wider(names_from = condition, values_from = .epred) %>%
    mutate(epred_diff = cond2 - cond1) %>%
    group_by(time) %>%
    summarise(m = mean(epred_diff > 0 + eps) ) %>%
    ungroup() %>%
    mutate(prob_ratio = m / (1 - m) ) %>%
    # replacing infinite values by maximum possible values (number of posterior samples)
    mutate(prob_ratio = ifelse(is.infinite(prob_ratio), n_posterior_samples, prob_ratio) ) %>%
    mutate(prob_ratio = ifelse(prob_ratio == 0, 1/n_posterior_samples, prob_ratio) )

# printing the identified clusters
# the signal was a truncated Gaussian defining an objective onset at 160 ms,
# a maximum at 250 ms, and an offset at 342 ms.
# exceeding_times_gam <- prob_y_above_0 %>%
#     dplyr::filter(prob_ratio > threshold) %>%
#     summarise(cluster_onset = min(time), cluster_offset = max(time) ) %>%
#     # mutate(
#     #     cluster_peak = prob_y_above_0 %>%
#     #         dplyr::filter(post_prob == max(post_prob) ) %>%
#     #         pull(time)
#     #     ) %>%
#     # mutate(true_onset = true_onset, true_peak = true_peak, true_offset = true_offset) %>%
#     mutate(true_onset = true_onset, true_offset = true_offset) %>%
#     mutate(model = "GAM")

# creating a grid covering the range of time and log-transformed y-values
bg_data <- expand.grid(
    time = seq(min(prob_y_above_0$time), max(prob_y_above_0$time), length.out = 100),
    # prob_ratio = 10^seq(log10(0.0001), log10(1e4), length.out = 100)
    prob_ratio = 10^seq(log10(min(prob_y_above_0$prob_ratio) ), log10(max(prob_y_above_0$prob_ratio) ), length.out = 100)
    )

# assigning a fill value, for example, using prob_ratio or another variable
bg_data$fill_value <- log10(bg_data$prob_ratio)

# plotting it
gam_ratio_plot <- prob_y_above_0 %>%
    mutate(above_thres = ifelse(prob_ratio > threshold, 1, NA) ) %>%
    ggplot(aes(x = time, y = prob_ratio) ) +
    # geom_raster(
    #     data = bg_data,
    #     aes(x = time, y = prob_ratio, fill = fill_value),
    #     alpha = 0.5,
    #     interpolate = FALSE,
    #     inherit.aes = FALSE,
    #     show.legend = FALSE
    #     ) +
    # scale_fill_scico(palette = "vik", midpoint = log10(1) ) +
    geom_hline(yintercept = 1, linetype = "dashed") +
    geom_hline(yintercept = 3, linetype = "dashed", color = "darkred") +
    geom_hline(yintercept = 1/3, linetype = "dashed", color = "darkred") +
    geom_hline(yintercept = 10, linetype = "dashed", color = "red") +
    geom_hline(yintercept = 1/10, linetype = "dashed", color = "red") +
    geom_hline(yintercept = 100, linetype = "dashed", color = "orangered") +
    geom_hline(yintercept = 1/100, linetype = "dashed", color = "orangered") +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_line(linewidth = 0.8) +
    geom_point(
        data = . %>% dplyr::filter(!is.na(above_thres) ),
        aes(y = threshold),
        colour = "darkgreen",
        shape = 16,
        size = 3,
        na.rm = TRUE
        ) +
    scale_y_log10(
        labels = label_log(digits = 2),
        breaks = c(1/1e4, 1/1e3, 1/1e2, 1/10, 1, 10, 1e2, 1e3, 1e4)
        # limits = c(0.0001, 1e4)
        ) +
    coord_cartesian(expand = FALSE) +
    labs(
        x = "Time (s)",
        y = expression(Pr(beta>0~"|"~data) / (1 - Pr(beta>0~"|"~data) ) )
        )

# retrieving posterior samples
# posterior_samples <- posterior_epred(object = gp_model, newdata = time_seq)

# computing the difference between cond2 and cond1
# posterior_samples <- posterior_samples[, (ncol(posterior_samples)/2+1):ncol(posterior_samples)] -
#     posterior_samples[, 1:(ncol(posterior_samples)/2)]

# computing the probability that y is above 0
# prob_y_above_0 <- data.frame(
#     time = seq(min(ppt_df$time), max(ppt_df$time), length.out = n_approx)
#     ) %>%
#     mutate(post_prob = colMeans(posterior_samples) ) %>%
#     mutate(m = colMeans(posterior_samples > eps) ) %>%
#     mutate(prob_ratio = m / (1 - m) )

# printing the identified clusters
# the signal was a truncated Gaussian defining an objective onset at 160 ms,
# a maximum at 250 ms, and an offset at 342 ms
# exceeding_times_gp <- prob_y_above_0 %>%
#     dplyr::filter(prob_ratio > threshold) %>%
#     summarise(cluster_onset = min(time), cluster_offset = max(time) ) %>%
#     mutate(
#         cluster_peak = prob_y_above_0 %>%
#             dplyr::filter(post_prob == max(post_prob) ) %>%
#             pull(time)
#         ) %>%
#     mutate(true_onset = true_onset, true_peak = true_peak, true_offset = true_offset) %>%
#     mutate(model = "GP")

# plotting it
# gp_ratio_plot <- prob_y_above_0 %>%
#     mutate(above_thres = ifelse(prob_ratio > threshold, 1, NA) ) %>%
#     ggplot(aes(x = time, y = prob_ratio) ) +
#     geom_hline(yintercept = 1, linetype = "dashed") +
#     geom_hline(yintercept = 3, linetype = "dashed", color = "darkred") +
#     geom_hline(yintercept = 1/3, linetype = "dashed", color = "darkred") +
#     geom_hline(yintercept = 10, linetype = "dashed", color = "red") +
#     geom_hline(yintercept = 1/10, linetype = "dashed", color = "red") +
#     geom_hline(yintercept = 100, linetype = "dashed", color = "orangered") +
#     geom_hline(yintercept = 1/100, linetype = "dashed", color = "orangered") +
#     geom_vline(xintercept = true_onset, linetype = 2) +
#     geom_vline(xintercept = true_offset, linetype = 2) +
#     geom_line() +
#     geom_point(
#         data = . %>% dplyr::filter(!is.na(above_thres) ),
#         aes(y = threshold),
#         colour = "darkgreen",
#         shape = 15,
#         size = 3,
#         na.rm = TRUE
#         ) +
#     scale_y_log10(labels = label_log(digits = 2), limits = c(NA, 1e4) ) +
#     labs(
#         x = "Time (s)",
#         y = expression(Pr(beta>0~"|"~data) / (1 - Pr(beta>0~"|"~data) ) )
#         )

# plotting results for the two models
# gam_ratio_plot + gp_ratio_plot

# or plotting only the GAM
plot_post_test(model = gam, eps = 0.1) + gam_ratio_plot

# printing the results
# bind_rows(exceeding_times_gam, exceeding_times_gp) %>%
#     mutate(across(.cols = where(is.numeric), .fns = ~round(.x, 2) ) ) %>%
#     relocate(model, .before = cluster_onset)
```

\newpage

## Multilevel modelling using summary statistics

The previous model only included constant (fixed) effects, thus not properly accounting for between-participant variability. We next fit a multilevel version of the GAM. Although it is possible to fit a GAMM at the single-trial level, we present a resource-efficient version of the model that is fitted directly on the by-participant summary statistics (mean and SD), similar to what is done in meta-analysis.

```{r meta-gam, message = FALSE}
# averaging across participants
summary_df <- raw_df %>%
    summarise(
        eeg_mean = mean(eeg),
        eeg_sd = sd(eeg),
        .by = c(participant, condition, time)
        )

# defining a contrast for condition
contrasts(summary_df$condition) <- c(-0.5, 0.5)

# fitting the GAM
meta_gam <- brm(
    # using by-participant SD of ERPs across trials
    eeg_mean | se(eeg_sd) ~
        condition + s(time, bs = "cr", k = 20, by = condition) +
        (1 | participant),
    data = summary_df,
    family = gaussian(),
    warmup = 2000,
    iter = 5000,
    chains = 8,
    cores = 8,
    file = "models/meta_gam.rds"
    )
```

```{r meta-preds, echo = FALSE, fig.width = 9, fig.asp = 0.5, fig.cap = "Posterior predictions for the hierarchical GAM."}
# plotting the posterior predictions
meta_gam_preds <- plot(
    conditional_effects(x = meta_gam, effect = "time:condition"),
    points = FALSE, theme = theme_light(), plot = FALSE
    )[[1]] +
    scale_colour_met_d(name = "Johnson") +
    scale_fill_met_d(name = "Johnson") +
    labs(x = "Time (s)", y = "EEG signal (a.u.)")
# meta_gp_preds <- plot(
#     conditional_effects(x = meta_gp, effect = "time:condition"),
#     points = FALSE, theme = theme_light(), plot = FALSE
#     )[[1]] +
#     scale_colour_met_d(name = "Johnson") +
#     scale_fill_met_d(name = "Johnson") +
#     labs(x = "Time (s)", y = "EEG signal (a.u.)")

# combining the two plots
# meta_gam_preds + meta_gp_preds
```

## Error properties of the proposed approach

We then compute the difference between the true and estimated onset/offset of the EEG difference according to various `k` (GAM basis dimension) and `threshold` values. Remember that the EEG signal was generated from a truncated Gaussian with an objective onset at 160 ms, a maximum at 250 ms, and an offset at 342 ms. @fig-onset-error shows that the multilevel GAM can *almost exactly* recover the true onset and offset values, given some reasonable choice of `k` and `threshold` values (e.g., a threshold of 20). We provide more detailed recommendations on how to set `k` in @sec-basis.

```{r fig-onset-error, eval = TRUE, echo = FALSE, results = "hide", dev = "png", dpi = 300, fig.width = 9, fig.asp = 0.5, fig.cap = "Average estimation error for the onset (left) and offset (right) according to various basis dimension and threshold values (according to the multilevel GAM)."}
# defining values of k and threshold to test
# threshold_values <- seq(from = 1, to = 50, by = 1)
# kvalues <- c(5, 10, 15, 20, 25, 30, 35, 40)

# number of simulations to run per combination
# nsims <- 10

# number of posterior samples to use
# n_post_samples <- 1e4

# importing the data generation functions 
# source(file = "code/generate_eeg_data.R")

# initialising results dataframe
# results <- crossing(kvalue = kvalues, sim_id = 1:nsims, threshold = threshold_values) %>%
#     mutate(
#         estimated_onset = NA,
#         estimated_offset = NA,
#         error_onset = NA,
#         error_offset = NA
#         )

# looping over different values of k and threshold
# for (i in seq_len(nrow(results) ) ) {
# 
#     # retrieving current kvalue and threshold
#     kvalue <- results$kvalue[i]
#     threshold <- results$threshold[i]
# 
#     # printing progress
#     cat(
#         "\nAssessing combination:", i, "out of", nrow(results), "combinations...\n",
#         "k-value:", kvalue, "\n", "threshold:", threshold, "\n"
#         )
# 
#     # when we assess a new k-value or new sim_id, we generate new data
#     # and fit a new model...
#     if (threshold == 1) {
# 
#         # generate some data
#         raw_df <- generate_data(
#             n_trials = n_trials, n_ppt = n_ppt, outvar = outvar,
#             srate = srate, ronset = ronset
#             )
# 
#         # sanity check
#         # head(raw_df)
# 
#         # averaging across participants
#         summary_df <- raw_df %>%
#             summarise(
#                 eeg_mean = mean(eeg),
#                 eeg_sd = sd(eeg),
#                 .by = c(participant, condition, time)
#                 )
# 
#         # defining a contrast for condition
#         contrasts(summary_df$condition) <- c(-0.5, 0.5)
# 
#         # fitting the GAM
#         error_model <- brm(
#             # using by-participant SD of EEG across trials
#             eeg_mean | se(eeg_sd) ~
#                 condition + s(time, bs = "cr", k = kvalue, by = condition) +
#                 (1 | participant),
#             data = summary_df,
#             family = gaussian(),
#             warmup = 2000,
#             iter = 5000,
#             chains = 8,
#             cores = 8,
#             ###################################
#             # when running on the HPC cluster #
#             ###################################
#             backend = "cmdstanr",
#             stan_model_args = list(stanc_options = list("O1") )
#             )
# 
#         prob_y_above_0 <- error_model$data %>%
#             # add_epred_draws(object = error_model, ndraws = n_post_samples) %>%
#             add_epred_draws(object = error_model) %>%
#             data.frame() %>%
#             dplyr::select(participant, time, condition, .epred, .draw) %>%
#             pivot_wider(names_from = condition, values_from = .epred) %>%
#             mutate(epred_diff = cond2 - cond1) %>%
#             # computing mean posterior probability at the group level
#             group_by(time) %>%
#             summarise(m = mean(epred_diff > 0) ) %>%
#             mutate(prob_ratio = m / (1 - m) ) %>%
#             ungroup() %>%
#             # ensuring there is no 0 or +Inf values
#             mutate(
#                 prob_ratio = ifelse(
#                     is.infinite(prob_ratio),
#                     ndraws(error_model),
#                     prob_ratio
#                     )
#                 ) %>%
#             mutate(
#                 prob_ratio = ifelse(
#                     prob_ratio == 0,
#                     1 / ndraws(error_model),
#                     prob_ratio
#                     )
#                 )
# 
#     }
# 
#     # finding onset, offset, and peak for the current threshold
#     exceeding_times <- prob_y_above_0 %>%
#         dplyr::filter(prob_ratio > threshold) %>%
#         summarise(
#             cluster_onset = min(time, na.rm = TRUE),
#             cluster_offset = max(time, na.rm = TRUE)
#             ) %>%
#         mutate(true_onset = true_onset, true_offset = true_offset)
# 
#     # storing the results in the dataframe
#     if (nrow(exceeding_times) > 0) {
# 
#         results$estimated_onset[i] <- exceeding_times$cluster_onset
#         results$estimated_offset[i] <- exceeding_times$cluster_offset
# 
#         # computing errors
#         results$error_onset[i] <- abs(
#             exceeding_times$cluster_onset - exceeding_times$true_onset
#             )
#         results$error_offset[i] <- abs(
#             exceeding_times$cluster_offset - exceeding_times$true_offset
#             )
# 
#     }
# 
# }

# saving the simulation results
# saveRDS(object = results, file = "results/meta_gam_error_properties.rds")

# running the simulation on the HPC cluster
# source("code/meta_gam_errors_cluster.R")

# loading the simulation results
# results <- readRDS(file = "results/meta_gam_error_properties.rds")
results <- readRDS(file = "results/meta_gam_error_properties_cluster.rds")

# plotting the results
results %>%
    group_by(kvalue, threshold, sim_id) %>%
    # mutate(error_onset = ) %>%
    summarise(
        # value = if (onset_offset[1] == "onset") min(value) else max(value),
        cluster_onset = min(cluster_onset),
        cluster_offset = max(cluster_offset)
        # .groups = "drop"
        ) %>%
    ungroup() %>%
    group_by(kvalue, threshold) %>%
    # summarise(across(estimated_onset:error_offset, median) ) %>%
    # summarise(across(estimated_onset:error_offset, mean) ) %>%
    mutate(
        error_onset = abs(cluster_onset - true_onset),
        error_offset = abs(cluster_offset - true_offset)
        ) %>%
    summarise(across(error_onset:error_offset, mean) ) %>%
    ungroup() %>%
    pivot_longer(cols = error_onset:error_offset) %>%
    mutate(name = factor(x = name, levels = c("error_onset", "error_offset") ) ) %>%
    ggplot(
        aes(
            x = as.factor(kvalue),
            y = as.factor(threshold),
            fill = value
            )
        ) +
    geom_raster(interpolate = FALSE) +
    # geom_tile() +
    # geom_point(
    #     data = results %>%
    #         group_by(kvalue, threshold) %>%
    #         summarise(across(estimated_onset:error_offset, mean) ) %>%
    #         ungroup() %>%
    #         na.omit() %>%
    #         filter(error_onset == min(error_onset) ),
    #     # aes(x = kvalue, y = threshold),
    #     aes(x = kvalue, y = value),
    #     color = "orangered", size = 4, shape = 4,
    #     # show.legend = FALSE
    #     show.legend = TRUE
    #     ) +
    # scale_x_continuous(expand = c(0, 0) ) +
    # scale_y_continuous(expand = c(0, 0) ) +
    facet_wrap(~name) +
    scale_x_discrete(expand = c(0, 0) ) +
    scale_y_discrete(expand = c(0, 0), breaks = c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50) ) +
    # scale_y_continuous(n.breaks=10) +
    scale_fill_gradientn(colors = rev(met.brewer("Hokusai1") ) ) +
    # scale_fill_viridis_c(direction = 1) +
    labs(
        x = "GAM basis dimension (k)",
        y = "Threshold",
        fill = "Error (s)"
        )

# results %>%
#     group_by(kvalue, threshold) %>%
#     summarise(across(estimated_onset:error_offset, mean) ) %>%
#     ungroup() %>%
#     # head()
#     pivot_longer(cols = error_onset:error_offset) %>%
#     mutate(name = factor(x = name, levels = c("error_onset", "error_offset") ) ) %>%
#     ggplot(
#         aes(
#             x = threshold,
#             y = value,
#             colour = kvalue,
#             fill = kvalue
#             )
#         ) +
#     # geom_raster(interpolate = FALSE) +
#     # geom_tile() +
#     # geom_line() +
#     geom_point() +
#     # geom_point(
#     #     data = results %>% dplyr::filter(error_onset == min(error_onset, na.rm = TRUE) ),
#     #     # aes(x = kvalue, y = threshold),
#     #     aes(x = as.factor(kvalue), y = threshold),
#     #     color = "orangered", size = 4, shape = 4,
#     #     # show.legend = FALSE
#     #     show.legend = TRUE
#     #     ) +
#     # scale_x_continuous(expand = c(0, 0) ) +
#     # scale_y_continuous(expand = c(0, 0) ) +
#     facet_wrap(~name) +
#     scale_colour_viridis_c() +
#     # scale_x_discrete(expand = c(0, 0) ) +
#     # scale_y_discrete(expand = c(0, 0) ) +
#     # scale_fill_gradientn(colors = rev(met.brewer("Hokusai1") ) )
#     labs(
#         x = "GAM basis dimension (k)",
#         y = "Threshold",
#         fill = "Error (s)"
#         )

# plotting the results
# onset_plot <- results %>%
#     # ggplot(aes(x = eps, y = threshold, fill = error_onset) ) +
#     # ggplot(aes(x = as.factor(kvalue), y = threshold, fill = error_onset) ) +
#     ggplot(aes(x = as.factor(kvalue), y = as.factor(threshold), fill = error_onset) ) +
#     geom_tile(show.legend = FALSE) +
#     geom_point(
#         data = results %>% dplyr::filter(error_onset == min(error_onset, na.rm = TRUE) ),
#         # aes(x = kvalue, y = threshold),
#         aes(x = as.factor(kvalue), y = threshold),
#         color = "orangered", size = 4, shape = 4,
#         # show.legend = FALSE
#         show.legend = TRUE
#         ) +
#     # scale_x_continuous(expand = c(0, 0) ) +
#     # scale_y_continuous(expand = c(0, 0) ) +
#     scale_x_discrete(expand = c(0, 0) ) +
#     scale_y_discrete(expand = c(0, 0) ) +
#     scale_fill_gradientn(colors = rev(met.brewer("Hokusai1") ) ) +
#     labs(
#         title = "Onset error",
#         # x = "eps",
#         x = "GAM basis dimension (k)",
#         y = "Threshold",
#         fill = "Error (s)"
#         )
# 
# offset_plot <- results %>%
#     # ggplot(aes(x = eps, y = threshold, fill = error_offset) ) +
#     # ggplot(aes(x = kvalue, y = threshold, fill = error_offset) ) +
#     ggplot(aes(x = as.factor(kvalue), y = as.factor(threshold), fill = error_offset) ) +
#     geom_tile(show.legend = FALSE) +
#     geom_point(
#         data = results %>% dplyr::filter(error_offset == min(error_offset, na.rm = TRUE) ),
#         # aes(x = kvalue, y = threshold),
#         aes(x = as.factor(kvalue), y = threshold),
#         color = "orangered", size = 4, shape = 4,
#         # show.legend = FALSE
#         show.legend = TRUE
#         ) +
#     # scale_x_continuous(expand = c(0, 0) ) +
#     # scale_y_continuous(expand = c(0, 0) ) +
#     scale_x_discrete(expand = c(0, 0) ) +
#     scale_y_discrete(expand = c(0, 0) ) +
#     scale_fill_gradientn(colors = rev(met.brewer("Hokusai1") ) ) +
#     labs(
#         title = "Offset error",
#         # x = "eps",
#         x = "GAM basis dimension (k)",
#         y = "Threshold",
#         fill = "Error (s)"
#         )

# combining the plots
# onset_plot + offset_plot + plot_layout(guides = "collect")

# displaying best parameters values (according to onset error)
# results %>%
#     dplyr::select(-estimated_peak, -error_peak) %>%
#     arrange(error_onset) %>%
#     data.frame() %>%
#     head()

# displaying best parameters values (according to offset error)
# results %>%
#     dplyr::select(-estimated_peak, -error_peak) %>%
#     arrange(error_offset) %>%
#     data.frame() %>%
#     head()

# displaying best parameters values (according to both onset and offset error)
# results %>%
#     group_by(kvalue, threshold) %>%
#     summarise(across(estimated_onset:error_offset, mean) ) %>%
#     # summarise(across(estimated_onset:error_offset, mode) ) %>%
#     ungroup() %>%
#     arrange(error_onset, error_offset) %>%
#     data.frame() %>%
#     head(40)
```

\newpage

## Comparing the identified onsets/offsets to other approaches

We then compared the ability of the GAMM to correctly estimate the onset and offset of the ERP difference to other widely-used methods. First, we conducted mass-univariate t-tests (thus treating each timestep independently) and identified the onset and offset of the ERP difference as the first and last values crossing an arbitrary significance threshold ($\alpha = 0.05$). We then followed the same approach but after applying different forms of multiplicity correction to the p-values. We compared two methods that control the false discovery rate (FDR) [i.e., BH95, @benjamini1995; and BY01, @benjamini2001], one method that controls the family-wise error rate (FWER) [i.e., Holm–Bonferroni method, @holm1979], and two cluster-based permutation methods [permutation with a single cluster-forming threshold and threshold-free cluster enhancement, TFCE, @smith2009]. The BH95, BY01, and Holm corrections were applied to the p-values using the `p.adjust()` function in `R`. The cluster-based inference was implemented using a cluster-sum statistic of squared t-values, as implemented in `MNE-Python` [@gramfort2013], called via `r pkrt("reticulate")`. We also compared these estimates to the onset and offset as estimated using the binary segmentation algorithm, as implemented in `r pkrt("changepoint")`, and applied directly to the squared t-values [as in @rousselet_using_2025]. @fig-corrections illustrates the onsets and offsets estimated by each method on a single simulated dataset and shows that all methods over-estimate the true onset and under-estimate the true offset.

```{=html}
<!--

For visualisation and interpretability purposes, we converted p-values to s-values, which can be interpreted as bits of surprising information, assuming a null effect [@greenland2019] (@fig-corrections).

-->
```

```{r fig-corrections, echo = FALSE, results = "hide", fig.width = 8, fig.asp = 0.5, fig.cap = "Exemplary timecourse of squared t-values with true onset and offset (vertical black dashed lines) and onsets/offsets identified using the raw p-values, the corrected p-values (BH95, BY01, Holm), the cluster-based methods (Cluster mass, TFCE), or using the binary segmentation method (Change point)."}
# defining an arbitrary alpha threshold
aath <- 0.05

# summarising raw_data per participant
ppt_data <- raw_df %>%
    summarise(eeg = mean(eeg), .by = c(participant, condition, time) ) %>%
    pivot_wider(names_from = condition, values_from = eeg) %>%
    mutate(eeg_diff = cond2 - cond1)

# massive univariate t-tests
tests_results <- ppt_data %>%
    group_by(time) %>%
    summarise(
        tval = t.test(x = eeg_diff, mu = 0)$statistic^2,
        pval = t.test(x = eeg_diff, mu = 0)$p.value
        ) %>%
    mutate(
        pval_bh = p.adjust(p = pval, method = "BH"),
        pval_by = p.adjust(p = pval, method = "BY"),
        pval_holm = p.adjust(p = pval, method = "holm")
        ) %>%
    ungroup()

# defining a function to find onset and offset in timeseries
find_clusters <- function (df, threshold = aath) {
    
    df %>%
        select(-tval) %>%
        pivot_longer(cols = -time) %>%
        group_by(name) %>%
        mutate(above_threshold = value <= threshold) %>%
        mutate(cluster_change = c(TRUE, diff(above_threshold) != 0) ) %>%
        filter(above_threshold) %>%
        mutate(cluster_id = cumsum(cluster_change) ) %>%
        group_by(name, cluster_id) %>%
        summarise(cluster_onset = first(time), cluster_offset = last(time) ) %>%
        select(cluster_onset, cluster_offset)
    
}

# estimating onset and offset using the binary segmentation method
res <- cpt.meanvar(data = tests_results$tval, method = "BinSeg", Q = 2)
onset_cpt <- tests_results$time[res@cpts[1]]
offset_cpt <- tests_results$time[res@cpts[2]]

# running the cluster-based permutation tests
source(file = "code/mne_clusters.R")

# joining results in a common dataframe
methods_offset = data.frame(
    method = c("pval", "pval_bh", "pval_by", "pval_holm", "cluster_mass", "cluster_tfce", "cpt"),
    y_offset = rev(seq(from = -150, to = -20, length.out = 7) )
    )

clusters <- find_clusters(df = tests_results) %>%
    rename(method = name, onset = cluster_onset, offset = cluster_offset) %>%
    bind_rows(
        data.frame(
            method = "cpt",
            onset = onset_cpt,
            offset = offset_cpt
            )
        ) %>%
    data.frame() %>%
    bind_rows(data.frame(method = "cluster_mass", onset = onset_cluster_mass, offset = offset_cluster_mass) ) %>%
    bind_rows(data.frame(method = "cluster_tfce", onset = onset_cluster_mass_tfce, offset = offset_cluster_mass_tfce) ) %>%
    left_join(methods_offset, by = "method") %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "pval", "pval_bh", "pval_by", "pval_holm",
                "cluster_mass", "cluster_tfce", "cpt"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm",
                "Cluster mass", "TFCE", "Change point"
                ),
            )
        )

# plotting the results
tests_results %>%
    ggplot(aes(x = time, y = tval) ) +
    geom_hline(yintercept = 0, linetype = 1) +
    geom_area(position = "identity") +
    geom_vline(xintercept = true_onset, linetype = 2) +
    geom_vline(xintercept = true_offset, linetype = 2) +
    geom_segment(
        # data = all_methods_onset_offset,
        data = clusters,
        aes(
            x = onset,
            xend = offset,
            y = y_offset,
            yend = y_offset,
            colour = method,
            ),
        linewidth = 1,
        lineend = "round",
        inherit.aes = FALSE
        ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(x = "Time (s)", y = "Squared t-value", colour = NULL)

# plotting -log2(p) (surprisal under H0) with thresholds (on p-values)
# https://lesslikely.com/statistics/s-values/
# a s-value of 4 (bits of information) is no more surprising than
# getting all heads on 4 fair coin tosses
# pvals_plot <- tests_results %>%
#     pivot_longer(cols = pval:pval_holm) %>%
#     mutate(sval = -log2(value) ) %>%
#     ggplot(aes(x = time, y = sval, colour = name) ) +
#     geom_density(
#         aes(fill = name),
#         stat = "identity",
#         alpha = 0.8,
#         show.legend = FALSE
#         ) +
#     geom_hline(yintercept = -log2(aath), linetype = 2) +
#     geom_vline(xintercept = true_onset, linetype = 2) +
#     geom_vline(xintercept = true_offset, linetype = 2) +
#     scale_fill_manual(values = met.brewer(name = "Johnson", n = 5) ) +
#     scale_colour_manual(values = met.brewer(name = "Johnson", n = 5) ) +
#     labs(
#         x = "Time (s)",
#         y = "-log2(p-value)"
#         )

# combining the two plots
# tvals_plot + pvals_plot +
#     # plot_annotation(title = plot_title, tag_levels = "A") &
#     # theme(plot.title = element_text(hjust = 0.5) )
#     plot_annotation(tag_levels = "A") + plot_layout(guides = "collect")
```

\newpage

## Simulation study

The various methods were compared using the median error (i.e., true-estimated) and variance of onset/offset estimates computed on 10.000 simulated datasets.

## Application to actual MEG data

To provide a more exhaustive assessment of these methods, we  also evaluated the performance of the proposed approach on actual MEG data [decoding results from @nalborczyk:inprep]. In this study, we conducted time-resolved multivariate pattern analysis (MVPA, also known as decoding) of MEG data during reading tasks. As a result, we obtain a timecourse of decoding performance (ROC AUC), bounded between 0 and 1, for each participant (for a total of 32 participants). Next, we want to *test* whether the group-level average decoding accuracy is above chance (i.e., 0.5) at each timestep (@fig-decoding-data). To achieve this, we fitted a similar GAM as discussed previously, but we replaced the $\mathrm{Normal}$ likelihood function by a $\mathrm{Beta}$ one to account for the bounded nature of AUC values (between 0 and 1) [for a tutorial on Beta regression, see @coretta2025].

Note that although we chose a basis dimension of $k=50$, which seems appropriate for the present data, this choice should be adapted according to the properties of the modelled data (e.g., signal-to-noise ratio, prior low-pass filtering, sampling rate, etc) and should be assessed by the usual model checking tools (e.g., posterior predictive checks, see also @sec-basis). To better distinguish signal from noise, we also defined a region of practical equivalence [ROPE, @kruschke2017], defined as the chance level plus the standard deviation of the (group-level average) decoding performance during the baseline period.

```{r decoding-model, eval = FALSE}
# fitting the Beta GAM
meg_decoding_gam <- brm(
    auc ~ s(time, bs = "cr", k = 50),
    data = decoding_df,
    family = Beta(),
    warmup = 2000,
    iter = 5000,
    chains = 4,
    cores = 4
    )
```

We assessed the reliability of the proposed approach using a form of permutation-based split-half reliability [as for instance in @rosenblatt2018], which consisted of the following steps. First, we created 1000 split halves of the data (i.e., with half the participants in the original data, that is, 16 participants). For each split, we estimated the onset/offset using all methods described previously. Third, we summarised the distribution of onset/offset estimates using the median "error" (i.e., difference between the split estimate and the estimate obtained using the full dataset) and the variance across splits. This approach allows assessing how similar the estimate of each half split is to the full dataset (thus acting as a proxy for the population) and how variable the estimates are (across split halves).

```{=html}
<!--

OR, we could compute the onset/offset at the participant-level and correlate estimates across splits, BUT, individual-level estimates are not available for all methods...

To test consistency between participants we calculated the split-half consistency across 1000 permutations. For each permutation, the participants were randomly split into two halves. Onset and offset were estimated at the participant level. Then, the Spearman rank correlation between the halves was calculated, corrected by the Spearman-Brown split-half reliability correction. A null distribution was calculated by randomly onset/offset values between words within one of the participant halves.

-->
```

```{r fig-decoding-data, cache = FALSE, echo = FALSE, out.width = "75%", fig.asp = 0.75, fig.cap = "Group-level average decoding performance (N=32) superimposed with the GAM predictions (in blue) and the region of practical equivalence (ROPE, in orange) computed from the baseline period. The blue horizontal markers indicate the timesteps at which the posterior probability ratio exceeds 20."}
# importing the reshaped MEG decoding results
decoding_df <- read.csv(file = "data/decoding_results_reshaped.csv") %>%
    # removing some participants
    mutate(participant_id = cur_group_id(), .by = participant) %>%
    filter(participant_id < 33) %>%
    select(-participant_id)

# averaging across participants
decoding_summary_df <- decoding_df %>%
    summarise(
        auc_mean = mean(auc),
        auc_sd = sd(auc),
        .by = c(participant, time)
        )

# defining the chance level
chance_level <- 0.5

# computing the smallest effect size of interest (SESOI)
# as the SD of decoding performance during the baseline
sesoi <- decoding_summary_df %>%
    filter(time < 0) %>%
    summarise(auc_mean = mean(auc_mean), .by = time) %>%
    summarise(sesoi = sd(auc_mean) ) %>%
    pull(sesoi)

# fitting the Beta GAM
meg_decoding_gam <- brm(
    auc_mean ~ s(time, bs = "cr", k = 50),
    data = decoding_summary_df,
    family = Beta(),
    warmup = 2000,
    iter = 5000,
    chains = 4,
    cores = 4,
    file = "models/meg_decoding_gam.rds"
    )

# fitting the Beta GAMM
# meg_decoding_gam <- brm(
#     # auc_mean ~ s(time, bs = "cr", k = 50),
#     auc_mean | se(auc_sd) ~ s(time, bs = "cr", k = 10) +
#         s(time, participant, bs = "fs", xt = "cr", k = 10, m = 1),
#     data = decoding_summary_df,
#     # family = Beta(),
#     # warmup = 2000,
#     # iter = 5000,
#     chains = 4,
#     cores = 4,
#     file = "models/meg_decoding_gamm.rds"
#     )

# some posterior predictive checks
# pp_check(gam_split, prefix = "ppd")
# pp_check(gam_split)
# pp_check(gam_split, type = "error_hist")
# pp_check(gam_split, type = "stat_2d")
# pp_check(gam_split, type = "ecdf_overlay")

# computing the posterior probability
prob_y_above_0 <- data.frame(time = unique(meg_decoding_gam$data$time) ) %>%
    # retrieving the posterior samples
    add_epred_draws(object = meg_decoding_gam) %>%
    # converting to dataframe
    data.frame() %>%
    # computing mean posterior probability at the group level
    group_by(time) %>%
    summarise(m = mean(.epred > (0 + chance_level + sesoi) ) ) %>%
    mutate(prob_ratio = m / (1 - m) ) %>%
    # ensuring there is no 0 or +Inf values
    mutate(prob_ratio = ifelse(is.infinite(prob_ratio), ndraws(meg_decoding_gam), prob_ratio) ) %>%
    mutate(prob_ratio = ifelse(prob_ratio == 0, 1 / ndraws(meg_decoding_gam), prob_ratio) ) %>%
    ungroup()

# finding onset and offset for the current threshold
# exceeding_times <- prob_y_above_0 %>%
#     dplyr::filter(prob_ratio > post_prob_ratio_threshold) %>%
#     summarise(
#         cluster_onset = min(time, na.rm = TRUE),
#         cluster_offset = max(time, na.rm = TRUE)
#         )
# or finding all clusters
# defining a function to find onset and offset in timeseries
# find_clusters <- function (mask, timeseries) {
#     
#     # initialising onset and offset values
#     onset <- NA
#     offset <- NA
#     
#     # identifying the onset and offset
#     masked_timesteps <- try(timeseries[which(mask)], silent = TRUE)
#     onset <- head(x = masked_timesteps, n = 1)
#     offset <- tail(x = masked_timesteps, n = 1)
#     
#     # returning the onset and offset
#     return (c(onset, offset) )
#     
# }
# 
# find_onset_offset(
#         tests_results$pval <= alpha_level,
#         tests_results$time
#         )
# prob_y_above_0 %>%
#     dplyr::filter(prob_ratio >= post_prob_ratio_threshold) %>%
#     dplyr::pull(time)
find_clusters <- function (df, threshold = 20) {
    
    df %>%
        mutate(above_threshold = prob_ratio >= threshold) %>%
        mutate(cluster_change = c(TRUE, diff(above_threshold) != 0) ) %>%
        filter(above_threshold) %>%
        mutate(cluster_id = cumsum(cluster_change)) %>%
        group_by(cluster_id) %>%
        summarise(cluster_onset = first(time), cluster_offset = last(time), .groups = "drop") %>%
        select(cluster_onset, cluster_offset)
    
}

post_prob_ratio_threshold <- 20
clusters <- find_clusters(prob_y_above_0, threshold = post_prob_ratio_threshold)
    
# checking model's predictions against raw data
brms_color <- met.brewer(name = "Johnson", n = 8)[8]
plot(
    conditional_effects(x = meg_decoding_gam),
    # line_args = list(colour = "steelblue", fill = "steelblue", alpha = 0.3),
    line_args = list(colour = brms_color, fill = brms_color, alpha = 0.3),
    points = FALSE, plot = FALSE
    )[[1]] +
    geom_hline(yintercept = 0.5, linetype = 2) +
    geom_vline(xintercept = 0.0, linetype = 2) +
    annotate(
        geom = "rect",
        xmin = -Inf, xmax = Inf,
        ymin = chance_level - sesoi, ymax = chance_level + sesoi,
        fill ="orangered",
        # fill ="darkred",
        alpha = 0.2
        ) +
    geom_segment(
        data = clusters,
        aes(
            x = cluster_onset,
            xend = cluster_offset,
            y = 0.48, yend = 0.48
            ),
        # colour = "steelblue",
        colour = brms_color,
        inherit.aes = FALSE,
        lineend = "round",
        linewidth = 2
        ) +
    geom_line(
        data = decoding_summary_df %>% summarise(auc_mean = mean(auc_mean), .by = time),
        aes(x = time, y = auc_mean), inherit.aes = FALSE
        ) +
    labs(x = "Time (s)", y = "Decoding accuracy (ROC AUC)")
```

\newpage

# Results

This section is divided in two parts. First, we present the results from the simulation study, assessing the bias and variance of each method when applied to simulated data in which the ground truth is known. Second, we present the results obtained when applying the different methods to actual MEG data (decoding performance through time), assessing the reliability of the estimates provided by each method.

## Simulation study (bias and variance)

@fig-simulation-results shows a summary of the simulation results, revealing that the proposed approach (`BGAM`) has the lowest median error and variance for both the onset and offset estimates. The `Cluster mass`, `Holm`, and `Change point` also have good performance, but surprisingly, the `TFCE` method has relatively bad performance for estimating the effect offset. Unsurprisingly, the `Raw p-value`, `FDR BH95`, and `FDR BY01` methods show the worst performance.

```{r table-simulation-results, eval = FALSE, echo = FALSE}
# loading the simulation results
# Here are a couple of example tables (@table-simulation-results)....
sim_results <- readRDS(file = "results/errors_results_100sims.rds")

# summarising the results
sim_results_summary <- sim_results %>%
    # keeping only threshold = 20 for BGAM and threshold = 0.05 for frequentist results
    filter(threshold %in% c(0.05, 20) ) %>%
    mutate(
        error = case_when(
            grepl("onset", onset_offset) ~ value - true_onset,
            grepl("offset", onset_offset) ~ value - true_offset
            )
        ) %>%
    group_by(method, onset_offset) %>%
    summarise(
        mean_error = mean(error, na.rm = TRUE),
        median_error = median(error, na.rm = TRUE),
        median_abs_error = median(abs(error), na.rm = TRUE),
        rmse = sqrt(mean(error)^2),
        variance = var(error, na.rm = TRUE),
        mad = mad(error, na.rm = TRUE),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm",
                "cluster_mass", "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (new method)"
                ),
            )
        ) %>%
    mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
    data.frame()

# creating the table
# library(tinytable)
# tt(
#     x = sim_results_summary,
#     digits = 2
#     )
```

```{r fig-simulation-results, echo = FALSE, fig.width = 8, fig.asp = 0.5, fig.cap = "Median error and variance of onset and offset estimates for each method. The vertical dashed line represents an unbiased estimate of the true onset/ofsset. Variance is plotted on a log10 scale for visual purposes."}
# loading the simulation results
sim_results <- readRDS(file = "results/errors_results_100sims.rds")

# identifying the best brms results
# sim_results %>%
#     filter(method == "brms") %>%
#     pivot_wider(names_from = onset_offset, values_from = value) %>%
#     summarise(
#         MAE_onset = median(abs(onset - true_onset) ),
#         variance_onset = var(onset),
#         MAE_offset = median(abs(offset - true_offset) ),
#         variance_offset = var(offset),
#         .by = threshold
#         ) %>%
#     arrange(MAE_onset, MAE_offset)

# computing and plotting bias and variance
# bias := true_onset - median(sim_onset_distrib)
# variance := variance(sim_onset_distrib)
sim_results %>%
    # keeping only threshold = 20 for BGAM and threshold = 0.05 for frequentist results
    filter(threshold %in% c(0.05, 20) ) %>%
    mutate(
        error = case_when(
            # grepl("onset", measure) ~ abs(value - true_onset),
            # grepl("offset", measure) ~ abs(value - true_offset)
            grepl("onset", onset_offset) ~ value - true_onset,
            grepl("offset", onset_offset) ~ value - true_offset
            )
        ) %>%
    group_by(method, onset_offset) %>%
    summarise(
        median_error = median(error, na.rm = TRUE),
        variance = var(error, na.rm = TRUE),
        # variance = mad(error, na.rm = TRUE),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm",
                "cluster_mass", "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (new method)"
                ),
            )
        ) %>%
    ggplot(
        aes(
            x = median_error,
            y = variance,
            colour = method,
            fill = method
            )
        ) +
    geom_vline(xintercept = 0, linetype = 2) +
    geom_point(
        size = 2,
        pch = 21,
        colour = "white",
        show.legend = FALSE
        ) +
    geom_label_repel(
        aes(label = method),
        colour = "white",
        size = 3,
        segment.color = NA,
        show.legend = FALSE
        ) +
    scale_y_log10() +
    facet_wrap(~onset_offset) +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(x = "Median error (s)", y = "Variance (log-scale)")

# saving the plot
ggsave(
    filename = "figures/simulation_results_median_error_variance.png",
    width = 12, height = 6, dpi = 300,
    device = "png"
    )
```

\newpage

## Application to actual MEG data (reliability)

@fig-onset-offset shows the group-level average decoding performance through time with onset and offset estimates for each method. Overall, this figure shows that both the `Raw p-value` and `FDR BH95` methods are extremely lenient, considering that the decoding performance is above chance before the onset of the stimulus (false positive) and until the end of the trial. The `Change point` and `Cluster mass` methods seem the most conservative methods, identifying a time window from approximately +60ms to +500ms. The `Holm`, `TFCE`, and `BGAM` methods produce similar estimates of onset and offset, ranging from approximately +60ms to +650ms, although the `BGAM` method seems to result in fewer clusters.[^1]

[^1]: It should be noted that although each method can produce several "clusters" of "significant" timesteps, we only considered the first (onset) and last (offset) timesteps identified by each method to compute the estimation error.

```{r fig-onset-offset, echo = FALSE, fig.width = 8, fig.asp = 0.6, fig.cap = "Group-level average decoding performance through time with onset and offset estimates for each method [data from @nalborczyk:inprep]."}
# loading the results
# reliability_results <- readRDS(file = "results/reliability_results.rds")
# reliability_results <- readRDS(file = "results/reliability_results_cluster.rds")
reliability_results <- readRDS(file = "results/reliability_results_100splits.rds")

# extracting the full data estimates
full_reliability_results <- reliability_results %>%
    filter(split_id == max(split_id) ) %>%
    # keeping only the first and last timestep
    # group_by(split_id, method, onset_offset) %>%
    # summarise(
    #     value = if (onset_offset[1] == "onset") min(value) else max(value),
    #     .groups = "drop"
    #     ) %>%
    # ungroup() %>%
    # pivot_longer(cols = -split_id, names_to = "measure", values_to = "value") %>%
    # data.frame() %>%
    # select(-split_id) %>%
    # separate(measure, into = c("type", "method"), sep = "_", extra = "merge") %>%
    pivot_wider(names_from = onset_offset, values_from = value) %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
                "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (new method)"
                )
            )
        ) %>%
    mutate(y_offset = 0.5 - as.numeric(factor(method) ) * 0.01 - 0.01)

# checking model's predictions against raw data
decoding_df %>%
    summarise(
        auc_mean = mean(auc),
        auc_sd = sd(auc),
        auc_se = sd(auc) / sqrt(n() ),
        .by = time
        ) %>%
    ggplot(aes(x = time, y = auc_mean) ) +
    geom_hline(yintercept = 0.5, linetype = 2) +
    geom_vline(xintercept = 0.0, linetype = 2) +
    # annotate(
    #     geom = "rect",
    #     xmin = -Inf, xmax = Inf,
    #     ymin = chance_level - sesoi, ymax = chance_level + sesoi,
    #     fill ="orangered",
    #     alpha = 0.2
    #     ) +
    geom_segment(
        data = full_reliability_results,
        aes(
            x = onset, xend = offset,
            y = y_offset, yend = y_offset,
            colour = method,
            ),
        inherit.aes = FALSE,
        lineend = "round",
        linewidth = 1.5
        ) +
    geom_ribbon(
        aes(ymin = auc_mean - auc_se, ymax = auc_mean + auc_se),
        alpha = 0.2,
        ) +
    geom_line(
        data = decoding_summary_df %>%
            summarise(auc_mean = mean(auc_mean), .by = time),
        aes(x = time, y = auc_mean), inherit.aes = FALSE
        ) +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(x = "Time (s)", y = "Decoding accuracy (ROC AUC)", colour = NULL)

# saving the plot
ggsave(
    filename = "figures/meg_decoding_results_all_methods.png",
    width = 12, height = 6, dpi = 300,
    device = "png"
    )
```

@fig-reliability shows the median difference between the onset and offset estimates from each data split and the onset and offset estimates from the full dataset (x-axis) along with the variance of its onset and offset estimates across data splits (error bar). This figure reveals that the `BGAM` *onset and offset* estimates on each split are the closest to the estimates from the full dataset on average (0ms difference for the onset estimate and 5ms difference for the offset estimate). The `Raw p-value` method has similar performance, but given the aberrant estimates it produces (cf. @fig-onset-offset), the fact that it is consistent between data splits and the full dataset is not convincing on its own. The `Change point` method also has a very good performance (i.e., very low difference between split estimates and full estimates), but produces too short cluster of significant decoding performance (cf. @fig-onset-offset).[^2] Overall, the figure reveals that for all other methods, split datasets produce later onset estimates and earlier offset estimates (as compared to the estimates from the model fitted on the full dataset). These results highlight that the results of any method should be assessed using both i) good asymptotic properties on simulated data, ii) sensible identified clusters in actual data, and iii) reliable/stable estimates on actual data.

[^2]: As in @rousselet_using_2025, we fixed the number of expected change points to two in the binary segmentation algorithm, thus producing always one cluster.

```{r fig-reliability, echo = FALSE, cache = FALSE, fig.width = 8, fig.asp = 0.5, fig.cap = "Median error and median absolute deviation of the error for the onset (left) and offset (right) estimates according to each method. Methods are ordered from lowest (top) to highest (bottom) median absolute error (separately for the onset and offset estimates)."}
# extracting the full data estimates
full_reliability_results <- reliability_results %>%
    filter(split_id == max(split_id) ) %>%
    # keeping only the first and last timestep
    group_by(split_id, method, onset_offset) %>%
    summarise(
        value = if (onset_offset[1] == "onset") min(value) else max(value),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    data.frame() %>%
    select(-split_id)

# analysing the split results
# reliability_results %>%
#     slice(1:nrow(.)-1) %>%
#     pivot_longer(cols = -split_id, names_to = "measure", values_to = "value") %>%
#     data.frame() %>%
#     left_join(
#         full_reliability_results,
#         by = "measure",
#         suffix = c("_split", "_full")
#         ) %>%
#     # mutate(error = abs(value_split - value_full) ) %>%
#     mutate(error = value_split - value_full) %>%
#     # mutate(error = (value_split - value_full)^2) %>%
#     separate(measure, into = c("type", "method"), sep = "_", extra = "merge") %>%
#     group_by(type, method) %>%
#     summarise(
#         MAE = median(error, na.rm = TRUE),
#         # variance = var(error, na.rm = TRUE),
#         variance = var(value_split, na.rm = TRUE)
#         # variance = sum(error, na.rm = TRUE)
#         # .groups = "drop"
#         ) %>%
#     ungroup() %>%
#     # now plotting
#     mutate(type = factor(x = type, levels = c("onset", "offset") ) ) %>%
#     mutate(
#         method = factor(
#             x = method,
#             levels = c(
#                 "p", "bh", "by", "holm", "cluster_mass",
#                 "cluster_tfce", "cpt", "brms", "brms_full"
#                 ),
#             labels = c(
#                 "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
#                 "TFCE", "Change point", "brms", "brms_full"
#                 ),
#             )
#         ) %>%
#     ggplot(
#         aes(
#             x = MAE,
#             y = variance,
#             colour = method,
#             fill = method
#             )
#         ) +
#     geom_vline(xintercept = 0, linetype = 2) +
#     geom_point(
#         size = 2,
#         pch = 21,
#         colour = "white",
#         show.legend = FALSE
#         ) +
#     geom_label_repel(
#         aes(label = method),
#         colour = "white",
#         size = 3,
#         segment.color = NA,
#         max.overlaps = 20,
#         show.legend = FALSE
#         ) +
#     scale_y_log10() +
#     facet_wrap(~type, scales = "free") +
#     scale_fill_manual(values = met.brewer(name = "Johnson", n = 9) ) +
#     scale_colour_manual(values = met.brewer(name = "Johnson", n = 9) ) +
#     labs(
#         x = "Average (median) difference to full dataset (s)",
#         y = "Variance across splits"
#         )

# defining a consistent colour palette using original method levels
original_methods <- c(
    "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
    "TFCE", "Change point", "BGAM (new method)"
    )

color_palette <- setNames(
    met.brewer(name = "Johnson", n = length(original_methods) ),
    original_methods
    )

# or plotting only the error in descending order
reliability_results %>%
    filter(split_id < max(split_id) ) %>%
    data.frame() %>%
    # keeping only the first and last timestep
    group_by(split_id, method, onset_offset) %>%
    summarise(
        value = if (onset_offset[1] == "onset") min(value) else max(value),
        .groups = "drop"
        ) %>%
    ungroup() %>%
    select(-split_id) %>%
    left_join(
        full_reliability_results,
        # by = "measure",
        by = c("method", "onset_offset"),
        suffix = c("_split", "_full")
        ) %>%
    group_by(method, onset_offset) %>%
    # mutate(error = abs(value_split - value_full) ) %>%
    mutate(error = value_split - value_full) %>%
    summarise(
        # mean_error = mean(error, na.rm = TRUE),
        # variance = var(error, na.rm = TRUE),
        # variance = var(value_split, na.rm = TRUE)
        # variance = sd(error, na.rm = TRUE) / sqrt(n() )
        # variance = sum(error, na.rm = TRUE)
        median_error = median(error, na.rm = TRUE),
        # variance = IQR(error, na.rm = TRUE)
        variance = mad(error, na.rm = TRUE)
        # .groups = "drop"
        ) %>%
    ungroup() %>%
    # head()
    mutate(onset_offset = factor(x = onset_offset, levels = c("onset", "offset") ) ) %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "raw_p", "pval_bh", "pval_by", "pval_holm", "cluster_mass",
                "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM (new method)"
                ),
            )
        ) %>%
    mutate(
        method_reordered = tidytext::reorder_within(
            x = method,
            by = 1 / abs(median_error),
            within = onset_offset
            )
        ) %>%
    ggplot(
        aes(
            x = median_error,
            xmin = median_error - variance,
            xmax = median_error + variance,
            y = method_reordered,
            colour = method,
            fill = method
            )
        ) +
    geom_vline(xintercept = 0, linetype = 2) +
    geom_pointrange(
        size = 0.25,
        linewidth = 0.75,
        show.legend = FALSE
        ) +
    facet_wrap(~onset_offset, scales = "free") +
    scale_y_reordered() +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(
        x = "Median difference to full dataset (s)",
        y = "Method"
        )
```

```{r fig-reliability-distributions, echo = FALSE, eval = FALSE, fig.width = 9, fig.asp = 0.5, fig.cap = "Median absolute error and variance of onset (left) and offset (right) estimates for each method."}
# plotting distributions
reliability_results %>%
    slice(1:nrow(.)-1) %>%
    pivot_longer(cols = -split_id, names_to = "measure", values_to = "value") %>%
    data.frame() %>%
    left_join(
        full_reliability_results,
        by = "measure",
        suffix = c("_split", "_full")
        ) %>%
    # mutate(error = abs(value_split - value_full) ) %>%
    mutate(error = value_split - value_full) %>%
    # mutate(error = (value_split - value_full)^2) %>%
    separate(measure, into = c("type", "method"), sep = "_", extra = "merge") %>%
    # now plotting
    mutate(type = factor(x = type, levels = c("onset", "offset") ) ) %>%
    filter(method != "brms_full") %>%
    mutate(
        method = factor(
            x = method,
            levels = c(
                "p", "bh", "by", "holm", "cluster_mass",
                "cluster_tfce", "cpt", "brms"
                ),
            labels = c(
                "Raw p-value", "FDR BH95", "FDR BY01", "Holm", "Cluster mass",
                "TFCE", "Change point", "BGAM"
                ),
            )
        ) %>%
    mutate(
        plot_row = case_when(
            method %in% c("Raw p-value", "Holm") ~ 1,
            method %in% c("FDR BH95", "FDR BY01") ~ 2,
            method %in% c("Cluster mass", "TFCE") ~ 3,
            method %in% c("Change point", "BGAM") ~ 4
            )
        ) %>%
    ggplot(
        aes(
            x = error,
            colour = method,
            fill = method
            )
        ) +
    geom_vline(xintercept = 0, linetype = 2) +
    geom_density(aes(fill = NULL) ) +
    facet_grid(plot_row~type, scales = "free_x") +
    theme_light(base_size = 12, base_family = "Open Sans") +
    scale_fill_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    scale_colour_manual(values = met.brewer(name = "Johnson", n = 8) ) +
    labs(x = "Onset/Offset (s)", y = "Density")
```

\newpage

# Discussion

## Summary of the proposed approach

Overall, before concluding on the onset/offset of effect based on the model, we need to ensure that the model provides a faithful description of the data-generating process (e.g., via posterior predictive checks etc)...

## Limitations and future directions

As in previous simulation work [e.g., @rousselet2008; @sassenhagen2019], the present simulation results depend on various choices such as the specific cluster-forming algorithm and threshold, signal-to-noise ratio, negative impact of preprocessing steps (e.g., low-pass filter) on temporal resolution... note however, that the same caveats apply to all methods...

The error properties depend on the threshold parameter, a value of 10 or 20 seems to be a reasonable default, but the optimal threshold parameter can be adjusted using split-half reliability assessment...

Can be applied to any 1D timeseries (e.g., pupillometry, electromyography)... Extending the approach to spatiotemporal data (i.e., time + sensors)...

We kept the exemplary models simple, but can be extended by adding varying/random effects (intercept and slope) for item (e.g., word)... but also continuous predictors at the trial level?

\newpage

# Data and code availability

The simulation results as well as the `R` code to reproduce the simulations are available on GitHub: <https://github.com/lnalborczyk/brms_meeg>. The `neurogam R` package is available at <https://github.com/lnalborczyk/neurogam>.

# Packages

```{r, echo = FALSE}
# citing the R packages we have used
cite_packages(
    output = "paragraph",
    omit = c("permuco", "reticulate"),
    out.dir = "."
    )
```

# Acknolwedgements

Centre de Calcul Intensif d’Aix-Marseille is acknowledged for granting access to its high performance computing resources.

\newpage

# References

::: {#refs}
:::

\newpage

# Appendix

# Application to 2D time-resolved decoding results (cross-temporal generalisation)

Assume we have M/EEG data and we have conducted cross-temporal generalisation analyses [@king2014]. As a result, we have a 2D matrix where each element contains the decoding accuracy (e.g., ROC AUC) of a classifier trained at timestep $\text{training}_{i}$ and tested at timestep $\text{testing}_{j}$ (@fig-sim-timegen).

```{r fig-sim-timegen, echo = FALSE, dpi = 300, fig.cap = "Exemplary (simulated) group-level average cross-temporal generalisation matrix of decoding performance (ROC AUC)."}
# number of participants
n_participants <- 10

# number of timepoints
# n_timepoints <- 240 # (1.2s at 200Hz)
n_timepoints <- 60 # (1.2s at 50Hz)
timepoints <- seq(from = -0.2, to = 1, length.out = n_timepoints)

# smooth onset and offset parameters
decode_onset <- 0.2
decode_offset <- 0.3

# smoothness of onset/offset
sigma_smooth <- 0.05

# generating participant-specific cross-temporal generalisation matrices
timegen_data <- tibble()

# for each participant
for (p in 1:n_participants) {
  
    # adding variability in peak amplitude and width per participant
    peak_variability <- runif(1, 30, 50) # random peak strength (higher = stronger decoding)
    smooth_variability <- runif(1, 0.04, 0.06) # random smoothness variation
    onset_variability <- runif(1, -0.05, 0.05) # small jitter in onset time
    offset_variability <- runif(1, -0.05, 0.05) # small jitter in offset time
    
    # computing smooth transition function (soft onset and offset with participant variation)
    decode_window <- outer(timepoints, timepoints, function (t1, t2) {
        
        # smooth onset
        onset_factor <- 1 /
            (1 + exp(-(t1 - (decode_onset + onset_variability) ) / sigma_smooth) )
        
        # smooth offset
        offset_factor <- 1 /
            (1 + exp((t1 - (decode_offset + offset_variability)) / sigma_smooth) )
        
        # diagonal Gaussian
        onset_factor * offset_factor * exp(-((t1 - t2)^2) / (2 * smooth_variability^2) )
        
    })
    
    # defining smooth alpha and beta parameters for Beta distribution
    base_alpha <- 10
    peak_alpha <- peak_variability
    alpha_matrix <- base_alpha + peak_alpha * decode_window
    beta_matrix <- base_alpha
    
    # generating AUC values using the Beta distribution
    auc_matrix_beta <- matrix(
        rbeta(n_timepoints^2, shape1 = alpha_matrix, shape2 = beta_matrix),
        nrow = n_timepoints, ncol = n_timepoints
        )
    
    # converting to long format for plotting
    temp_data <- expand.grid(
        train_time = timepoints, test_time = timepoints
        ) %>%
        mutate(auc = as.vector(auc_matrix_beta), participant = p)
    
    # storing results
    timegen_data <- bind_rows(timegen_data, temp_data)
    
}

# plotting cross-temporal generalization matrix
timegen_data %>%
    summarise(auc = mean(auc), .by = c(train_time, test_time) ) %>%
    ggplot(aes(x = train_time, y = test_time, fill = auc) ) +
    geom_tile() +
    geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
    geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
    geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
    scale_x_continuous(expand = c(0, 0) ) +
    scale_y_continuous(expand = c(0, 0) ) + 
    coord_fixed() +
    scale_fill_scico(palette = "vik", midpoint = 0.5, name = "AUC") +
    labs(x = "Testing time (s)", y = "Training time (s)")
```

```{r mne-timegen, eval = FALSE, echo = FALSE}
# defining the function in R (it will be executed in Python)
mne_timegen <- function (X, labels, ncores = 8) {
    
    # converting R dataframe to NumPy array (reshaping if needed)
    # X should be a matrix before conversion
    X_np <- np$array(X)
    
    if (length(dim(X_np) ) == 2) {

        # adding a second dimension (channels) if missing
        X_np <- np$expand_dims(X_np, axis = as.integer(1) )

    }
  
    # defining the classifier
    clf <- sklearn$linear_model$LogisticRegression(solver = "liblinear")
    
    # sliding the estimator on all time frames
    time_gen <- mne$decoding$GeneralizingEstimator(
        clf,
        n_jobs = as.integer(ncores),
        scoring = "roc_auc",
        verbose = FALSE
        )
    
    # or using N-fold cross-validation
    scores <- mne$decoding$cross_val_multiscore(
        time_gen,
        X_np,
        labels,
        cv = as.integer(4),
        n_jobs = as.integer(ncores),
        verbose = FALSE
        )
    
    # returning the scores
    return (scores)
    
}

# listing all participants
participants <- unique(raw_df$participant)

# initialising empty decoding results
group_timegen_scores <- data.frame()

# running decoding for each participant
for (ppt in participants) {
    
    # printing progress
    print(ppt)
    
    # retrieve data from one participant
    ppt_data <- raw_df %>%
        filter(participant == ppt) %>%
        select(-participant) %>%
        pivot_wider(names_from = time, values_from = eeg) %>%
        select(-condition, -trial)
        
    # extracting the labels
    labels <- raw_df %>%
        filter(participant == ppt) %>%
        select(-participant) %>%
        pivot_wider(names_from = time, values_from = eeg) %>%
        pull(condition) %>%
        as.numeric()
    
    # extracting the timesteps
    timesteps <- raw_df %>%
        filter(participant == ppt) %>%
        select(-participant) %>%
        pull(time) %>%
        unique()
    
    # running the decoding
    timegen_scores <- mne_timegen(
        X = ppt_data,
        labels = labels-1
        )
    
    # computing the average over CV folds
    # a <- timegen_scores %>%
    #     apply(MARGIN = c(2, 3), FUN = mean) %>%
    #     data.frame()
    timegen_scores <- data.frame(np$mean(timegen_scores, axis = as.integer(0) ) )
    
    # sanity check
    # image(as.matrix(timegen_scores) )
    
    # appending to previous results
    group_timegen_scores <- bind_rows(group_timegen_scores, timegen_scores)
    
}

# saving the scores
saveRDS(object = group_timegen_scores, file = "results/timegen_scores.rds")

# reshaping the data
colnames(group_timegen_scores) <- unique(raw_df$time)
timegen_data <- group_timegen_scores %>%
    mutate(participant = rep(x = participants, each = n_distinct(raw_df$time) ) ) %>%
    mutate(train_time = rep(x = unique(raw_df$time), times = length(participants) ) ) %>%
    pivot_longer(
        cols = -c(participant, train_time),
        names_to = "test_time",
        values_to = "auc"
        ) %>%
    mutate(test_time = as.numeric(test_time) )
```

```{=html}
<!--

Now, we want to test whether and when decoding performance is above chance level (0.5 for a binary decoding task). These two models are computationally heavier to fit (more observations and 2D smooth functions)...

See https://bookdown.org/epeterson_2010/bios526_book/Module_5_3.html#bivariate-splines

-->
```

To model cross-temporal generalisation matrices of decoding performance (ROC AUC), we extended the initial (decoding) GAM to take into account the bivariate temporal distribution of AUC values, thus producing naturally smoothed estimates (timecourses) of AUC values and posterior probabilities. This model can be written as follows:

$$
\begin{aligned}
\text{AUC}_{i} &\sim \mathrm{Beta}(\mu_{i}, \phi)\\
g(\mu_{i}) &= f \left(\text{train}_{i}, \text{test}_{i} \right)\\
\end{aligned}
$$

where we assume that AUC values come from a $\mathrm{Beta}$ distribution with two parameters $\mu$ and $\phi$. We can think of $f \left(\text{train}_{i}, \text{test}_{i} \right)$ as a surface (a smooth function of two variables) that we can model using a 2-dimensional splines. Let $\mathbf{s}_{i} = \left(\text{train}_{i}, \text{test}_{i} \right)$ be some pair of training and testing samples, and let $\mathbf{k}_{m} = \left(\text{train}_{m}, \text{test}_{m} \right)$ denote the $m^{\text{th}}$ knot in the domain of $\text{train}_{i}$ and $\text{test}_{i}$. We can then express the smooth function as:

$$
f \left(\text{train}_{i}, \text{test}_{i} \right) = \alpha + \sum_{m=1}^M \beta_{m} b_{m} \left(\tilde{s}_{i}, \tilde{k}_{m} \right)
$$

Note that $b_{m}(,)$ is a basis function that maps $R \times R \rightarrow R$. A popular bivariate basis function uses *thin-plate splines* [@wood2003], which extend to $\mathbf{s}_{i} \in \mathbb{R}^{d}$ and $\partial l_{g}$ penalties. These splines are designed to interpolate and approximate smooth surfaces over two dimensions (hence the "bivariate" term). For $d=2$ dimensions and $l=2$ (smoothness penalty involving second order derivative):

$$
f \left(\tilde{s}_{i} \right) = \alpha + \beta_{1} x_{i} + \beta_{2} z_{i} +\sum_{m=1}^{M} \beta_{2+m} b_m\left(\tilde{s}_i, \tilde{k}_m\right)
$$

using the the radial basis function given by:

$$
b_m\left(\tilde{s}_i, \tilde{k}_m\right)=\left\|\tilde{s}_i-\tilde{k}_m\right\|^2 \log \left\|\tilde{s}_i-\tilde{k}_m\right\|
$$

where $\left\|\mathbf{s}_i-\mathbf{k}_{m}\right\|$ is the Euclidean distance between the covariate $\mathbf{s}_{i}$ and the knot location $\mathbf{k}_{m}$. We fitted this model using `brms`...

```{r gam-gp-timegen, message = FALSE}
# fitting a GAM with two temporal dimensions
timegen_gam <- brm(
    # 2D thin-plate spline (tp)
    auc ~ t2(train_time, test_time, bs = "tp", k = 20),
    data = timegen_data,
    family = Beta(),
    warmup = 1000,
    iter = 2000,
    chains = 8,
    cores = 8,
    file = "models/timegen_gam_t2.rds"
    )

# fitting a GP with two temporal dimensions
# timegen_gp <- brm(
#     auc ~ gp(train_time, test_time, k = 20),
#     data = timegen_data,
#     family = Beta(),
#     control = list(adapt_delta = 0.95),
#     iter = 2000,
#     chains = 4,
#     cores = 4,
#     file = "models/timegen_gp.rds"
#     )
```

```{r gam-timegen-preds, eval = FALSE, echo = FALSE, fig.cap = "Posterior predictions of the GAM fitted on decoding accuracy over time."}
# plotting the posterior predictions
plot(
    conditional_effects(x = timegen_gam, prob = 0.95),
    points = FALSE,
    # point_args = list(size = 1, pch = 21, colour = "grey"),
    # line_args = list(colour = "black"),
    method = "posterior_epred",
    surface = TRUE,
    theme = theme_light(),
    plot = TRUE
    )#[[1]] +
    # geom_hline(yintercept = 0.5, linetype = 2) +
    # geom_vline(xintercept = 0.0, linetype = 2) +
    # ylim(0, 1) +
    # labs(x = "Time (s)", y = "Decoding accuracy (ROC AUC)")

# posterior predictive checks
pp_check(timegen_gam)
```

```{r gam-timegen-post-preds, echo = FALSE, fig.width = 8, fig.asp = 0.6, fig.cap = "Predicted AUC values (left) and posterior probabilies of decoding accuracy being above chance level (right) according to the bivariate GAM."}
# defining a function to plot posterior prob of slope above 0
plot_post_timegen <- function (model, data, chance_level = 0.5, eps = 0.1) {
    
    # defining a sequence of x values to make predictions
    preds_conds <- crossing(
        train_time = unique(data$train_time)[c(TRUE, FALSE)],
        test_time = unique(data$test_time)[c(TRUE, FALSE)]
        )
    
    # retrieving posterior predictions
    post_preds <- conditional_effects(
        x = model,
        conditions = preds_conds,
        method = "posterior_epred",
        prob = 0.95,
        ndraws = 100
        )[[1]]
    
    # plotting cross-temporal generalization matrix
    post_preds_plot <- post_preds %>%
        summarise(auc = mean(estimate__), .by = c(train_time, test_time) ) %>%
        ggplot(aes(x = train_time, y = test_time, fill = auc) ) +
        geom_tile() +
        geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
        scale_x_continuous(expand = c(0, 0) ) +
        scale_y_continuous(expand = c(0, 0) ) + 
        coord_fixed() +
        scale_fill_scico(palette = "vik", midpoint = 0.5, name = "AUC") +
        labs(x = "Testing time (s)", y = "Training time (s)")

    # retrieving posterior samples
    posterior_samples <- posterior_epred(object = model, newdata = preds_conds)
    
    # computing the probability that y is above 0
    prob_y_above_0 <- preds_conds %>%
        mutate(post_prob = colMeans(posterior_samples) ) %>%
        mutate(m = colMeans(posterior_samples > (chance_level + eps) ) ) %>%
        mutate(prob_ratio = m / (1 - m) )
        
    # plotting it
    # prob_y_above_0 %>% filter(prob_ratio != Inf) %>% pull(prob_ratio) %>% range()
    prob_above_chance_plot <- prob_y_above_0 %>%
        mutate(prob_ratio = ifelse(prob_ratio == Inf, 2000, prob_ratio) ) %>%
        mutate(prob_ratio = ifelse(prob_ratio == 0, 0.0005, prob_ratio) ) %>%
        ggplot(aes(x = train_time, y = test_time, fill = prob_ratio) ) +
        geom_tile() +
        geom_vline(xintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_hline(yintercept = 0, linetype = 2, linewidth = 0.5) +
        geom_abline(slope = 1, intercept = 0, linetype = 2, linewidth = 0.5) +
        scale_x_continuous(expand = c(0, 0) ) +
        scale_y_continuous(expand = c(0, 0) ) + 
        coord_fixed() +
        scale_fill_scico(palette = "vik", midpoint = 0, name = "p/(1-p)", trans = "log10") +
        # scale_fill_scico(palette = "vik", midpoint = 1, name = "p/(1-p)") +
        # theme_light() +
        labs(x = "Testing time (s)", y = "Training time (s)")
    
    # combining the two plots
    post_preds_plot + prob_above_chance_plot
    
    # some tests
    # post_preds_plot +
    #     geom_contour(
    #         data = prob_y_above_0,
    #         aes(x = train_time, y = test_time, z = prob_ratio),
    #         breaks = 10,
    #         color = "red",
    #         linewidth = 1,
    #         inherit.aes = FALSE
    #         )
  
}

# using the function with the decoding GAM
plot_post_timegen(
    model = timegen_gam, data = timegen_data,
    chance_level = 0.5, eps = 0.01
    )
```

Could be extended to spatial and temporal dimensions with formulas such as `te(x, y, Time, d = c(2, 1) )`...

\newpage

```{=html}
<!--

# Threshold-free cluster enhancement

Cluster-based permutation approaches require defining a cluster-forming threshold (e.g., a t- or f-value) as the initial step of the algorithm. As different cluster-forming thresholds lead to clusters with different spatial or temporal extent, this initial threshold modulates the sensitivity of the subsequent permutation test. The threshold-free cluster enhancement method (TFCE) was introduced by @smith2009 to overcome this choice of an arbitrary threshold.

In brief, the TFCE method works as follows. Instead of picking an arbitrary cluster-forming threshold (e.g., $t=2$), we try all (or many) possible thresholds in a given range and check whether a given timestep/voxel belongs to a significant cluster under any of the set of thresholds. Then, instead of using cluster mass (e.g., the sum of squared t-values within the cluster), we use a weighted average between the cluster extend ($e$, how broad is the cluster, that is, how many connected samples it contains) and the cluster height ($h$, how high is the cluster, that is, how large is the test statistic). The TFCE score at each timestep/voxel $t$ is given by:

$$
\text{TFCE(t)} = \int_{h=h_{0}}^{h_{t}} e(h)^{E} h^{H} \mathrm{d}h
$$

where $h_{0}$ is typically $0$ and parameters $E$ and $H$ are set a priori (typically to 0.5 and 2, respectively) and control the influence of the extend and height on the TFCE. Then, p-value for timestep/voxel $t$ is computed by comparing it TFCE with the null distribution of TFCE values. For each permuted signal, we keep the maximal value over the whole signal for the null distribution of the TFCE.... But see @sassenhagen2019...

\newpage

-->
```

# Alternative to GAMs: Approximate Gaussian Process regression

A Gaussian process (GP) is a stochastic process that defines the distribution over a collection of random variables indexed by a continuous variable, that is $\{f(t): t \in \mathcal{T}\}$ for some index set $\mathcal{T}$ [@riutort-mayol_practical_2023; @rasmussen2005]. Whereas Bayesian linear regression outputs a distribution over the parameters of some predefined parametric model, the GP approach, in contrast, is a non-parametric approach, in that it finds a distribution over the possible functions that are consistent with the observed data. However, note that nonparametric does not mean there aren't parameters, it means that there are infinitely many parameters.

From [brms documentation](https://www.rdocumentation.org/packages/brms/versions/2.22.0/topics/gp): A GP is a stochastic process, which describes the relation between one or more predictors $x=\left(x_{1}, \ldots, x_{d}\right)$ and a response $f(x)$, where $d$ is the number of predictors. A GP is the generalization of the multivariate normal distribution to an infinite number of dimensions. Thus, it can be interpreted as a prior over functions. The values of $f()$ at any finite set of locations are jointly multivariate normal, with a covariance matrix defined by the covariance kernel $k_p\left(x_i, x_j\right)$, where $p$ is the vector of parameters of the GP:

$$
\left(f\left(x_{1}\right), \ldots f\left(x_{n}\right) \sim \operatorname{MVN}\left(0, \left(k_p\left(x_{i}, x_{j}\right)\right)_{i, j=1}^{n}\right)\right.
$$

The smoothness and general behaviour of the function $f$ depends only on the choice of covariance kernel, which ensures that values that are close together in the input space will be mapped to similar output values...

From this perspective, $f$ is a realisation of an infinite dimensional normal distribution:

$$
f \sim \mathrm{Normal}(0, C(\lambda))
$$

where $C$ is a covariance kernel with hyperparameters $\lambda$ that defines the covariance between two function values $f\left(t_1\right)$ and $f\left(t_2\right)$ for two time points $t_1$ and $t_2$ [@rasmussen2005]. Similar to the different choices of the basis function for splines, different choices of the covariance kernel lead to different GPs. In this article, we consider the squared-exponential (a.k.a. radial basis function) kernel, which computes the squared distance between points and converts it into a measure of similarity. It is defined as:

$$
C(\lambda) := C\left(t_1, t_2, \sigma, \gamma\right) := \sigma^2 \exp \left(-\frac{||t_1-t_2||^{2}}{2 \gamma^2}\right)
$$

with hyperparameters $\lambda = (\sigma, \gamma)$, expressing the overall scale of GP and the length-scale, respectively [@rasmussen2005]. The advantages of this kernel are that it is computationally efficient and (infinitely) smooth making it a reasonable choice for the purposes of the present article. Here again, $\lambda$ hyperparameters are estimated from the data, along with all other model parameters.

Taken from <https://michael-franke.github.io/Bayesian-Regression/practice-sheets/10c-Gaussian-processes.html>: For a given vector $\mathbf{x}$, we can use the kernel to construct finite multi-variate normal distribution associated with it like so:

$$
\mathbf{x} \mapsto_{G P} \operatorname{MVNormal}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}))
$$

where $m$ is a function that specifies the mean for the distribution associated with $\mathbf{x}$. This mapping is essentially the Gaussian process: a systematic association of vectors of arbitrary length with a suitable multi-variate normal distribution.

Low-rank approximate Gaussian processes are of main interest in machine learning and statistics due to the high computational demands of exact Gaussian process models [@riutort-mayol_practical_2023]...

```{r gp, echo = FALSE, eval = FALSE, message = FALSE}
# See also <https://jtimonen.github.io/lgpr-usage/articles/math.html>...
# Now we fit the GP regression model...
# computing eeg_diff and averaging across participants
# group_df <- raw_df %>%
#     group_by(participant, time) %>%
#     pivot_wider(names_from = condition, values_from = eeg, values_fn = mean) %>%
#     mutate(eeg_diff = cond2 - cond1) %>%
#     summarise(eeg_diff = mean(eeg_diff) ) %>%
#     ungroup()

# see https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html
# and https://betanalpha.github.io/assets/case_studies/gaussian_processes.html
gp_priors <- c(
    set_prior("normal(0, 1)", class = "Intercept"),
    set_prior("normal(0, 1)", class = "b"),
    set_prior("exponential(1)", class = "sigma"),
    set_prior("exponential(1)", class = "sdgp")
    )

gp_model <- brm(
    # k refers to the number of basis functions for
    # computing Hilbert-space approximate GPs
    # if k = NA (default), exact GPs are computed
    # eeg ~ gp(time, by = condition),
    # eeg ~ condition + gp(time, by = condition, k = 20, cov = "exp_quad"),
    eeg ~ condition + gp(time, by = condition, k = 20, cov = "matern32"),
    data = ppt_df,
    family = gaussian(),
    # prior = gp_priors,
    control = list(adapt_delta = 0.99, max_treedepth = 15),
    iter = 2000,
    chains = 4,
    cores = 4,
    file = "models/gp.rds"
    )
```

```{r meta-gp, echo = FALSE, eval = FALSE, message = FALSE}
# fitting the GP
meta_gp <- brm(
    # using by-participant SD of ERPs across trials
    eeg_mean | se(eeg_sd) ~
        condition + gp(time, k = 20, by = condition) +
        (1 | participant),
    data = summary_df,
    family = gaussian(),
    control = list(adapt_delta = 0.99),
    iter = 5000,
    chains = 4,
    cores = 4,
    file = "models/meta_gp.rds"
    )
```

\newpage

# How to choose the GAM basis dimension? {#sec-basis}

Here provide recommendation about how to define `k`. An option is to vary `k` and examine the predictions and posterior predictive checks (PPCs) of each model... In this example (@fig-choose-k)... However, it is not possible to provide general recommendations, as the optimal `k` depends on the sampling rate, the preprocessing steps (e.g., signal-to-noise ratio, low-pass filtering, etc), and the neural dynamics of the phenomenon under study.

```{r fig-choose-k, results = "hide", echo = FALSE, fig.width = 9, fig.asp = 1, fig.cap = "Posterior predictions and posterior predictive checks for the GAM with varying k (in rows)."}
# defining a list of possible k values
# k_values <- c(5, 10, 20, 50)

# initialising lists to store models and plots
# gam_models <- list()
# post_pred_plots <- list()
# ppc_dens_plots <- list()
# ppc_ecdf_plots <- list()
# ppc_stat2d_plots <- list()
# 
# # looping over k values
# for (k_value in k_values) {
#     
#     # printing progress
#     cat("Processing k =", k_value, "\n")
#     
#     # fitting the GAM
#     gam <- brm(
#         eeg ~ condition + s(time, bs = "tp", k = k_value, by = condition),
#         data = ppt_df,
#         family = gaussian(),
#         warmup = 2000,
#         iter = 5000,
#         chains = 4,
#         cores = 4
#         )
#     
#     # storing the model
#     gam_models[[as.character(k_value)]] <- gam
#     
#     # generating posterior predictions
#     post_pred_plots[[as.character(k_value)]] <- plot_post_preds(model = gam) +
#         ggtitle(paste("Posterior predictions (k =", k_value, ")") )
#     
#     # generating posterior predictive checks
#     ppc_dens_plots[[as.character(k_value)]] <- pp_check(gam, ndraws = 100, type = "dens_overlay") + 
#         labs(x = "EEG signal", y = "Density") +
#         ggtitle(paste("Density overlay (k =", k_value, ")") ) +
#         theme(legend.position = "none")
#     
#     ppc_ecdf_plots[[as.character(k_value)]] <- pp_check(gam, ndraws = 100, type = "ecdf_overlay") + 
#         labs(x = "EEG signal", y = "Cumulative density") +
#         ggtitle(paste("ECDF overlay (k =", k_value, ")") ) +
#         theme(legend.position = "none")
#     
#     ppc_stat2d_plots[[as.character(k_value)]] <- pp_check(gam, ndraws = 500, type = "stat_2d", stat = c("mean", "sd") ) + 
#         labs(x = "Mean", y = "SD") +
#         ggtitle(paste("Mean vs. SD (k =", k_value, ")") )
# 
# }

# saving the results
# saveRDS(object = post_pred_plots, file = "results/post_pred_plots.rds")
# saveRDS(object = ppc_dens_plots, file = "results/ppc_dens_plots.rds")
# saveRDS(object = ppc_ecdf_plots, file = "results/ppc_ecdf_plots.rds")
# saveRDS(object = ppc_stat2d_plots, file = "results/ppc_stat2d_plots.rds")

# importing the results
post_pred_plots <- readRDS(file = "results/post_pred_plots.rds")
ppc_dens_plots <- readRDS(file = "results/ppc_dens_plots.rds")
ppc_ecdf_plots <- readRDS(file = "results/ppc_ecdf_plots.rds")
ppc_stat2d_plots <- readRDS(file = "results/ppc_stat2d_plots.rds")

# importing the results

# pp_check(gam, ndraws = 100, type = "ecdf_overlay") + 
#     labs(x = "EEG signal", y = "Cumulative density") + ggtitle(paste("ECDF overlay (k =", k_value, ")") ) + theme(legend.position = "none")
# pp_check(gam, ndraws = 200, type = "stat_2d", stat = c("median", "mad") ) + 
#     labs(x = "Mean", y = "SD") + ggtitle(paste("Mean vs. SD (k =", k_value, ")") )

# arranging all plots in a 4x4 grid using patchwork
# final_plot <- (
#     post_pred_plots[["5"]] + ppc_dens_plots[["5"]] + ppc_ecdf_plots[["5"]] + ppc_stat2d_plots[["5"]] +
#     post_pred_plots[["10"]] + ppc_dens_plots[["10"]] + ppc_ecdf_plots[["10"]] + ppc_stat2d_plots[["10"]] +
#     post_pred_plots[["20"]] + ppc_dens_plots[["20"]] + ppc_ecdf_plots[["20"]] + ppc_stat2d_plots[["20"]] +
#     post_pred_plots[["50"]] + ppc_dens_plots[["50"]] + ppc_ecdf_plots[["50"]] + ppc_stat2d_plots[["50"]]
#     ) + plot_layout(ncol = 4, nrow = 4)
final_plot <- (
    post_pred_plots[["5"]] + ppc_dens_plots[["5"]] + ppc_stat2d_plots[["5"]] +
    post_pred_plots[["10"]] + ppc_dens_plots[["10"]] + ppc_stat2d_plots[["10"]] +
    post_pred_plots[["20"]] + ppc_dens_plots[["20"]] + ppc_stat2d_plots[["20"]] +
    post_pred_plots[["50"]] + ppc_dens_plots[["50"]] + ppc_stat2d_plots[["50"]]
    ) + plot_layout(ncol = 3, nrow = 4)

# displaying the final plot
print(final_plot)

# saving the plot
# ggsave(
#     filename = "figures/k_value_ppc.png",
#     width = 20, height = 15, dpi = 200,
#     device = "png"
#     )

# combining posterior predictions and some posterior predictive checks
# post_preds <- plot_post_preds(model = gam)
# ppc_dens <- pp_check(object = gam, ndraws = 100, type = "dens_overlay") + labs(x = "EEG signal", y = "Density")
# ppc_ecdf <- pp_check(object = gam, ndraws = 100, type = "ecdf_overlay") + labs(x = "EEG signal", y = "CDF")
# ppc_stat2d <- pp_check(object = gam, ndraws = 200, type = "stat_2d", stat = c("mean", "sd") ) + labs(x = "Mean", y = "SD")
# post_preds + ppc_dens + ppc_ecdf + ppc_stat2d

# some posterior predictive checks
# pp_check(gam, prefix = "ppd")
# pp_check(gam, type = "error_hist")
# pp_check(object = gam, ndraws = 10, type = "intervals", x = "time")
# pp_check(object = gam, ndraws = 10, type = "scatter_avg")
# pp_check(object = gam, ndraws = 20, type = "hist")
# pp_check(object = gam, ndraws = 50, type = "stat_2d")
# pp_check(object = gam, ndraws = 50, type = "ecdf_overlay")
# pp_check(object = gam, ndraws = 200, type = "stat", stat = "mean")
# pp_check(object = gam, ndraws = 200, type = "stat", stat = "median")
# pp_check(object = gam, ndraws = 200, type = "stat_2d", stat = c("mean", "sd") )
# pp_check(object = gam, ndraws = 200, type = "stat_2d", stat = c("median", "mad") )
```

\newpage

# Integration with `MNE-Python`

For users who are already familiar with `brms`, the recommended pipeline is to import ERPs or decoding results in `R` and analyse these data using the code provided in the main paper. However, it is also possible to simply call functions from the `neurogam R` package (available at <https://github.com/lnalborczyk/neurogam>), which come with sensible defaults.

```{r, eval = FALSE, echo = TRUE}
# installing (if needed) and loading the neurogam R package
# remotes::install_github("https://github.com/lnalborczyk/neurogam")
library(neurogam)

# using the testing_through_time() function from the neurogam package
gam_onset_offset <- testing_through_time(
    data = raw_df,
    threshold = 10,
    participant_id = "participant", meeg_id = "eeg",
    time_id = "time", predictor_id = "condition",
    warmup = 1000, iter = 5000, chains = 4, cores = 4
    )

# displaying the results
gam_onset_offset$clusters
```

The `neurogam` package can also be called from `Python` using the `rpy2` module, and can easily be integrated into `MNE-Python` pipelines. For example, we use it below to estimate the onset and offset of effects for one channel from a MNE evoked object. Note that the code used to reshape the `sample MNE` dataset is provided later, and we refer to the [MNE documentation](https://mne.tools/stable/auto_tutorials/epochs/50_epochs_to_data_frame.html) about converting `MNE` epochs to `Pandas` dataframes in long format.

```{python, eval = FALSE, echo = TRUE}
# loading the Python modules
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr
from rpy2.robjects import pandas2ri
from rpy2.robjects.conversion import localconverter

# importing the "neurogam" R package
neurogam = importr("neurogam")

# activating automatic pandas-R conversion
pandas2ri.activate()

# assuming reshaped_df is some M/EEG data reshaped in long format
with localconverter(robjects.default_converter + pandas2ri.converter):
    
    reshaped_df_r = robjects.conversion.py2rpy(reshaped_df)
    

# using the testing_through_time() function from the neurogam R package
gam_onset_offset = neurogam.testing_through_time(
    data=reshaped_df_r,
    threshold=10,
    multilevel=False
    )

# displaying the results
print(list(gam_onset_offset) )
```

```{python, eval = FALSE, echo = TRUE}
# loading the Python modules
import mne
from mne.datasets import sample
import numpy as np
import pandas as pd

# defining the path to the "sample" dataset
path = sample.data_path()

# loading the evoked data
evokeds = mne.read_evokeds(path / "MEG" / "sample" / "sample_audvis-ave.fif")

# defining a function to reshape the data
def reshape_eeg_channels_as_participants(evokeds, condition_labels):
    
    all_dfs = []

    for evoked, condition in zip(evokeds, condition_labels):
        
        # selecting only EEG channels
        picks = mne.pick_types(evoked.info, meg=False, eeg=True)
        data = evoked.data[picks, :]
        channel_names = np.array(evoked.ch_names)[picks]

        n_channels, n_times = data.shape
        # repeating time for each channel
        times = np.tile(evoked.times, n_channels)
        meeg_values = data.flatten()
        pseudo_participant_ids = np.repeat(channel_names, n_times)
        
        # converting to dataframe
        df = pd.DataFrame({
            "participant": pseudo_participant_ids,
            "time": times,
            "eeg": meeg_values,
            "condition": condition
            })

        all_dfs.append(df)

    return pd.concat(all_dfs, ignore_index=True)

# picking two evoked conditions
faces = evokeds[0]
scrambled = evokeds[1]

# reshaping data by pretending each EEG channel is a participant
reshaped_df = reshape_eeg_channels_as_participants(
    evokeds=[faces, scrambled],
    condition_labels=["faces", "scrambled"]
    )
```
